---
layout: page

theme: dark
permalink: features/nexus-iri

title: Linking Experimental Facilities and Leadership Computing
hero-img-source: polaris+aps_1600x900.jpg
hero-img-caption: "The co-location of the ALCF and APS at Argonne provides an ideal environment for developing and demonstrating capabilities for a broader Integrated Research Infrastructure."
intro: "Argonne researchers are working to advance the DOE’s vision by integrating experimental facilities with ALCF computing resources."
---


As the volume of data generated by large-scale experiments continues to grow, the need for rapid data analysis capabilities is becoming increasingly critical to new discoveries. 

Argonne’s Advanced Photon Source Upgrade project (APS-U), for example, completed in 2024, increased the brightness of APS x-rays by as much as 500 times. With a corresponding increase in the amounts of experimental data generated, to quickly process and analyze the results requires the use of high-performance computing (HPC) systems.

Numerous ALCF activities and achievements have helped realize the DOE effort to build an Integrated Research Structure (IRI) that seamlessly connects experimental facilities with its world-class supercomputing resources, including:

-	Developing and testing methods to closely integrate supercomputers and experiments for near-real-time data analysis.
-	Partnering with Pathfinder projects to advance plasma physics and fusion energy research.
-	Participating in leadership groups and technical subcommittees dedicated to the design and implementation of computing facility functionality useful for experimentalists.


# Nexus
Under [Argonne's Nexus effort](https://www.anl.gov/nexus-connect), Argonne researchers are working to advance the DOE’s vision by integrating experimental facilities with ALCF computing resources. 

As IRI aims to deliver DOE-enterprise-wide infrastructure for computing, ALCF has continued its commitment to linking experimental facilities with ALCF computing. Work with APS over recent years has been a primary driver for defining new functionality and services ALCF has deployed to satisfy experiment-time computing needs at APS beamlines. Service accounts enable APS users to leverage automated analysis of their data at ALCF in a shared environment in a streamlined fashion throughout their multi-day beamline campaigns, with jobs running immediately at experiment time in the on-demand queue on Polaris. Analysis results are available to scientists at the beamline via Globus Sharing enabled on the Eagle filesystem, at the time of experiment and post-hoc. Building on these ALCF-deployed features, Globus Compute and Globus Flows manage application execution and data transfer in a frictionless manner, for projects across the DOE-SC program offices.


# Facility Integration

For over a decade, the ALCF and the APS have been collaborating to build the infrastructure for integrated ALCF-APS research, including the develpment of workflow management tools and enable secure access to on-demand computing.

With the upgraded APS providing x-rays up to 500 times brighter than before, the APS-ALCF collaboration is paying off with increased computational power at experiment time, a requirement given the corresponding growth in data rates and resulting computing needs. The APS powers more than 60 beamlines, each representing a unique experimental apparatus and research community. Of these, more than 20 identified significant computing needs and have engaged the full power of ALCF's Nexus services and functionality, using service accounts for transparent access to ALCF, and the demand queue for time-sensitive analysis of beamline data through integration with the APS Data Management system. With more beamlines coming online with greater data analysis needs, the APS demand for computing at ALCF will continue to grow, and also leverage the newly upgraded network connectivity between the facilities.

Working with a team at the Lawrence Berkeley National Laboratory Advanced Light Source, ALCF staff have helped to automate analysis of data from a tomography beamline on Polaris. Using a service account to submit jobs to Polaris through Globus Compute and the demand queue to analyze data at experiment time, the team has moved beyond an initial prototype and is now able to run analysis in a dedicated discretionary allocation. This production-ready capability is planned to be used in upcoming beamline experiments.

<br>

{% include media-video.html
   embed-code= '<iframe src="https://www.youtube.com/embed/twLutyNPmX4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>'
   caption= "Argonne's Nicholas Schwarz discusses how the integration of Aurora and the upgraded APS will transform science."
   credit= "Argonne National Laboratory"
%}


# Expanding and Demonstrating Capabilities

Getting instant access to DOE supercomputers for data analysis requires a shift in how the computing facilities operate. Each facility has established policies and processes for gaining access to machines, setting up user accounts, managing data and other tasks. If a researcher is set up at one computing facility but needs to use supercomputers at the other facilities, they would have to go through a similar set of steps again for each site.

Once a project is set up, researchers submit their jobs to a queue, where they wait their turn to run on the supercomputer. While the traditional queuing system helps optimize supercomputer usage at the facilities, it does not support the rapid turnaround times needed for the IRI.

To make things easy for the end users, the IRI will require implementing a uniform way for experimental teams to gain quick access to the DOE supercomputing resources.

To that end, Argonne has developed and demonstrated methods for overcoming both the user account and job scheduling challenges. The co-location of the APS and the ALCF on the Argonne campus has offered an ideal environment for testing and demonstrating such capabilities. When the ALCF launched the Polaris supercomputer in 2022, four of the system’s racks were dedicated to advancing the integration efforts with experimental facilities.

{% include media-img.html
   source= "ALCF-Polaris.jpg"
   caption= "The ALCF’s Polaris supercomputer is supporting research to advance the development of an Integrated Research Infrastructure."
   credit= "Argonne National Laboratory"
%}

In the case of user accounts, the existing process can get unwieldy for experiments involving several team members who need to use the computing facilities for data processing. Because many experiments have a team of people collecting data and running analysis jobs, it is important to devise a method that supports the experiment independent of who is operating the instruments on a particular day. In response to this challenge, the Argonne team has piloted the idea of employing “service accounts” that provide secure access to a particular experiment instead of requiring each team member to have an active account. 

To address the job scheduling issue, the Argonne team has set aside a portion of Polaris nodes to run with “on-demand” and “preemptable” queues. This approach allows time-sensitive jobs to run on the dedicated nodes immediately. 

Using data collected during an APS experiment, the team was able to complete their first fully automated end-to-end test of the service accounts and preemptable queues on Polaris with no humans in the loop. While work continues to enable these capabilities at more and more beamlines, this effort points to a future where the integration of the upgraded APS and the ALCF's Aurora exascale supercomputer will transform science at Argonne and beyond.

# Bringing It All Together

While Argonne and its fellow national labs have been working on projects to demonstrate the promise of an integrated research paradigm for the past several years, DOE’s Advanced Scientific Computing Research (ASCR) program made it a more formal initiative in 2020 with the creation of the IRI Task Force. Comprised of members from several national labs, including Argonne’s Corey Adams, Jini Ramprakash, Nicholas Schwarz, and Tom Uram, the task force identified the opportunities, risks, and challenges posed by such an integration.

{% include media-img.html
   source= "IRI-blueprint-report.png"
   caption= "DOE's IRI Architecture Blueprint Activity Report provides the conceptual foundations to move forward with coordinated DOE implementation efforts."
%}

ASCR recently launched the IRI Blueprint Activity to create a framework for implementing the IRI. In 2023, the blueprint team, which included Ramprakash and Schwarz, released the [IRI Archictecture Blueprint Activity Report](https://www.osti.gov/biblio/1984466), which describes a path forward from the lab’s individual partnerships and demonstrations to a broader long-term strategy that will work across the DOE ecosystem. Over the past year, the blueprint activities have started to formalize with the introduction of IRI testbed resources and environments. Now in place at each of the DOE computing facilities, the testbeds facilitate research to explore and refine IRI ideas in collaboration with teams from DOE experimental facilities.

{% include media-download.html
   thumbsource= "IRI-blueprint-report.png"
   filesource= "https://www.osti.gov/biblio/1984466"
   title= "IRI Architecture Blueprint Activity Report"
   credit= "Department of Energy"
   filetype= "PDF"
   filesize= "1.2kb"
%}

With the launch of Argonne’s Nexus effort, the lab will continue to leverage its expertise and resources to help DOE and the larger scientific community enable and scale this new paradigm across a diverse range of research areas, scientific instruments, and user facilities. 
