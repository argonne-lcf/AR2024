<!DOCTYPE html>
<html lang='en'><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search Results | 2024 ALCF Annual Report</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Search Results" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="2024 ALCF Annual Report" />
<meta property="og:description" content="2024 ALCF Annual Report" />
<link rel="canonical" href="http://localhost:4000/search.html" />
<meta property="og:url" content="http://localhost:4000/search.html" />
<meta property="og:site_name" content="2024 ALCF Annual Report" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search Results" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"2024 ALCF Annual Report","headline":"Search Results","url":"http://localhost:4000/search.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://use.typekit.net/vpa7ous.css">
  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="2024 ALCF Annual Report" /><meta property="og:title" content="ALCF 2023 Annual Report" />
  <meta property="og:image" content="http://localhost:4000/assets/images/og-ar2023.jpg" />
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V7JP2R5JPH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V7JP2R5JPH');
</script>

    <body class="search-results">
  <header class="header" role="banner">

  <div class="nav-wrapper">
    <div class="nav-desktop">

      <div class="masthead">
        <h1><a href="http://localhost:4000/index.html">ALCF Annual Report</a></h1>
      </div>


      <div class="nav">
        
          <h2>
            
              <a href="http://localhost:4000/year-in-review/directors-letter">Year in Review</a>
            
            <!-- <a href="http://localhost:4000/year-in-review/directors-letter">Year in Review</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/features">Features</a>
            
            <!-- <a href="http://localhost:4000/features">Features</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/community-and-outreach">Growing the HPC Community</a>
            
            <!-- <a href="http://localhost:4000/community-and-outreach">Growing the HPC Community</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/expertise-and-resources">Expertise and Resources</a>
            
            <!-- <a href="http://localhost:4000/expertise-and-resources">Expertise and Resources</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/science">Science</a>
            
            <!-- <a href="http://localhost:4000/science">Science</a> -->
          </h2>
        
        <div class='search-trig' id='js-search-trig'>
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
        </div>

        <form class='searchbar' id='js-searchbar' action="http://localhost:4000/search.html" method="get">
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
          <input 
            aria-label="Search the annual report"
            autocomplete="off"
            inputmode="search"
            placeholder="Search"
            type="search"
            id="search-box" 
            name="query"
          >
          <div class='x-box' id='js-searchbar-close'>
            <svg class='x' fill="currentColor" viewBox="0 0 512 512">
              <g>
                <polygon points="512,59.076 452.922,0 256,196.922 59.076,0 0,59.076 196.922,256 0,452.922 59.076,512 256,315.076 452.922,512 512,452.922 315.076,256"/>
              </g>
            </svg>
          </div>
        </form>

      </div>

    </div>
  </div>


  <div class="nav-wrapper">
    <div class="nav-mobile">

      <div class="masthead">
        <h1><a href="http://localhost:4000/index.html">ALCF Annual Report</a></h1>
      </div>


      <div class="nav">
        <div class="js-drop menuitem-wrapper" id="toggle-menu" data-menu="menu">
          <h1>Menu <!--&nbsp;&blacktriangledown;--></h1> 
          <div id="menu" class="dropdown js-dropdown-hidden">

            <div class="nav-wrapper">
               

                <div class="chunk">
                  <h2>
                    <a href="/year-in-review/directors-letter" class="menuitem">
                      Year in Review
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/year-in-review/directors-letter">
                            Director&rsquo;s Letter
                          </a>
                        </li>
                      
                        <li>
                          <a href="/year-in-review/year-in-review">
                            ALCF Leadership
                          </a>
                        </li>
                      
                        <li>
                          <a href="/year-in-review/about-alcf">
                            About ALCF
                          </a>
                        </li>
                      
                        <li>
                          <a href="/year-in-review/about-alcf#2024-by-the-numbers">
                            2024 by the Numbers
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/features" class="menuitem">
                      Features
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/features/aurora">
                            Aurora Nears Full Deployment
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/aurora-performance-highlights">
                            Aurora Performance Highlights
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/nexus-iri">
                            Linking Experimental Facilities and Leadership Computing
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/alcf-ai-testbed">
                            ALCF Continues to Expand AI Testbed Systems Deployed for Open Science
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/ai-training">
                            Preparing a new generation of AI-ready researchers
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/community-and-outreach" class="menuitem">
                      Growing the HPC Community
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/community-and-outreach/industry">
                            Partnering with Industry
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/building-an-hpc-workforce">
                            Building an HPC Workforce
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/hpc-community-activities">
                            Shaping the Future of Supercomputing
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/user-training-activities">
                            Training ALCF Users
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/educational-outreach-activities">
                            Inspiring Students
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/expertise-and-resources" class="menuitem">
                      Expertise and Resources
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/expertise-and-resources/systems">
                            ALCF Systems
                          </a>
                        </li>
                      
                        <li>
                          <a href="/expertise-and-resources/team">
                            ALCF Team
                          </a>
                        </li>
                      
                        <li>
                          <a href="/expertise-and-resources/staff-spotlights">
                            Staff Spotlights
                          </a>
                        </li>
                      
                        <li>
                          <a href="/expertise-and-resources/staff-news">
                            Staff News
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/science" class="menuitem">
                      Science
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/science/allocation-programs">
                            Allocation Programs
                          </a>
                        </li>
                      
                        <li>
                          <a href="/science/highlights">
                            2024 Science Highlights
                          </a>
                        </li>
                      
                        <li>
                          <a href="/science/projects">
                            2024 Award List
                          </a>
                        </li>
                      
                        <li>
                          <a href="/science/publications">
                            Publications
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
                
            </div>   
          </div>
        </div>
        <div class='search-trig' id='js-search-trig-mobile'>
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
        </div>
        <form class='searchbar' id='js-searchbar-mobile' action="http://localhost:4000/search.html" method="get">
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
          <input 
            aria-label="Search the annual report"
            autocomplete="off"
            inputmode="search"
            placeholder="Search"
            type="search"
            id="search-box" 
            name="query"
          >
          <div class='x-box' id='js-searchbar-close-mobile'>
            <svg class='x' fill="currentColor" viewBox="0 0 512 512">
              <g>
                <polygon points="512,59.076 452.922,0 256,196.922 59.076,0 0,59.076 196.922,256 0,452.922 59.076,512 256,315.076 452.922,512 512,452.922 315.076,256"/>
              </g>
            </svg>
          </div>
        </form>
      </div>

    </div>
  </div>

</header>




<nav class="sidenav">

  

    
    
    
    
  

    
    
    
    
  

    
    
    
    
  

    
    
    
    
  

    
    
    
    
  


</nav>

<div class="page-intro-text">
    	<!-- 

TEXT, BREADCRUMBS

-------------------------------------------------------------------------------

available parameters:

* required

-------------------------------------------------------------------------------

-->

<div id="breadcrumbs">
 
<a href="http://localhost:4000/index">Home</a>

  
    / Search Results
  

</div>
    	<h1 class="page-title">Search Results</h1>

    	
    </div>

    <div class="page-intro-media">
      
    </div>
    
    

    <div class="height-wrapper">
    	<main class="page-content layout-wrapper page" aria-label="Content">

<!-- Based on tutorial: https://learn.cloudcannon.com/jekyll/jekyll-search-using-lunr-js/ -->
<!-- ------------------------------------------------------------------------------------ -->



<h2 id='search-title'></h2>
<ul id="search-results"></ul>



<script>
  
  
  

  const highlights_json = {
    
      "science-highlights-ramanathan": {
        "title": "MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization",
        "content": "The ability to design and discover new proteins drives advances in medicine, catalysis, and numerous other applications. Leveraging AI and some of the world’s most powerful supercomputers, a multi-institutional team led by researchers at Argonne National Laboratory has developed a multimodal framework to accelerate the design of novel proteins.\n\nChallenge\n\nMapping a protein’s amino acid sequence to its structure and function is a long-standing research challenge. Each unique arrangement of amino acids—the building blocks of proteins—can yield different properties and behaviors. The sheer volume of potential variations makes it impractical to test them all through experiments alone. AI models offer a way to analyze and predict promising protein structures, but they must integrate diverse data sources to generate reliable designs. By combining AI with high-performance computing, researchers can overcome these challenges and accelerate the discovery process.\n\nApproach\n\nThe team developed MProt-DPO, a scalable, end-to-end workflow for protein design that integrates protein sequences, structural and functional information, and natural language descriptions of biochemical properties while incorporating feedback from experiments and molecular simulations. A key component of their framework is Direct Preference Optimization (DPO), which refines AI-generated protein designs based on fitness landscapes to improve their effectiveness. By leveraging multimodal data, the workflow enables AI to learn directly from experimental and computational feedback, enhancing the reliability of designed proteins. The framework was developed and deployed on five leading HPC systems: ALCF’s Aurora, OLCF’s Frontier, the Swiss National Supercomputing Centre’s ALPS, Cineca’s Leonardo, and NVIDIA’s PDX machine. The team achieved over one exaflop of sustained performance (mixed precision) on each machine, with a peak performance of 5.57 exaflops on Aurora.\n\nResults\n\nThe team tested MProt-DPO on two tasks to demonstrate its ability to handle complex protein design challenges: enhancing the yeast protein HIS7 based on deep mutational scanning data and improving the catalytic efficiency of malate dehydrogenase using molecular simulations. The framework demonstrated the ability to integrate diverse data sources, refine AI-generated sequences, and produce biologically plausible protein variants with high predicted fitness scores. The team is collaborating with Argonne biologists to validate the AI-generated designs in a laboratory, where initial tests have shown they are performing as expected. Their innovative approach was named a finalist for the 2024 Gordon Bell Prize, highlighting its impact in advancing AI-driven science.\n\nImpact\n\nThe team’s development of MProt-DPO marks a major advance in AI-driven protein design, with the potential to greatly accelerate the discovery of new proteins for applications ranging from biodegradation and rare earth extraction to biofuels and biomedicine. By integrating experimental validation into the AI training process, MProt-DPO improves the reliability of generative protein design models. The framework establishes a new benchmark for multimodal AI-driven protein design and contributes to broader AI for science initiatives, including Argonne’s development of AuroraGPT, a foundation model designed to aid in autonomous scientific exploration across disciplines.",
        "url": "http://localhost:4000//science/highlights/ramanathan"
      }
      ,
    
      "science-highlights-jiang": {
        "title": "Assessment of Turbulent Prandtl Number for Heavy Liquid Metal Flow in a Bare Rod Bundle",
        "content": "Westinghouse Electric Company is working with an international team to develop its next-generation high-capacity nuclear power plant based on lead-cooled fast reactor technology. Using ALCF supercomputers, researchers from Argonne National Laboratory are collaborating with the company to provide insights into the reactor’s flow physics and heat transfer mechanisms.\n\nChallenge\n\nLead-cooled fast reactors are a type of nuclear reactor design that offer many advantages, including the ability to operate at higher thermal efficiencies than existing commercial light water reactors. Developing these advanced reactors poses challenges due to the unique characteristics of heavy liquid metal (HLM) coolants, such as a low Prandtl number (Pr) compared to water. Existing turbulence models are inadequate for accurately predicting heat transfer in HLM flows, making the selection of an appropriate turbulent Prandtl number (Prt) critical. Accurate modeling and simulation of heat transfer and mixing in the HLM coolant is needed to help prepare the technology for licensing.\n\nApproach\n\nFor this effort, the team performed large eddy simulations (LES) using the open-source Nek5000 code on ALCF’s Theta system to study nuclear fuel rod bundles with HLM flows. LES do not require a Prt to model turbulence-driven heat transfer, and thus can be used as benchmarks for selecting a Prt in a less computationally expensive Reynolds Averaged Navier–Stokes (RANS) model which requires this parameter.\n\nResults\n\nIn a paper published in Nuclear Engineering and Design, the researchers showed that the selection of the appropriate Prt significantly impacts the accuracy of simulations for advanced nuclear reactors. By analyzing a prototypical lead-cooled fast reactor assembly with different Prt values, the team found that inappropriate Prtcan introduce error in Nusselt number (a measure of heat transfer) by up to 44 percent. They also compared detailed temperature distributions obtained by computationally expensive LES and less expensive RANS simulations to better understand the deviation introduced by the turbulence model. The analysis shows that the RANS model with Prt=1.5 shows the best agreement with LES on the prediction of local temperature distribution and global Nusselt number.\n\nImpact\n\nThe team’s research is helping to enhance the understanding and modeling of heavy liquid metal flow behavior and heat transfer mechanisms for next-generation nuclear reactors. In addition, their study provides valuable high-fidelity reference data that can be used by the nuclear reactor research community to validate and calibrate less computationally expensive models.",
        "url": "http://localhost:4000//science/highlights/jiang"
      }
      ,
    
      "science-highlights-prince": {
        "title": "Demonstrating Cross-Facility Data Processing at Scale with Laue Microdiffraction",
        "content": "The ongoing Advanced Photon Source (APS) upgrade project will offer users x-ray beams 500 times more powerful than previously available, with commensurate increases in the amounts of experimental data generated for processing. To meet the demands of the upgrade, a team of researchers led by Argonne National Laboratory developed high-performance software tools and data infrastructure to integrate APS data processing and analysis in near-real time with ALCF computing resources.\n\nChallenge\n\nThe co-location of the ALCF and the APS offers an ideal proving ground for methods to closely integrate supercomputers and experiments for near-real-time data analysis. The research team deployed a fully automated pipeline that uses ALCF resources to rapidly process data obtained from x-ray experiments at the APS. To demonstrate the capabilities of the pipeline, the team carried out a study using a technique called Laue microdiffraction, which is employed at the APS and other light sources to analyze materials with crystalline structures but requires significant computational resources.\n\nApproach\n\nAs a focused x-ray beam passes through a material, individual crystallites along the path of the beam diffract at different angles depending on their orientation. To obtain a full 3D map of the structure, the angle and position of each diffracted beam must be resolved. A new coded aperture Laue reconstruction algorithm is used instead of using a time-intensive scan to complete the analysis, necessitating supercomputers.\nThe automatic pipeline the team built for processing APS data leveraged infrastructure and tools being deployed between APS and ALCF as part of the Argonne Nexus effort. Globus handles much of the cross-facility data management. The APS Data Management System integrates with Globus Gladier/FuncX workflow tools to provide a single end-to-end data pipeline.\n\nResults\n\nWith ALCF resources, the team demonstrated the on-demand reconstruction of data obtained from the APS beamline, returning reconstructed scans to the APS within 15 minutes of them being sent to the ALCF. Near-ideal scaling for the workflow has been shown on as many as 100 nodes. Continuously using up to 50 nodes on Polaris, it was able to keep up with the data generation rate processing scans, which came in every 1 to 2 minutes throughout 6-to-12 hour runs. The work resulted in a Best Paper Award at the SC23 conference’s XLOOP workshop.\n\nImpact\n\nThe team’s results carry implications for future software development, engineering and beamline science. The parallelization, optimization, and deployment onto Polaris of the Laue microdiffraction technique has enabled full-scale analysis of microdiffraction data. The automated near-real-time reconstruction of coded aperture Laue datasets will enable users of ALCF supercomputing resources to collect data at speeds ten times faster than is currently possible, thereby accelerating the pace of scientific discovery. Furthermore, the full-scale reconstructions produced with Polaris are being used to improve the underlying beamline technique.",
        "url": "http://localhost:4000//science/highlights/prince"
      }
      ,
    
      "science-highlights-tramm": {
        "title": "Efficient Algorithms for Monte Carlo Particle Transport on AI Accelerator Hardware",
        "content": "In this study, researchers led by Argonne National Laboratory examined the feasibility of performing continuous energy Monte Carlo (MC) particle transport on the Cerebras Wafter-Scale Engine 2 (WSE-2). The researchers ported a key kernel from the MC transport algorithm to the Cerebras Software Language (CSL) programming model and evaluated the performance of the kernel on the Cerebras WSE-2.\n\nChallenge\n\nBeyond the challenge of porting the kernel into the low-level CSL programming model, the team proposed and tested various new algorithms to handle the decomposition of neutron cross-sectional data (which is used to generate random samples for particle behavior) into the small local memory domains contained in each of 750,000-some WSE-2 units.\n\nApproach\n\nThe researchers ported a simplified version of the MC cross-section lookup kernel (a kernel used by the MC neutral particle transport algorithm) using the Cerebras SDK and the Cerebras CSL programming model. Their decomposition and communication scheme involved three stages: (1) the sorting of particles into energy bands within each column of compute cores; (2) an iterative diffusion-based load balancing stage for balancing starting particle loads within each row; and (3) an exchange of particles to allow particles to accumulate nuclide information from each column in the row. All communication patterns had to be developed to avoid any concept of global synchronization or point-to-point message passing, given the limitations of the WSE-2 hardware. Additionally, the team developed an architecture-specific optimization to leverage the capabilities of the WSE-2 and a highly optimized CUDA kernel for testing on an NVIDIA A100 graphics processing unit to provide a baseline to contextualize the performance of the WSE-2.\n\nResults\n\nA single WSE-2 was found to run 130 times faster than the highly optimized CUDA version of the kernel deployed on a single NVIDIA A100—significantly outpacing expected performance increase, given the difference in transistor counts between the architectures. However, the performance gains came at a cost—namely, increases in both software programming and algorithmic complexity. Considering how AI accelerators such as the WSE-2 were designed almost exclusively around deep learning AI tasks, it is noteworthy that the WSE-2 is already able to exceed performance expectations relative to GPUs—an architecture that has had several decades to mature and is now quite friendly to HPC simulation applications. A follow-up study saw the WSE-2 achieve a 182x speedup over the A100.\n\nImpact\n\nThe team’s analysis suggests the potential for a wide variety of complex and irregular simulation methods to be mapped efficiently onto AI accelerators like the Cerebras WSE-2. Such accelerators could offer significant advantages to traditional simulation workloads, and the development of higher-level programming models to more readily enable software development and exploration could greatly benefit HPC simulations. MC simulations themselves offer the potential to fill in crucial gaps in experimental and operational nuclear reactor data.",
        "url": "http://localhost:4000//science/highlights/tramm"
      }
      ,
    
      "science-highlights-pal": {
        "title": "Robust Gas Turbine Film Cooling Under Manufacturing Uncertainty for Improved Jet Engine Lifecycle Energy Efficiency",
        "content": "Gas turbines in aircraft operate at extreme temperatures, making efficient cooling strategies essential for preventing thermal damage and extending component lifetimes. As next-generation aircraft engines become more compact and run at higher pressures, managing increased heat loads becomes even more critical. To address this challenge, researchers from RTX Technology Research Center and Argonne National Laboratory are using DOE supercomputers to better understand cooling flow physics, providing insights that can help inform more effective turbine cooling designs.\n\nChallenge\n\nFilm cooling is a widely used technology for protecting turbine components from intense heat by injecting cooling air through small holes to create a protective layer. Optimizing these designs is challenging due to the complex interactions between flow dynamics, heat transfer, and variations in cooling hole geometry. Surface roughness caused by manufacturing processes or material deposition can degrade cooling performance, often leading to conservative design choices that prioritize durability at the cost of efficiency. To improve turbine cooling, researchers need predictive models that accurately capture near-wall cooling flow physics, heat transfer, and the impact of manufacturing deviations like in-hole surface roughness. While computational fluid dynamics (CFD) simulations are essential for modeling these effects, the computational resources needed to generate fine-grained models of the flow near the turbine walls are often beyond the capabilities of most manufacturers. By leveraging HPC and AI, researchers can enhance modeling accuracy to inform the development of more efficient cooling strategies.\n\nApproach\n\nWith support from DOE’s High-Performance Computing for Energy Innovation (HPC4EI) program, the RTX-Argonne team continues its work to use ALCF and OLCF supercomputers to optimize turbine cooling designs. In the first phase of the project, researchers used NekRS, a high-fidelity CFD solver optimized for DOE supercomputers, to analyze how surface roughness inside cooling holes affects coolant flow and heat transfer. Their simulations achieved resolutions unattainable with conventional computing, enabling a more precise assessment of roughness effects. Now in phase two of the project, the team is integrating high-fidelity CFD simulations with machine learning to develop data-driven surrogate models for predicting the impact of in-hole surface roughness on film cooling effectiveness. This approach aims to develop reduced-order models to quantify the impact of manufacturing variability introduced by surface roughness on the performance of film cooling schemes, with implications for energy efficiency and durability of gas turbine engine components.\n\nResults\n\nIn a paper presented at the 2024 AIAA Aviation Forum and ASCEND conference, the team detailed how increased in-hole roughness reduces film cooling effectiveness, with roughness element height having the most significant impact. Comparisons with experimental data revealed that while simulations captured overall trends, discrepancies remained, particularly in cases with higher roughness. Moreover, it was observed that increasing the length of the roughness unit, while keeping the other parameters constant, significantly increases the thermal cooling effectiveness.\n\nImpact\n\nThe team’s research is advancing our understanding of film cooling and its impact on gas turbine performance. By refining predictive high-fidelity CFD models and developing reduced-order surrogate models , their work will enhance turbine cooling designs to maximize efficiency, reduce thermal stress, and extend the lifespan of aircraft engine components. The ongoing effort to integrate machine learning methods with high-fidelity simulations and supercomputing will further pave the way for shrinking design cycles, making advanced cooling designs more efficient and accessible to manufacturers.",
        "url": "http://localhost:4000//science/highlights/pal"
      }
      ,
    
      "science-highlights-grover": {
        "title": "From Quantum Mechanics to Hypersonic Aerothermodynamics",
        "content": "The development of hypersonic flight—the ability to fly at five times the speed of sound—and related capabilities is now of interest across the globe, with the potential to revolutionize technologies for national security, aviation, and space exploration. At the core of this research is the pursuit of accuracy in the prediction of the environment surrounding the hypervelocity vehicle.  This work, led by a research team from the University of Dayton Research Institute and Air Force Research Laboratory, replicated a hypervelocity ground-test experiment using the novel direct molecular simulation (DMS) method.\n\n\nChallenge\n\nAt high Mach numbers, the shock encapsulating a vehicle is strong enough to cause chemical reactions and excitation of internal energy modes in the shock-heated gas. A precise description of these processes is essential to many aspects of flight, especially the heat loads experienced by the vehicle. DMS methods were used to model a reactive Mach 8.2 oxygen flow over a double-cone geometry. The method relies on molecular dynamics guided by interaction potentials grounded in quantum mechanics to construct this complex flow-field. The free-stream conditions and the article configuration generate a flow with thermal, chemical, and mechanical nonequilibrium.\n\nApproach\n\nThe researchers used a custom, massively parallel version of the SPARTA DSMC code to carry out the DMS simulation on ALCF computing resources. Due to the robust grid adaption and efficient parallelization of the code, the team was able to carry out a DMS of a continuum-scale flow.\n\nResults\n\nAs detailed in Science Advances, the team used ALCF computing resources to simulate, for the first time, using a method grounded in quantum mechanics, a full-scale reactive hypersonic flow studied experimentally. Due to the fundamental nature of the simulation technique, this work provides a molecular-level description of internal energy excitation and reaction mechanisms throughout the system. Building on direct simulation Monte Carlo (DSMC) methods, DMS replaces stochastic collision models for particles in the flow with molecular dynamics calculations using interaction potentials derived from quantum mechanics. These ab-initio interaction potentials are the only modeling input in the flow; therefore, all flow features can be attributed the quantum mechanically=guided molecular interactions.\n\nImpact\n\nThis was the first-ever quantum mechanically guided simulation of a hypersonic ground test. The team’s research is helping to advance our understanding of the complex aerothermodynamics of hypersonic flight, providing insights that could help inform the design of safer and more efficient technologies for space travel and defense. The simulation performed is of sufficiently great fidelity and detail that it can be used as a benchmark solution to verify computational fluid mechanics (CFD) codes and assess fundamental gaps and opportunities of improvement for the physics simulated by lower fidelity models. This, in turn, enables robust and accurate CFD solutions to mission-critical scenarios where the flow field may be in thermal and chemical nonequilibrium.",
        "url": "http://localhost:4000//science/highlights/grover"
      }
      ,
    
      "science-highlights-larsson": {
        "title": "Three-Dimensional Shock Boundary Layer Interactions over Flexible Walls",
        "content": "To design safe and efficient hypersonic aircraft, engineers must understand how shockwaves and turbulence affect the aircraft’s performance and structural integrity. Recently, a team from the University of Southern California and the University of Maryland used ALCF supercomputing resources to develop predictive 3D simulations of shock wave and turbulent interactions over flexible walls.\n\nChallenge\n\nHigh-speed airflows create a thin boundary layer along solid surfaces. When a shock hits the boundary layer forcefully enough, it may create high-amplitude, low-frequency oscillations that can damage the aircraft. To address this, we need to understand the mechanics of shockwave and turbulent boundary-layer interactions (STBLI) and the fluid-structure interactions (FSI). Researchers have not extensively studied the fluid-structural coupling of STBLIs. While teams have led efforts to investigate the relationships of STBLIs with flexible panels, it is extremely challenging to characterize this class of interactions experimentally. Numerical simulations are crucial to provide these fundamental insights. Several simulation approaches have been tested, but many suffer from high associated computational costs. Approaches that have lower computational costs suffer from lower accuracy.\n\nApproach\n\nThe team used the ALCF’s Theta supercomputer to perform fully coupled fluid-structural 3D simulations of STBL over flexible walls to replicate and complement wind-tunnel experiments. They studied the interactions’ characteristic low-frequency motions on flexible panels using wall-modeled large-eddy simulations (WMLES). This method models rather than resolves the inner boundary layer, reducing the computational cost of the simulation and maintaining the physical fidelity of flow features like separation and reattachment. This approach allows for sufficiently long integration times needed to capture the low-frequency motions of interest. The team coupled the WMLES with a finite-element solid mechanics (FEM) solver to incorporate structural damping. This is the first time researchers have combined these approaches to study such interactions on flexible panels.\n\nResults\n\nTo validate the high-fidelity simulation methodology, the team used WMLES to replicate experiments at different strengths of the incident shock on the turbulent boundary layer. Based on these results, the team assessed the importance of the 3D effects in those interactions by conducting reduced-span simulations with imposed periodicity in the spanwise direction. The simulations replicated the coupled interactions observed experimentally with better accuracy than prior numerical studies, while also providing additional insights into the wind-tunnel experiments.\n\nImpact\n\nThe ultimate goal of the team’s research is to develop improved modeling techniques for the prediction of fluid-thermal-structural interactions through coupled specialized domain-specific solvers. These techniques will reduce the uncertainty factored into designing hypersonic vehicles and propulsion systems, leading to safer and more precise aircraft designs. This work, directly validated by x-ray free electron laser, ultrafast electron diffraction, and neutron experiments at DOE facilities, will enable future production of high-quality custom quantum material architectures for broad and critical applications for continued U.S. leadership in technology development, including that for sustainable ammonia, thereby addressing DOE basic research needs for transformative manufacturing and quantum materials. The Allegro–Legato model exhibits excellent computational scalability and GPU acceleration in carrying out NNQMD simulations, with strong promise for emerging exascale systems.",
        "url": "http://localhost:4000//science/highlights/larsson"
      }
      ,
    
      "science-highlights-chang": {
        "title": "Exascale Gyrokinetic Study of ITER Challenge on Power-Exhaust and ELM-Free Edge",
        "content": "This project aims to significantly advance our understanding of fundamental edge plasma physics in fusion reactors, answering questions critical to the successful operation of ITER and to the design of fusion power plants (FPPs).\n\nChallenge\n\nThe goal of this project is to perform two-pronged, inter-related fundamental edge physics studies of critical importance to the successful operation of ITER and to the design of FPPs. The first prong is the mitigation of high stationary heat-flux densities that will damage material walls while maintaining the high edge plasma pedestal within a safe operational window. The second prong is avoiding explosive transient power flow to material walls caused by edge localized mode crash.\n\nApproach\n\nAchieving their goals necessitates that the researchers employ DOE supercomputing resources. The team uses the electromagnetic edge gyrokinetic particle-in-cell code XGC, which enables the inclusion of two important but computationally expensive components: (a) the addition of tungsten impurity particles that are sputtered from ITER’s material wall as a third species along with deuterium and tritium fuel particles, and (b) the capability for plasma detachment from the divertor plates. Tungsten impurity particles, beyond their deleterious effect of radiating away plasma energy in the core, are known to significantly impact the edge physics, and the detached plasma is known to significantly reduce the divertor heat load.\n\nResults\n\nA paper published in Physics of Plasmas demonstrated the application of a bundling technique to model the diverse charge states of tungsten impurity species in total-f gyrokinetic simulations. XGC was used to simulate a JET H-mode-like plasma across an entire plasma volume, spanning from the magnetic axis to the divertor. Tungsten impurities were found to affect the deuterium fluxes of particles and heat.\n\nImpact\n\nAs fusion power represents a paradigm-shifting breakthrough, success of the ITER project is a high-priority challenge for the DOE mission, which will ultimately lead to more economical FPPs. To accomplish this goal, simulations based on first principles must be deployed to solve issues with power exhaust, including mitigating stationary heat-flux densities and avoiding unacceptably high transient power flow to material walls.",
        "url": "http://localhost:4000//science/highlights/chang"
      }
      ,
    
      "science-highlights-heitmann": {
        "title": "Simulating the Cosmos for the Roman and Rubin Telescopes",
        "content": "The Vera C. Rubin Observatory and Nancy Grace Roman Space Telescope are state-of-the-art telescopes set to revolutionize our understanding of the universe when they begin operations in 2025 and 2027, respectively. To prepare for the vast amounts of observational data they will generate, researchers from DOE, NASA, and U.S. universities joined forces to use the ALCF’s Theta supercomputer to produce nearly 4 million simulated images that depict the cosmos as the telescopes will see it.\n\nChallenge\n\nNASA’s space-based Roman telescope and the ground-based Rubin telescope, jointly funded by the National Science Foundation (NSF) and DOE, will provide unprecedented views of the cosmos to help advance research on dark energy, dark matter, and the evolution of the universe. Because the two surveys will produce highly complementary datasets, joint data processing and analysis has been identified as a critical step for achieving the most robust and powerful cosmological and astrophysical results. This approach, however, requires powerful computing resources and advanced simulation techniques to develop methods that take full advantage of the overlapping survey data.\n\nApproach\n\nResearchers from DOE (Argonne, SLAC), NASA, and academia partnered to use the ALCF’s Theta supercomputer to produce a set of overlapping joint synthetic Roman-Rubin time-domain surveys. Carried out as part of the broader NASA-led project, OpenUniverse, the team’s simulations account for the telescopes’ unique instrument performances, making them the most accurate predictions to date of what the telescopes will observe.\n\nResults\n\nThe team leveraged Theta’s processing power to generate approximately 4 million simulated images for the Rubin and Roman telescopes in about nine days—a task that would have taken around 300 years on a personal computer. The simulations cover the same patch of the sky, spanning 70 square degrees (roughly equivalent to the sky area covered by 350 full Moons). The team has released an initial 10-terabyte subset of the simulation data for the community to explore, with the remaining 390 terabytes to follow once all the data has been processed. The simulated images give researchers the opportunity to exercise their data processing pipelines, better understand their analysis codes, and accurately interpret the results so they are ready to use the real observational data right as soon as it starts coming in.\n\nImpact\n\nThe team’s simulations will help inform future research with the Roman and Rubin telescopes to advance our understanding of dark energy, dark matter, and the evolution of the universe. By overlapping the simulation data for each survey, scientists can learn how to use the best aspects of each telescope—Rubin’s broader view and Roman’s sharper, deeper vision. The combination will provide more precise and accurate insights than could be achieved from either observatory alone.",
        "url": "http://localhost:4000//science/highlights/heitmann"
      }
      ,
    
      "science-highlights-burrows": {
        "title": "State-of-the-Art High-Resolution 3D Simulations of Core-Collapse Supernovae",
        "content": "Supernovae shape the universe and life as we know it, but the physical mechanisms that cause a star to explode remain a mystery. Using ALCF supercomputing resources, a team from Princeton University has been generating one of the largest collections of 3D supernova simulations to shed light on the physics behind these cosmic events.\n\nChallenge\n\nModeling the physics of supernovae has posed a persistent challenge to astrophysicists for decades. Models must not only be shown to explode, but must reach the asymptotic state of the blast to determine many of the observables. Many relevant simulations have been developed. However, the necessary sophisticated and expensive 3D simulations typically have not run long enough after bounce to successfully capture asymptotic kick speeds. Even the few existing longer-term studies do not explore the systematics with the broad range of progenitor masses to determine the relationships between the kick speed and progenitor mass or the initial core structure.\n\nApproach\n\nThe team carried out 20 state-of-the-art 3D long-term core-collapse simulations generated using the code FORNAX. They performed these simulations on multiple supercomputers, including the ALCF’s Theta and Polaris systems. The researchers focused their study on the kicks during the simultaneous accretion and explosion phase, with attention towards the crucial first few seconds post-bounce. They then complemented their kick study with a study of the associated induced spins. The ALCF Catalyst team provided support to transition the code to Polaris (NVIDIA GPUs) and has worked with the researchers to port the FORNAX GPU version to Aurora to enable exascale simulations.\n\nResults\n\nFor the first time, using a large and uniform collection of 3D supernova models ranging from 9- to 60-solar-mass stars, the researchers asymptoted the kicks or came within 20 percent of doing so. They obtained an integrated and wide-angle perspective of the overall dependence of the recoil kicks and induced spins upon progenitor mass and their Chandrasekhar-like core structures, the latter indexed approximately by compactness. The team found that the mass and compactness of the progenitors directly correlated to the size of the neutron star’s kicks. These two classes can be correlated to the gravitational mass of the residual neutron star, which suggests the survival of binary neutron star systems may be due to their lower observed kick speeds. Their new 3D model suite provides a greatly expanded perspective and appears to explain some observed pulsar properties by default.\n\nImpact\n\nThe team’s simulations represent the largest set of long-term 3D state-of-the-art core-collapse simulations ever created. These simulations lay the groundwork for more comprehensive research that will address other aspects and outcomes of core collapse and their dependence upon progenitors. These simulations provide a qualitative picture, paving the way to develop a quantitative explanation of the survival of binary neutron star systems.",
        "url": "http://localhost:4000//science/highlights/burrows"
      }
      
    
  };
  const performance_json = {
    
      "features-aurora-performance-highlights-connectomics": {
        "title": "A Large-Scale Foundation Model for Advancing Science: AuroraGPT",
        "content": "The AuroraGPT project leverages DOE supercomputing resources to develop and enhance understanding of powerful foundation models (FMs) such as large language models (LLMs), for science. By creating FMs for science—while developing underlying capabilities, tools and workflows, data resources, and other processes and artifacts—Argonne Argonne aims to significantly improve how science is conducted, by fostering a deeper integration of AI capabilities into research workflows. To this end, Argonne’s AuroraGPT project is creating and evaluating a series of increasingly powerful FMs, each with more parameters and/or trained on more data than those that precede it, designed to assist researchers in making more informed and efficient discoveries. The AuroraGPT research program focuses on producing this sequence of models while ensuring that each provides both a scientifically useful capability and knowledge concerning scientific and computational performance to guide the design of the next model in the sequence.\n\n\nChallenge\n\nThe main tasks in the project include collecting and refining large-scale scientific datasets; building models at 8 billion to 400 billion or more parameter scales using general texts, code, and specific scientific data, and evaluating their performance on the Aurora and Polaris supercomputers; refining the models for deployment and introducing post-processing techniques such as instruct tuning and reinforcement learning for aligned chat-based interfaces; and evaluating the effectiveness of the models on scientific tasks.\n\nPerformance Results\nThe team’s software stack and frameworks have the capability to train LLMs efficiently at scale and on Polaris. The development team trained a model on 2 trillion tokens, for which, after  debugging and implementing certain code enhancements, they successfully reduced preprocessing time from approximately 1 hour to 2 minutes. Considerable effort has been put into making checkpoints convertible between the AuroraGPT software stack frameworks, particularly Megatron-DeepSpeed and Hugging Face.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\n\nAuroraGPT offers a transformative opportunity to leverage AI for scientific discovery, potentially redefining problem-solving across various domains critical to the DOE’s mission. AuroraGPT will enable Aurora users to both train LLMs from scratch and finetune existing models. A pretrained model—trained on 2 trillion tokens—is complete. The work has the potential to improve significantly how science is conducted by fostering a deeper integration of AI capabilities into research workflows.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/connectomics"
      }
      ,
    
      "features-aurora-performance-highlights-copper": {
        "title": "Cooperative Caching Layer for Scalable Parallel Data Movement in Exascale Supercomputing: Copper",
        "content": "Users of exascale computers like the ALCF’s Aurora—which feature more than 10,000 compute nodes—can experience as much as 20 minutes of idle time for a single line of code during application initialization. Copper, a scalable data loading library developed at the ALCF by an Argonne research team, addresses this problem—reducing load times by as much as 95 percent—by freeing up energy and compute resources. This is achieved without any changes to application codes, by means of a cooperative caching layer.\n\nChallenge\nThe ALCF’s file system struggled to accommodate requests commensurately with the rate at which users made them, leading to frequent crashes. The researchers responded to this I/O bottleneck by introducing a data loader capable of reducing reducing storage-side congestion: Copper, a read-only cooperative caching layer.\n\nWithout the use of algorithms, Copper reduces initialization and loading times by creating a remote procedure call (RPC) overlay network tree with local cache on each node. The root node performs the data loading from the underlying storage system and then distributes to requesting nodes over the ALCF’s Slingshot network.\n\nPerformance Results\nCopper users merely need to append a prefix to the data path without any application code changes. This simple path change has enabled scaling with near constant data-loading on many thousands of nodes of the Aurora system, as demonstrated in a paper presented at the SC24 conference. By freeing up highly contended computing resources and network infrastructure with vastly reduced I/O demands, Copper improves performance in numerous ways.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nCopper has not only drastically reduced the total time to load data and software libraries, but has also freed up such previously contended, highly demanded resources such as filesystem responders and storage network bandwidth. The savings in time and energy have enabled the ALCF to permit more science-campaign jobs with more efficient utilization of resources. Copper also provides an effective means to adopt and reuse existing software libraries developed within the DOE laboratory system. The researchers are currently assisting other exascale computing facilities with the depoloyment of Copper.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/copper"
      }
      ,
    
      "features-aurora-performance-highlights-harvey": {
        "title": "Extreme-Scale Visualization and Analysis of Fluid-Structure Interactions: HARVEY",
        "content": "HARVEY, a massively parallel computational fluid dynamics code that predicts and simulates how blood cells flow through the human body, is used to study the mechanisms driving disease development, inform treatment planning, and improve clinical care.\nA team of researchers, led by Duke University, aims to repurpose HARVE to understand metastasis in cancer better.\nOne in four deaths in the United States is due to cancer, and metastasis is responsible for more than 90 percent of these deaths. The metastatic patterns of circulating tumor cells (CTCs) are strongly influenced by both a favorable microenvironment and mechanical factors such as blood flow.\nAdvancing the use of data science to drive in situ analysis of extreme-scale fluid-structure- interaction (FSI) simulations, this work aims to leverage the ALCF’s exascale Aurora system to model and analyze the movement of CTCs through the complex geometry of the human vasculature and thereby lay the groundwork for a predictive model of cancer metastasis. Simulating the rare cells, nearby red blood cells, and underlying fluid of the arterial network presents not only a computationally challenging simulation but a large data problem for posterior analysis. Scalable and in situ analysis of massively parallel FSI models, including cellular-level flow, will be critical for enabling new scientific insights into the mechanisms driving cancer progression. \n\n\nChallenge\nHARVEY is based on the lattice Boltzmann method (LBM) for fluid dynamics. Advantages of LBM over other numerical solvers of the Navier-Stokes equations include its amenability to parallelization due to its underlying stencil structure and the local availability of physical quantities, eliminating the need for global communication among processors required of Poisson solvers. \nHARVEY adopts for execution on GPUs the MPI+X parallelization model, where X was originally OpenMP for CPUs and CUDA. The code has easily separable kernels that are optimized uniquely for each system’s architecture to take advantage of system features and avoid potential performance pitfalls for the computation of three primary components: task distribution, density and momentum vector, and collision and equilibrium relaxation. Over the last few years HARVEY has been ported to SYCL, HIP, and Kokkos to enable functionality and performance on a variety of supercomputing systems, each with di↵erent hardware. \nThe researchers have successfully built the code’s framework, on Aurora, on top of which in the Kokkos version they integrated in-situ visualization. This integration has been verified by rendering the fluid velocity field generated by the in-situ visualization of a ”fluid in a tube” test case.\n\nPerformance Results\nAs of 2024, the researchers have demonstrated HARVEY’s functionality on Aurora for two distinct cases. The first is for fluid-only calculations and the second for fluid and red-blood cell simulations. In both cases, HARVEY ran on full-body human vasculatures. The researchers successfully performed fluid-only simulations on as many 2048 Aurora. The team deployed HARVEY on 1024 nodes for red-blood cell and fluid simulations.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tScreening ~40-60B of the most synthesizable compounds made possible using the system capabilities and software stack on Aurora. * Simplified Molecular Input Line Entry System (SMILES) - Representation for Molecules.\n\t\t\t\n\t\t\n\t\n\n\nImpact\nThe exascale-optimized HARVEY application will offer the ability to create personalized models for individual patients. Blood flow simulations have the potential to greatly benefit diagnosis and treatment of patients suffering from vascular disease. By predicting where metastasized cells might travel in the body, HARVEY can help doctors anticipate early on where secondary tumors may form. Empowering models of the full arterial tree can provide insight into diseases such as arterial hypertension, while enabling the study of how local factors impact global hemodynamics.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/harvey"
      }
      ,
    
      "features-aurora-performance-highlights-phasta": {
        "title": "Machine Learning-Guided Computational Fluid Dynamics at Extreme Scales",
        "content": "This University of Colorado-led effort is aimed at developing a scalable framework to enable in situ machine learning (ML) capabilities from exascale simulations, with the goal of using high-fidelity data from direct numerical simulations (DNS) of complex, wall-bounded turbulent flows to learn accurate and generalizable sub-grid stress (SGS) models for large eddy simulation (LES). LES solves the filtered Navier Stokes equations, and therefore can be run on a coarser mesh, reducing the computational expense, however it relies on SGS models to accurately predict the effects of the unresolved scales. The project also aims to address I/O bottlenecks that are accentuated by the rise of exascale computing. Aurora enables simulations to be conducted at unprecedented scales, producing more data than ever before, making the traditional approach of naively saving the simulation data to disk to then perform training of ML models a bottleneck. By performing training of the ML model concurrently alongside the simulation, we can avoid the file system altogether and efficiently stream data between simulation and ML training modules, storing the necessary data in-memory on the compute nodes. The research team has worked to develop such a framework and ensure scalability to thousands of nodes on Aurora, and to enable efficient inferencing from within the CFD code so these ML models could be deployed in a performant manner on Aurora’s GPUs.\n\nChallenge\nThe main challenge for the online ML component was to ensure the framework which supports running a large-scale high-fidelity CFD simulation alongside distributed ML training, and the periodic transfer of training data between the two (up to terabytes in size for a single simulation snapshot when running at 2000 nodes), would scale to thousands of nodes with minimal overhead on either the simulation or the training. To do this, the development team leveraged an open-source library called SmartSim, allowing them to decouple the data transfer between simulation and training by staging the data in a database deployed on the compute nodes. In this way, the data production rate from the simulation is completely independent of the data consumption rate of the ML training, with neither component blocking the progress of the other. Additionally, the team spearheaded a co-located framework with SmartSim which places all components (simulation, training, and database) on the same set of nodes, sharing the available CPU and GPU resources. With this approach, all training data transfer between simulation and ML training is confined within a single node: even though both the MPI simulation and distributed training span thousands of nodes, the framework, in effect, is made scalable.\n\nPerformance Results\nFocusing on the performance of the online ML framework, the main results is summarized by the figure below. The relative overhead generated by CFD simulation and ML trainer communication of data to and from the database as the application scaled from 1 to 1536 nodes of Aurora at a constant level. In other words, the plots show how much the framework slows down the progress of the simulation and ML training as a fraction of the run time. Total the overhead is small, only around 5% of the runtime and, moreover, remains effectively constant independent of the number of nodes the application is run on. The framework is therefore both performant and scalable, with low and constant overhead.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nThe high-resolutions simulations of turbulent flows at high Reynolds numbers performed will help refine the turbulence models currently available, while improving the robustness and accuracy of turbine models. These models can in turn be used for improved design of aircraft and other mechanical systems.\n\nThis project provides a scalable and performant solution to performing in situ machine learning in the exascale era, thereby unlocking the ability to access much richer datasets for training ML models from scientific simulations. Additionally, accuracy and uncertainty quantification metrics obtained from the ongoing training inform the data-generation process within the simulation, which can lead to intelligent data sub-sampling techniques to train models on fewer data samples and at reduced energy cost.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/phasta"
      }
      ,
    
      "features-aurora-performance-highlights-uintah": {
        "title": "Reverse Monte-Carlo Radiative Ray Tracing Calculations at Scale: Uintah",
        "content": "The Uintah Computational Framework, an open-source asynchronous many-task (AMT) runtime system, has been modified to be performance and large-scale portable across exascale DOE supercomputing systems, including Aurora. This work is the culmination of Uintah’s decade-long University of Utah-led preparation for DOE exascale systems through highly collaborative multidisciplinary efforts pursued as a part of the Predictive Science Academic Alliance Program II and the Aurora Early Science Program. Notable updates to Uintah’s support for Kokkos were required to make this extension possible through Uintah’s adoption of a portable MPI+X hybrid parallelism approach using the Kokkos performance portability library (that is, MPI and Kokkos).\n\nChallenge\n\nTo scale application codes to hundreds and thousands of Aurora processes via the Asynchronous Many-Task (AMT) Uintah framework, the development team focused on a Reverse Monte-Carlo Ray Tracing radiation benchmark calculation, central to the University of Utah’s predictive boiler simulations. This benchmark involves potentially global all-to-all communication and uses adaptive mesh refinement and ray tracing to achieve scalability. This benchmark has been used as part of previous scalability studies on a number of pre-exascale systems and the DOE-operated exascale Frontier system while harnessing 98 percent of the machine.\n\nPerformance Results\nRunning on Aurora, the team was able to scale from 1280 nodes up to 10,240 nodes for the Burns and Christon benchmark problem on a 2-level structured adaptive mesh refinement grid with more than 129 billion cells, demonstrating that good strong-scaling efficiency is achievable.\n\nThese large-scale studies explored the impact of a larger refinement ratio to determine if aggressive mesh refinement will make full-system runs possible. The Uintah runtime systems have proved to be exceptionally capable when it comes to enabling the application to run in a scalable way on Aurora.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nThe scalability results obtained are a promising start and help show the capabilities of Uintah as an exascale AMT runtime system and pave the way for scientific simulations, such as fluid-structure interaction problems or simulations of turbulent reacting flows at unprecedented sizes on exascale machines. Additional work is underway to improve performance, and to better understand how to balance the tradeoffs between computation loads and communications costs when running at the largest scales.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/uintah"
      }
      
    
  };
  const pages_json = {
    
      "kitchen-sink": {
          "title": "Kitchen Sink",
          "content": "H1 Subhead\n\nH2 Subhead\n\nH3 Subhead\n\nH4 Subhead\n\nParagraph text, the quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. Includes italic text, bold text, highlight, strikethrough, and \tlinks.\n\n\n  Unordered list item 1\n  Unordered list item 2\n  Unordered list item 3\n\n\n\n  Ordered list item 1\n  Ordered list item 2\n  Ordered list item 3\n\n\nCode block\nvar example = example;\n\n\n\n  \n    \n      Example\n      Table\n      Table subhead\n      Table subhead\n    \n  \n  \n    \n      Value 1a\n      Value 1b\n      Value 1c\n      Value 1d\n    \n    \n      Value 2a\n      Value 2b\n      Value 2c\n      Value 2d\n    \n    \n      Value 3a\n      Value 3b\n      Value 3c\n      Value 3d\n    \n  \n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\n\t\t\t\tImage: Name A. Name\n\t\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\n\n\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\t\n\t\t\t\t\tImage: Name A. Name\n\t\t\t\t\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\t\n\t\t\n\n\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\t\n\t\t\t\t\tImage: Name A. Name\n\t\t\t\t\n\t\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\n\t\t\tCredit: Name A. Name\n\t\t\n\t\n\n\n\n\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\n\t\t\t\tCredit: Name A. Name\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\n\t\t\t\tCredit: Name A. Name\n\t\t\t\n\t\t\n\t\n\n\n\n\nExample Section with Meta Info\n\n\n\n\n\n\t\n\t\tNAME 1: \n\t\tValue 1\n\t\n\t\n\t\n\t\t\n\t\t\tNAME 2: \n\t\t\tValue 2\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tNAME 3: \n\t\t\tValue 3\n\t\t\n\t\n\n\t\n\n\t\n\n\t\n\n\n\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n\n\n\n\n\n\n\t\n\t\tNumber of things\n\t\t3k\n\t\t\n\t\t\tthings from 2022 count of stuff\n\t\t\t\n\t\n\t\n\t\t\n\t\t\tPercentage of things\n\t\t\t75%\n\t\t\t\n\t\t\t\tthings from 2022 count of stuff",
          "url": "http://localhost:4000//kitchen-sink/"
        },
  
    
      "404-html": {
          "title": "",
          "content": "404\n\n  Page not found :(\n  The requested page could not be found.",
          "url": "http://localhost:4000//404.html"
        },
  
    
      "year-in-review-about-alcf": {
          "title": "About ALCF",
          "content": "The Argonne Leadership Computing Facility (ALCF), a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory, enables breakthroughs in science and engineering by providing supercomputing and AI resources to the research community.\n\nALCF computing resources—available to researchers from academia, industry, and government agencies—support large-scale computing projects aimed at solving some of the world’s most complex and challenging scientific problems. Through awards of computing time and support services, the ALCF enables researchers to accelerate the pace of discovery and innovation across a broad range of disciplines.\n\nAs a key player in the nation’s efforts to provide the most advanced computing resources for science, the ALCF is helping to chart new directions in scientific computing through a convergence of simulation, data science, and AI methods and capabilities.\n\nSupported by the DOE’s Advanced Scientific Computing Research (ASCR) program, the ALCF and its partner organization, the Oak Ridge Leadership Computing Facility, operate leadership-class supercomputing resources that are orders of magnitude more powerful than the systems typically used for open scientific research.",
          "url": "http://localhost:4000//year-in-review/about-alcf"
        },
  
    
      "about": {
          "title": "About",
          "content": "Argonne Leadership Computing Facility\nArgonne’s Leadership Computing Facility Division operates the Argonne Leadership Computing Facility (ALCF) as part of the U.S. Department of Energy’s effort to provide leadership-class computing resources to the scientific community. The ALCF is supported by the DOE Office of Science, Advanced Scientific Computing Research (ASCR) program.\n\nArgonne National Laboratory\nArgonne is a U.S. Department of Energy Laboratory managed by UChicago Argonne, LLC, under contract DE-AC02-06CH11357. The Laboratory’s main facility is outside of Chicago, at 9700 South Cass Avenue, Lemont, Illinois 60439. For information about Argonne and its pioneering science and technology programs, visit www.anl.gov.",
          "url": "http://localhost:4000//about"
        },
  
    
      "features-ai-training": {
          "title": "Preparing a new generation of AI-ready researchers",
          "content": "Since its launch in 2021, the ALCF’s annual “Intro to AI-driven Science on Supercomputers” training series has been helping students develop the expertise to harness the power of AI and HPC to accelerate scientific discoveries and innovation.\n\nAimed at undergraduate and graduate students from U.S. universities and community colleges, the program offers a combination of hands-on sessions and expert guidance to equip attendees with the knowledge and skills to effectively use AI in their research. Over the past four years, the series has welcomed over 700 total participants.\n\nComprehensive AI for Science Curriculum\n\nThe primary goal of the series is to introduce students to the integration of AI into scientific research, preparing them for careers in both AI and HPC. The training covers essential AI concepts, including large language models (LLMs), neural networks, and AI accelerators, while also giving participants the opportunity to work directly with ALCF’s powerful systems, such as Polaris and the ALCF AI Testbed. The series is designed specifically for students with basic experience in Python, ensuring that participants gain the skills necessary to apply AI in their research. By providing this foundational knowledge, the training helps participants bridge the gap between theoretical AI concepts and practical application using world-class computational resources.\n\nThe training series covers fundamental AI concepts and introduces students to the advanced computational capabilities at Argonne, such as the Polaris supercomputer and the ALCF AI Testbed. With weekly sessions led by ALCF AI experts, the program offers detailed instruction on topics like large language models, neural networks, model training, and AI accelerators. In addition, attendees learn about the infrastructure necessary to run large-scale AI models efficiently, from computational strategies to optimizing workloads on advanced hardware systems like GPUs and novel AI accelerators.\n\nEach session concludes with a science talk, delivered by different domain scientists, offering insights into how various Argonne teams are using AI to drive advancements across fields such as biology, cosmology, and materials science. These talks not only provide context for how AI is applied but also highlight the potential of supercomputing resources in accelerating science and engineering.\n\nA key feature of the series is its hands-on learning approach, where participants apply their new skills using ALCF computing resources. Each session includes exercises that reinforce key concepts, while also providing a platform for students to explore AI applications in science. At the end of the program, participants who complete all exercises receive a certificate of completion and a digital badge, marking their achievements.\n\nExpanding Reach and Impact\n\nThe ALCF training series continues to expand its reach, with over 250 students registered in 2024 alone. By offering free access to advanced tools and resources, the program plays a vital role in developing the next generation of AI researchers and practitioners.\n\nIn addition to the live sessions, ALCF has made all training materials and session recordings publicly available on YouTube and GitHub. This ensures that even students who cannot attend live sessions have access to the curriculum and can continue their education at their own pace. By broadening access to these valuable resources, the training series empowers students nationwide to gain practical, real-world experience with AI and HPC.\n\nThrough its commitment to education and the development of an AI-ready workforce, the ALCF’s training series is playing a vital role in shaping the future of AI-driven science. By preparing students to leverage the power of AI and supercomputers, the program is empowering a new generation of researchers who will drive scientific breakthroughs and innovations in the years to come.\n\nAttendees who complete all in-class and post-class exercises by the end of the series receive a certificate of completion and a digital badge.",
          "url": "http://localhost:4000//features/ai-training"
        },
  
    
      "features-alcf-ai-testbed": {
          "title": "ALCF Continues to Expand AI Testbed Systems Deployed for Open Science",
          "content": "In 2024, the ALCF AI Testbed was upgraded with the deployment of SambaNova Suite and established benchmarks for its optimized Cerebras Wafer-Scale Cluster WSE-2 that promise to make extreme-scale AI computing substantially more manageable and effective.\n\nThe testbed is a growing collection of some of the world’s most advanced AI accelerators available for open science. Designed to enable researchers to explore next-generation machine learning applications and workloads to advance AI for science, the systems are also helping the facility to gain a better understanding of how novel AI technologies can be integrated with traditional supercomputing systems powered by CPUs and GPUs. With the AI Testbed, the ALCF user community can leverage novel AI technologies for innovative research projects involving large language models (LLMs), large-scale data analysis, and the development of trustworthy AI.\n\nThe testbed’s AI accelerators are equipped with unique hardware and software features to efficiently handle a variety of AI tasks, including:\n\n\n  AI Model Training: Using large datasets to “teach” an AI model to detect patterns and make accurate, trustworthy predictions.\n  Inference: Employing a trained AI model to make predictions on new data.\n  Large Language Models (LLMs): AI models that are trained on large amounts of text data to understand, generate, and predict text-based content.\n  Computer Vision Models: AI models that are trained to understand and analyze visual data for tasks such as image classification and object recognition.\n  Foundation Models: Similar to LLMs, these AI models are trained on diverse datasets to perform a broad set of processing tasks. Foundation models, however, can serve as a starting point for developing more specialized AI models for specific domains or applications.\n\n\nThese methods are powerful tools for speeding up scientific progress. Computer vision models can help scientists automate the analysis of images generated by microscopes, x-ray light sources, and other imaging techniques. LLMs, on the other hand, are helping researchers to sift through massive amounts of published scientific data quickly to identify promising materials for medicines, batteries, and other uses.\n\nExperimental data analysis also benefits from the lab’s AI Testbed. Researchers from Argonne’s Advanced Photon Source (APS) are exploring how different accelerators can enable fast, scalable AI model training and inference to accelerate the analysis of x-ray imaging data. Rapid data analysis methods are becoming increasingly important for the APS and other experimental facilities as data generation rates continue to grow.\n\nSambaNova\nThe ALCF AI Testbed expanded its SambaNova platform with the addition of SambaNova Suite. SambaNova Suite, powered by SambaNova DataScale SN40L systems, is a fully integrated hardware-software platform that enables users to train, fine tune, and deploy AI workloads. Optimized for low-latency, high-throughput inference, the platform provides scientists with a new AI resource to accelerate scientific research.\n\nThe deployment of the DataScale SN40L system extends advanced AI inference capabilities beyond the ALCF’s traditional ALCF user base. By making trained AI models more accessible, the platform enables a wider community of researchers to explore new directions in generative and agentic AI workloads for science and engineering.\n\nBeing able to rapidly evaluate AI models and adjust parameters for improved performance is crucial for driving progress in AI-driven science across many research areas, including drug discovery, materials identification, and brain mapping.\n\nThe ALCF’s platform contains sixteen of SambaNova’s Reconfigurable DataFlow Units (RDU). The system’s capabilities support the development of large foundation models like Argonne’s AuroraGPT, which is being built to enable autonomous scientific exploration across disciplines, including biology, chemistry, and materials science. AuroraGPT is being trained on Argonne’s Aurora exascale system.\n\nThe ability to switch between different AI models instantly and fine-tune them using domain-specific datasets can help streamline the process of testing and validating their performance.\n\nThe system also gives the lab a new platform to continue its explorations into energy-efficient technologies for next-generation supercomputers and data centers, as one of the aims of the ALCF AI Testbed is to determine how novel AI accelerators like the SN40L can be integrated with future supercomputers to enhance energy efficiency.\n\nCerebras\nThe Cerebras system, previously upgraded to a Wafer-Scale Cluster WSE-2, optimized the ALCF’s existing Cerebras CS-2 system to include two CS-2 engines, enabling near-perfect linear scaling of large language models (LLMs). This capability helps make extreme-scale AI substantially more manageable.\n\nAn Argonne-led research team examined the feasibility of performing continuous energy Monte Carlo (MC) particle transport on the Cerebras WSE-2—simulations with the potential to fill in crucial gaps in experimental and operational nuclear reactor data.\n\nThe researchers ported a key kernel from the MC transport algorithm to the Cerebras Software Language programming model and evaluated the performance of the kernel on the Cerebras WSE-2. The team developed an architecture-specific optimization to leverage the capabilities of the WSE-2 and a highly optimized CUDA kernel for testing on a conventional graphics processing unit (GPU), which served as a baseline to contextualize the WSE-2’s performance.\n\nA single WSE-2 was found to run 130 times faster than the highly optimized CUDA version of the kernel deployed on the conventional GPU comparison—significantly outpacing expected performance increase, given the difference in transistor counts between the architectures. A follow-up study saw the WSE-2 achieve a 182x speedup over the GPU.\n\nThe team’s analysis suggests the potential for a wide variety of complex and irregular simulation methods to be mapped efficiently onto AI accelerators like the Cerebras WSE-2, providing users with an invaluable tool for effective scientific discovery.",
          "url": "http://localhost:4000//features/alcf-ai-testbed"
        },
  
    
      "science-allocation-programs": {
          "title": "Accessing ALCF Resources for Science",
          "content": "Researchers gain access to ALCF systems for computational science and engineering projects through competitive, peer-reviewed allocation programs supported by the DOE and Argonne.\n\nThe ALCF also hosts competitive, peer-reviewed application programs designed to prepare key scientific applications and innovative computational methods for the architecture and scale of DOE supercomputers.\n\nAllocation Programs\n\nINCITE\nThe Innovative Novel Computational Impact on Theory and Experiment (INCITE) program aims to accelerate scientific discoveries and technological innovations by awarding ALCF computing time and resources to large-scale, computationally intensive projects that address grand challenges in science and engineering.\n\nALCC\nThe ASCR Leadership Computing Challenge (ALCC) program allocates ALCF computing resources to projects that advance the DOE mission; help to broaden the community of researchers capable of using leadership computing resources; and serve the national interests for scientific discovery, technological innovation, and economic competitiveness.\n\nDirector’s Discretionary\nDirector’s Discretionary projects are dedicated to leadership computing preparation, INCITE and ALCC scaling, and efforts to maximize scientific application efficiency and productivity on leadership computing platforms.\n\nESP\nAs part of the process of bringing a new supercomputer into production, the ALCF conducts its Early Science Program (ESP) to prepare applications for the architecture and scale of a new system. ESP projects represent a typical system workload at the ALCF and cover key scientific areas and numerical methods.",
          "url": "http://localhost:4000//science/allocation-programs"
        },
  
    
      "features-aurora": {
          "title": "Aurora Nears Full Deployment",
          "content": "In 2024, the ALCF made significant progress toward the full deployment of its Aurora exascale supercomputer for scientific research. After completing extensive system validation, verification, and scale-up efforts, Aurora is undergoing acceptance testing in December 2024, with plans to enter full production mode in January 2025.\n\nOnce operational, it will provide the research community with powerful simulation, AI, and data analysis capabilities to drive breakthroughs in physics, engineering, materials science, and other domains.\n\nBreaking the Exascale Barrier\n\nOver the course of the year, Aurora demonstrated its capabilities in various performance benchmarks that solidified its place among the world’s most powerful supercomputers. The ALCF system officially broke the exascale barrier in June, achieving 1.012 exaflops on the High Performance LINPACK (HPL) benchmark. Aurora also set a new record for AI performance, registering 11.6 exaflops on the HPL-MxP mixed-precision benchmark. Its strengths in data-intensive applications were further highlighted with leading results on the Graph500 and HPCG benchmarks, while its storage system, DAOS, retained the top ranking on the IO500 production list.\n\nTogether with Oak Ridge National Laboratory’s Frontier and Lawrence Livermore National Laboratory’s El Capitan, DOE is now home to the world’s first three exascale systems. These machines not only mark the first to reach exascale but are also the three fastest supercomputers on the TOP500 List.\n\nBuilt in partnership with Intel and HPE, Aurora’s architecture represents a first-of-its-kind deployment, integrating cutting-edge technologies at an unprecedented scale. Equipped with 63,744 GPUs and 84,992 network endpoints, the system is designed to tackle complex computational challenges in ways that were previously unimaginable.\n\n\n\n\n\t\n\t\t\n\t\n\t\n\t\tQuick brown fox\n\t\t\n\t\t\tCredit: Argonne National Laboratory\n\t\t\n\t\n\n\nWorld-Class Simulation, AI, and Data Capabilities\n\nAurora’s computing power and advanced capabilities are expected to transform research across a wide range of scientific domains. Ahead of the system’s deployment, teams participating in DOE’s Exascale Computing Project (ECP) the ALCF’s Aurora Early Science Program (ESP) have demonstrated its potential in training large-scale AI models and carrying out extreme-scale modeling and simulation campaigns.\n\nOne key target involves the development of AI-driven scientific models that can accelerate discovery across multiple disciplines, including materials design, drug development, and energy research. The system is also being prepared to support high-fidelity simulations of complex systems, such as the human circulatory system, nuclear reactors, and supernovae, to gain new insights into their behavior. Additionally, its capacity to process massive datasets will be critical for analyzing the growing data streams from large-scale research facilities such as Argonne’s Advanced Photon Source and CERN’s Large Hadron Collider.\n\nPreparing for Science on Day One\n\nBringing a system of this scale online has required close collaboration among the ALCF, Intel, HPE, and researchers from the DOE’s Exascale Computing Project and Aurora Early Science Program. Throughout 2024, these teams worked to optimize codes and stress-test the system, ensuring it would be ready for science from day one of production.\n\nA co-design approach was essential in this effort, with hardware and software developed in tandem to maximize performance and usability. As part of this process, researchers ran early science applications to fine-tune their software for Aurora’s architecture, resulting in a suite of computational tools that will be ready to accelerate discoveries as soon as the system becomes fully operational.\n\nWith acceptance testing underway and final preparations in progress, Aurora is poised to take its place at the forefront of scientific computing in January 2025. As it moves into production, it will provide researchers with a transformative platform for tackling some of the most ambitious challenges in science and engineering.",
          "url": "http://localhost:4000//features/aurora"
        },
  
    
      "credits": {
          "title": "Credits",
          "content": "ALCF Leadership: Michael E. Papka (Division Director), Bill Allcock (Director of Operations), Susan Coghlan (ALCF-X Project Director, Kalyan Kumaran (Director of Technology), Jini Ramprakash (Deputy Division Director), and Katherine Riley (Director of Science)\n\nEditorial Team: Beth Cerny, Jim Collins, Nils Heinonen, Logan Ludwig, and Laura Wolf\n\nDesign and Production: Sandbox Studio, Chicago",
          "url": "http://localhost:4000//credits"
        },
  
    
      "year-in-review-directors-letter": {
          "title": "Director’s Letter",
          "content": "The process of planning for and installing a supercomputer takes years. It includes a critical period of stabilizing the system through validation, verification, and scale-up activities, which can vary for each machine. However, unlike ALCF’s previous or current production machines, Aurora’s long ramp-up journey has also included several configuration changes and COVID-related supply chain issues.\n\nAurora is a highly advanced system designed for various AI and scientific computing applications. It will also be used to train a one-trillion-parameter large language model for scientific research. Aurora’s architecture boasts more endpoints in the interconnect technology than any other system, and it has over 60,000 GPUs, making it the system with the largest number of GPUs in the world.\n\nIn 2023, ALCF made significant progress toward realizing Aurora’s full capabilities. In June, Aurora completed the installation of its 10,624th and final blade. Shortly after, Argonne shared the results of benchmarking runs for about half of Aurora to the TOP500. These results were used in the November announcement of the world’s fastest supercomputers, where Aurora secured the second position. Once the full system goes online, its theoretical peak performance is expected to be approximately two exaflops.\n\nSome application teams participating in the DOE’s Exascale Computing Project and the ALCF’s Aurora Early Science Program have begun using Aurora to scale and optimize their applications for the system’s initial science campaigns. Soon to follow will be all the early science teams and an additional 24 INCITE research teams in 2024.\n\nThis new exascale machine brings with it some more big changes. Theta, one of ALCF’s production systems, was retired on December 31, 2023. ThetaGPU will be decoupled and reconfigured to become a new system named Sophia, which will be used for AI development and as a production resource for visualization and analysis. Meanwhile, the ALCF AI Testbed will continue to make more production systems available to the research community.\n\nFor more than three decades, researchers at Argonne have been developing tools and methods that connect powerful computing resources with large-scale experiments, such as the Advanced Photon Source and the DIII-D National Fusion Facility. Their work is shaping the future of inter-facility workflows by automating them and identifying ways to make these workflows reusable and adaptable for different experiments. Argonne’s Nexus effort, in which ALCF plays a key role, offers the framework for a unified platform to manage high-throughput workflows across the HPC landscape.\n\nIn the following pages, you will learn more about how Nexus supports the DOE’s goal of building a broadscale Integrated Research Infrastructure (IRI) that leverages supercomputing facilities for experiment-time data analysis. The IRI will accelerate the next generation of data-intensive research by combining scientific facilities, supercomputing resources, and new data technologies like AI, machine learning, and edge computing.\n\nIn 2023, we continued our commitment to education and workforce development by organizing a number of informative learning experiences and training events. As part of this effort, ALCF staff members led a pilot program called “Introduction to High-Performance Computing Bootcamp” in collaboration with other DOE labs. This was an immersive program designed for students in STEM to work on energy justice projects using computational and data science tools learned throughout the week. In a separate effort, the ALCF worked on developing the curriculum for its “Intro to AI-Driven Science on Supercomputers” training course, with the aim of adapting the content to introduce undergraduates and graduates to the basics of large language models for future course offerings.\n\nTo conclude, I express my sincere gratitude to the exceptional staff, vendor partners, and program office, who have all contributed to making ALCF one of the leading scientific supercomputing facilities in the world. Each year, we take the time to share our numerous achievements with you in our Annual Report, and while there are many more exciting changes on the horizon, I truly appreciate this opportunity.",
          "url": "http://localhost:4000//year-in-review/directors-letter"
        },
  
    
      "disclaimer": {
          "title": "Disclaimer",
          "content": "This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor UChicago Argonne, LLC, nor any of their employees or officers, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of document authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof, Argonne National Laboratory, or UChicago Argonne, LLC.",
          "url": "http://localhost:4000//disclaimer"
        },
  
    
      "community-and-outreach-educational-outreach-activities": {
          "title": "Inspiring Students",
          "content": "ALCF Student Summer Program\nEvery summer, the ALCF opens its doors to a new class of student researchers who work alongside staff mentors to tackle research projects that address issues at the forefront of scientific computing. In 2024, facility hosted more than 40 students ranging from high school seniors to Ph.D. candidates. Students contributed to ALCF projects spanning from scaling up deep learning benchmark applications for exascale computing to compressing data for AI models that can give us insights into nuclear fusion, and had hands-on experience with some of the most advanced computing technologies in the world.  For a recap of the 2024 program, read the article on our website.\n\nBig Data Camp\nArgonne’s annual Big Data Camp introduced high school juniors and seniors to the advanced tools used by professional data scientists. Campers were required to have coding experience and learned techniques for probing and analyzing massive scientific datasets, such as the dataset from the Array of Things (AoT) urban sensor project. Participants were also introduced to the foundational concepts underlying artificial intelligence and machine learning and why supercomputers are so important to use these tools effectively. This camp was organized by Argonne’s Educational Programs and Outreach staff and taught by ALCF scientists and visualization experts, hosting 12 participants in 2024.  For more details, visit the camp website.\n\nBreakthrough Tech Sprinternship Program\nALCF participated for the first time in the Breakthrough Tech Sprinternship program, run in partnership with the University of Illinois Chicago. The program offers quick and immersive micro-internships designed to support computer science undergraduates who identify as women and non-binary. ALCF hosted a team of 5 students for a 3-week program in which the developed a program to more effectively track allocations awards and their impact on scientific publications. All students were offered an extension to their sprinternship through the end of Summer 2024, and 4 of the 5 students accepted the extension.\n\nCodeGirls@Argonne Camp\nThe annual CodeGirls@Argonne Camp hosts sixth- and seventh-grade girls each summer for a five-day event dedicated to teaching them the fundamentals of coding. The camp highlights the essential role that women have played throughout history in technology development and invites lab researchers to talk to the students and share how they turned their coding interests into careers. The girls in the camp build friendships as they work together to solve coding-related challenges. For a recap on the 2024 event, read the article on our website.\n\nCoding for Science Camp\nAimed at high school students, Argonne’s Coding for Science Camp features programming activities that link computational science with current scientific challenges. Over the course of the week-long camp, the students enhance their problem-solving and teamwork skills through hands-on activities, while discovering how computing is useful and often essential to solving problems in science. They also get an opportunity to interact with Argonne staff members to explore the diverse career pathways that need coding skills. For a recap on the 2024 event, read the article on our website.\n\nCSEdWeek/Hour of Code\nAs part of the national Computer Science Education Week (CSEdWeek) and the Hour of Code in December, ALCF staff members provide in-person and virtual talks and demos to Chicago area schools to spark interest in computer science. Working with students in classes from elementary to high school, the volunteers led a variety of activities designed to teach the basics of coding. CSEdWeek was established by Congress in 2009 to raise awareness about the need to elevate computer science education at all levels.\n\nIntroduce a Girl to Engineering Day\nALCF staff members regularly serve as mentors and volunteers for Argonne’s Introduce a Girl to Engineering Day (IGED) program. The annual event gives eighth-grade students a unique opportunity to discover engineering careers alongside Argonne’s world-class scientists and engineers. Participants hear motivational presentations by Argonne engineers, tour the lab’s cutting-edge research facilities, connect with mentors, engage in hands-on engineering experiments, and compete in a team challenge.\n\nIntroduction to AI-driven Science on Supercomputers: A Student Training Series\nThe AI-driven Science on Supercomputers 7-8-week webinar series is aimed at undergraduate and graduate students enrolled at U.S. universities and community colleges and designed to attract a new generation of AI users by having a low entry barrier; that is, attendees need to have only a basic experience with the Python programming language as the pre-requisite. This year’s focus was on introducing participants to foundational concepts in AI and ML and then diving into the fundamentals of large language models. ALCF computer scientists led the weekly sessions and hands-on exercises along with talks by Argonne scientists who use AI in their research. The programs run in 2024 trained just over 500 attendees from over 100 universities from the U.S. and beyond, including undergraduates, graduate students, postdocs, and faculty from fields spanning computer science to materials science to biology. For a recap of the 2024 event, read the article on our website.\n\nScience Careers in Search of Women\nALCF staff members continued to contribute to Argonne’s annual Science Careers in Search of Women (SCSW) conference. The event hosts female high school students for a day of inspiring lectures, facility tours, career booth exhibits, and mentoring. SCSW provides participants with the unique experience to explore their desired profession or area of interest through interaction with Argonne’s women scientists and engineers. For a recap of the 2024, read the article on Argonne’s website.",
          "url": "http://localhost:4000//community-and-outreach/educational-outreach-activities"
        },
  
    
      "science-highlights": {
          "title": "2024 Science Highlights",
          "content": "Biological Sciences\n  \t\tMProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization\n      PI: Arvind Ramanathan, Argonne National Laboratory \n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Energy Technologies\n  \t\tAssessment of Turbulent Prandtl Number for Heavy Liquid Metal Flow in a Bare Rod Bundle\n      PI: Yiqi Yu, Emily Shemon, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Computer Science\n  \t\tDemonstrating Cross-Facility Data Processing at Scale with Laue Microdiffraction\n      PI: Michael Prince, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Computer Science\n  \t\tEfficient Algorithms for Monte Carlo Particle Transport on AI Accelerator Hardware\n      PI: John Tramm, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Engineering\n  \t\tRobust Gas Turbine Film Cooling Under Manufacturing Uncertainty for Improved Jet Engine Lifecycle Energy Efficiency\n      PI: Pinaki Pal and Muhsin Ameen, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Engineering\n  \t\tFrom Quantum Mechanics to Hypersonic Aerothermodynamics\n      PI: Maninder Grover, University of Dayton Research Institute\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Engineering\n  \t\tThree-Dimensional Shock Boundary Layer Interactions over Flexible Walls\n      PI: Johan Larsson, University of Maryland; Ivan Bermejo-Moreno, University of Southern California\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Physics\n  \t\tExascale Gyrokinetic Study of ITER Challenge on Power-Exhaust and ELM-Free Edge\n      PI: Choongseok Chang, Princeton Plasma Physics Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Physics\n  \t\tSimulating the Cosmos for the Roman and Rubin Telescopes\n      PI: Katrin Heitmann, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Physics\n  \t\tState-of-the-Art High-Resolution 3D Simulations of Core-Collapse Supernovae\n      PI: Adam Burrows, Princeton University",
          "url": "http://localhost:4000//science/highlights"
        },
  
    
      "community-and-outreach-hpc-community-activities": {
          "title": "Shaping the Future of Supercomputing",
          "content": "Computing Conferences and Events\n\nALCF researchers regularly contribute to some of the world’s leading computing conferences and events to share their latest advances in areas ranging from computational science and AI to HPC software and exascale technologies. In 2024, Argonne staff participated in a wide range of events including SC24, ISC High Performance, Grace Hopper Celebration, SIAM Conference on Computational Science and Engineering, Richard Tapia Celebration of Diversity in Computing Conference, IEEE International Parallel &amp; Distributed Processing Symposium, International Conference on Parallel Processing, International Symposium on Cluster, Cloud and Grid Computing, International Workshop on OpenCL and SYCL, Platform for Advanced Scientific Computing Conference, HPC User Forum, Energy High-Performance Computing Conference, Lustre User Group Conference, Intel eXtreme Performance Users Group Conference, Conference on Machine Learning and Systems, American Physical Society, Energy HPC, High-Performance Computing Security, International Conference On Preconditioning Techniques For Scientific and Industrial Applications, IEEE Conference on Artificial Intelligence, IRI/HPDF Meeting, PEARC 2024, NIST Artificial Intelligence for Materials Science (AIMS) Workshop, ADAC Workshop, ISO C++ Conference, and more.\n\nHPC Standards and Community Groups\n\nALCF staff members remain actively involved in several HPC standards and community groups that help drive improvements in the usability and efficiency of scientific computing tools, technologies, and applications. Staff activities include contributions to the C++ Standards Committee, Cray User Group, DAOS Foundation, HPC User Forum, HPSF High Performance Software Foundation, Intel eXtreme Performance Users Group (IXPUG), Khronos OpenCL and SYCL Working Groups , LDMS User Group, UXL Silver Member/Steering Member, MLCommons, NITRD Middleware and Grid Infrastructure Team, Open Fabrics Alliance, Open Scalable File Systems (OpenSFS) Board, OpenMP Architecture Review Board, OSTI ORCiD Consortium Membership, SPEC (Standard Performance Evaluation Corporation) HPG (High Performance Group), Better Scientific Software, Energy Efficient High Performance Computing, Unified Communications Framework, OCHAMI, and more.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tQuick brown fox.\n\t\t\t\n\t\t\n\t\n\n\nPerformance Portability\n\nThe ALCF continued its collaboration with NERSC and OLCF to operate and maintain a website dedicated to enabling performance portability across the DOE Office of Science HPC facilities. The website serves as a documentation hub and guide for applications teams targeting systems at multiple computing facilities. The DOE computing facilities staff also collaborate on various projects and training events to maximize the portability of scientific applications on diverse supercomputer architectures.\n\nVendor Collaborations\n\nThe ALCF works closely with many companies in the HPC and AI industries to develop and deploy cutting-edge hardware and software for the research community. This includes collaborating with Intel and HPE to deliver the Aurora exascale system, working with HPE to deploy the Polaris testbed supercomputer, and partnering with NVIDIA on system enhancements and training related to ThetaGPU. Such partnerships are critical to ensuring the facility’s supercomputing resources meet the requirements of the scientific computing community. In addition, the ALCF is working with several AI start-up companies, including Cerebras, Graphcore, Groq, and SambaNova, to deploy a diverse set of AI accelerators as part of the ALCF AI Testbed. The testbed is playing a key role in determining how AI accelerators can be applied to scientific research, while also allowing vendors to prepare their software and hardware for scientific AI workloads.",
          "url": "http://localhost:4000//community-and-outreach/hpc-community-activities"
        },
  
    
      "": {
          "title": "Argonne Leadership Computing Facility",
          "content": "&#9650; Researchers from the University of Dayton Research Institute and the Air Force Research Laboratory are using ALCF supercomputers to shed light on the complex thermal environment that hypersonic vehicles encounter. This image shows the contour of gas velocity along an experimental test article.\n\t\tImage: ALCF Visualization and Data Analytics Team, Air Force Research Laboratory, and University of Dayton Research Institute\n\t\n\n\n\n \n\n\n\t\n\t\tFeatures\n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tAurora Nears Full Deployment\n\t\t\t    \t\tThe system surpassed the exascale barrier, demonstrated its world-class AI capabilities, and completed critical preparations for its release to the research community in 2025.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tAurora Performance Highlights\n\t\t\t    \t\tWith Aurora fully assembled, early science team members began running and optimizing their applications to prepare the system for open access. Here are some early performance results on Aurora.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tLinking Experimental Facilities and Leadership Computing\n\t\t\t    \t\tArgonne researchers are working to advance the DOE’s vision by integrating experimental facilities with ALCF computing resources.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tALCF Continues to Expand AI Testbed Systems Deployed for Open Science\n\t\t\t    \t\tBy updating and expanding its testbed for AI accelerators, the ALCF enables users to harness leading-edge AI technologies for efficient and impactful scientific discovery.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tPreparing a new generation of AI-ready researchers\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\t\n\t\t\t\n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tScience Highlights\n\t\t\t    \t\tIn 2023, scientists from across the world used ALCF supercomputing and AI resources to accelerate discovery and innovation across a wide range of research areas. The following science highlights detail some of the groundbreaking research campaigns carried out by ALCF users over the past year.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\n\t\n\n\n\n\n\n\t\n\t\tInsights\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog. Credit: Argonne National Laboratory\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog. Credit: Argonne National Laboratory\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog. Credit: Argonne National Laboratory",
          "url": "http://localhost:4000//"
        },
  
    
      "community-and-outreach-industry": {
          "title": "Powering breakthroughs for U.S. industry",
          "content": "Through collaborations with industry partners, the ALCF provides companies with access to powerful computing resources that drive technological advancements, accelerate product development, and strengthen the nation’s innovation infrastructure.\n\nThe ALCF’s Industry Partnerships Program enables companies of all sizes—from startups to Fortune 500 corporations—to leverage leadership computing resources and expertise. These collaborations help industry partners to tackle R&amp;D challenges that exceed the capabilities of traditional computing systems.\n\nWith state-of-the-art capabilities for simulation, AI, and data analysis, ALCF computing resources help researchers create higher-fidelity models, achieve more accurate predictions, and quickly process massive datasets. As a result, companies can accelerate breakthroughs, reduce uncertainties, and minimize the need for costly prototypes.\n\nThe ALCF continues to expand its industry outreach efforts by partnering with other Argonne divisions and user facilities, including the Science and Technology Partnership Outreach (STPO) division. This integrated approach provides a more comprehensive understanding of the laboratory’s capabilities and promotes broader, more impactful partnerships.\n\nThe ALCF offers several avenues for industry partners to access its computing resources. These include the INCITE, ALCC, and Director’s Discretionary allocation programs, which provide researchers with opportunities to apply for computing time and collaborate on high-impact projects.\n\nFor companies focused on addressing energy-related challenges in manufacturing or materials development, DOE’s High Performance Computing for Energy Innovation (HPC4EI) program is another available option. HPC4EI provides access to DOE’s world-class supercomputing resources through two subprograms: HPC4Mfg, which focuses on optimizing manufacturing processes, and HPC4Mtls, which supports the development of advanced materials that perform well in harsh or complex environments.\n\nThrough these programs, the ALCF helps U.S. industries stay competitive in an evolving global market. By providing access to HPC and AI computing resources and expertise, these partnerships enable companies to tackle complex challenges, accelerate the R&amp;D process, and drive technological innovation. Contact us at industry@alcf.anl.gov to explore potential collaboration opportunities with the ALCF.\n\nHere are some examples of how ALCF resources are helping companies to advance their R&amp;D efforts.\n\nMSBAI\nResearchers at MSBAI are using DOE’s exascale supercomputers as part of an ALCC project aimed at enhancing GURU, an autonomous system designed to reduce modeling and simulation setup time from hours to minutes for a range of engineering applications. The team will use ALCF’s Aurora and OLCF’s Frontier to drive advancements in hybrid interaction, skills agent learning, and geometry search and synthesis – core components of GURU’s AI-driven workflow automation. By addressing key barriers in simulation and CAD data handling, the project aims to accelerate product development across industries such as energy, automotive, aerospace, and medical devices, while democratizing access to HPC resources.\n\nRTX Technology Research Center\nWith support from DOE’s HPC4EI program, researchers from RTX Technology Research Center are working with Argonne to develop reduced-order deep learning surrogate models to capture the impact of manufacturing uncertainties on the performance of film cooling schemes used for thermal management of aviation gas turbines. Reliable film cooling drives durability and thermal efficiency in gas turbine engines but is greatly sensitive to variations in the shape of cooling holes due to surface roughness induced by the manufacturing process. To this end, the team leveraged ALCF supercomputers and Argonne’s highly scalable nekRS solver to perform morphology-resolved computational fluid dynamics simulations of gas turbine film cooling schemes incorporating surface roughness of the cooling hole. Detailed analysis of the numerical results was carried out to gain insights into the impact of various surface roughness parameters on the downstream film cooling effectiveness. High-fidelity datasets from these simulations will be combined with data from coarse-grained simulations to develop multi-fidelity deep learning surrogate models to predict the impact of surface roughness on film cooling effectiveness. The team’s framework aims to help the company improve the fuel efficiency and durability of aircraft engines while reducing design times and costs.\n\nTAE Technologies\nTAE Technologies is leveraging ALCF supercomputers to advance fusion energy research and accelerate the development of a commercially viable fusion-based electricity generator. By running high-fidelity simulations, researchers are gaining critical insights into plasma stability and the conditions needed to sustain fusion reactions. This work supports the development of field-reversed configuration plasma and neutral beam injection techniques, key to achieving the extreme temperatures required for fusion with proton-boron-11 fuel—a safer, low-waste alternative to conventional fusion methods. These advancements are accelerating the design of compact, efficient fusion reactors, moving the world closer to a transformative, near-limitless energy source.",
          "url": "http://localhost:4000//community-and-outreach/industry"
        },
  
    
  
    
      "features-nexus-iri": {
          "title": "Linking Experimental Facilities and Leadership Computing",
          "content": "As the volume of data generated by large-scale experiments continues to grow, the need for rapid data analysis capabilities is becoming increasingly critical to new discoveries.\n\nArgonne’s Advanced Photon Source Upgrade project (APS-U), for example, completed in 2024, increased the brightness of APS x-rays by as much as 500 times. With a corresponding increase in the amounts of experimental data generated, to quickly process and analyze the results requires the use of high-performance computing (HPC) systems.\n\nNumerous ALCF activities and achievements have helped realize the DOE effort to build an Integrated Research Structure (IRI) that seamlessly connects experimental facilities with its world-class supercomputing resources, including:\n\n\n  Developing and testing methods to closely integrate supercomputers and experiments for near-real-time data analysis.\n  Partnering with Pathfinder projects to advance plasma physics and fusion energy research.\n  Participating in leadership groups and technical subcommittees dedicated to the design and implementation of computing facility functionality useful for experimentalists.\n\n\nNexus\nUnder Argonne’s Nexus effort, Argonne researchers are working to advance the DOE’s vision by integrating experimental facilities with ALCF computing resources.\n\nAs IRI aims to deliver DOE-enterprise-wide infrastructure for computing, ALCF has continued its commitment to linking experimental facilities with ALCF computing. Work with APS over recent years has been a primary driver for defining new functionality and services ALCF has deployed to satisfy experiment-time computing needs at APS beamlines. Service accounts enable APS users to leverage automated analysis of their data at ALCF in a shared environment in a streamlined fashion throughout their multi-day beamline campaigns, with jobs running immediately at experiment time in the on-demand queue on Polaris. Analysis results are available to scientists at the beamline via Globus Sharing enabled on the Eagle filesystem, at the time of experiment and post-hoc. Building on these ALCF-deployed features, Globus Compute and Globus Flows manage application execution and data transfer in a frictionless manner, for projects across the DOE-SC program offices.\n\nFacility Integration\n\nFor over a decade, the ALCF and the APS have been collaborating to build the infrastructure for integrated ALCF-APS research, including the development of workflow management tools and enable secure access to on-demand computing.\n\nWith the upgraded APS providing x-rays up to 500 times brighter than before with, the APS-ALCF collaboration is providing increased computational power at experiment time. More than 20 beamlines housed at the APS identified significant computing needs and have engaged the full power of ALCF’s Nexus services and functionality, using service accounts for transparent access to ALCF, and the demand queue for time-sensitive analysis of beamline data through integration with the APS Data Management system. With more beamlines coming online with ever greater computational needs, the APS demand for ALCF supercomputing resources and newly upgraded inter-facility network connectivity will continue to grow.\n\nWorking with a team at the Lawrence Berkeley National Laboratory Advanced Light Source, ALCF staff have helped to automate analysis of data from a tomography beamline on Polaris. Using a service account to submit jobs to Polaris through Globus Compute and the demand queue to analyze data at experiment time, the team has moved beyond an initial prototype and is now able to run analysis in a dedicated discretionary allocation. This production-ready capability is planned to be used in upcoming beamline experiments.\n\nExpanding and Demonstrating Capabilities\n\nIn a recent achievement of facility integration for near-real-time data analysis, Argonne deployed a fully automated pipeline that uses ALCF resources to rapidly process data obtained from the x-ray experiments at the APS.\n\nTo demonstrate the capabilities of the pipeline, Argonne researchers carried out a study focused on a technique called Laue microdiffraction, which is employed at the APS and other light sources to analyze materials with crystalline structures. The team used the ALCF’s Polaris supercomputer to reconstruct data obtained from an APS experiment, returning reconstructed scans to the APS within 15 minutes of them being sent to the ALCF. The beamline technique introduced in the study allows users to collect data about 10 times faster than was previously possible.\nThese results carry implications for future software development, engineering, and beamline science.\nArgonne researchers showcased the use of the Polaris system for processing data from APS experiments in near-real time during a demonstration at the SC24 conference.\n\nAdditional experiments and papers presented at the SC24 XLOOP workshop explored multiple IRI-related issues, including the scaling capabilities of file-based reconstruction of ptychography data—which requires particularly short data-processing turnaround times. New scans on the APS beamline storage system were automatically transferred to ALCF’s Eagle file system through Globus using the file-based workflow, which automatically launched reconstruction jobs on Polaris compute nodes using an on-demand queue. Once the reconstruction results were available on Eagle, they were transferred back to the APS through the same Globus transfer workflow.\n\nPartnering to Advance Energy Technologies\n\nThe Plasma Physics and Fusion Energy Pathfinder aims to incorporate remote use of high-performance computing into experiments running at the General Atomics DIII-D National Fusion Facility in San Diego, California.\n\nEach DIII-D experiment runs on a 20-minute cycle that requires time-sensitive analysis of the data generated to inform and ready the next experiment. Working closely with the DIII-D team and NERSC researchers, ALCF staff have collaborated with teams from DIII-D and the National Energy Research Scientific Computing Center (NERSC) to improve and automate the Consistent Automatic Kinetic Equilibrium (CAKE) workflow, developed and implemented at DIII-D to produce low-error, kinetically constrained magnetic equilibrium reconstructions without human intervention. The automation has yielded dramatic spikes in reconstruction productivity.\n\nALCF staff also worked to automate the Ion Orbiter workflow, which simulates particle trajectories and determines their hit locations on tokamak walls, culminating in production-ready analysis during experiments at DIII-D using Globus Flows to analyze data automatically between experiments Ultimately the IonOrbiter workflow will enable control-room personnel to quickly determine future wall-heating to enable plasma adjustments as needed.\n\nBoth the CAKE and Ion Orbiter workflows were demonstrated at the SC24 conference at the DOE booth.\n\nLeading the Future of Inter-Facility Science\n\nALCF staff participate in and co-chair weekly Leadership Group meetings to direct overall IRI efforts and specific tasks for technical subcommittees, form new subcommittees, and work with the Pathfinder projects. In 2024, ALCF staff served on the organizing committee for the IRI/HPDF kickoff meeting in Gaithersburg, Maryland, and produced related materials describing outcomes from the meeting. ALCF staff also presented during the Leadership Group’s participation in the DOE ASCAC meeting in May 2024.\n\nALCF staff have participated in all existing IRI technical subcommittees from day one, including Outreach and Engagement, Interfaces, and TRUSTID. These groups are dedicated to designing and building functionality at computing facilities to facilitate their use by experimentalists.",
          "url": "http://localhost:4000//features/nexus-iri"
        },
  
    
      "features-aurora-performance-highlights": {
          "title": "Aurora Performance Highlights",
          "content": "A Large-Scale Foundation Model for Advancing Science: AuroraGPT\n      PI: Rick Stevens, Argonne National Laboraotory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tCooperative Caching Layer for Scalable Parallel Data Movement in Exascale Supercomputing: Copper\n      PI: Venkat Vishwanath, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tExtreme-Scale Visualization and Analysis of Fluid-Structure Interactions: HARVEY\n      PI: Amanda Randles, Duke University\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tMachine Learning-Guided Computational Fluid Dynamics at Extreme Scales\n      PI: Kenneth Jansen, University of Colorado\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tReverse Monte-Carlo Radiative Ray Tracing Calculations at Scale: Uintah\n      PI: Martin Berzins, The University of Utah",
          "url": "http://localhost:4000//features/aurora-performance-highlights"
        },
  
    
      "science-projects": {
          "title": "2023 Award List",
          "content": "INCITE 2024\n\nBIOLOGICAL SCIENCES\n\nCOMbining Deep-Learning with Physics-Based affinIty estimatiOn 3 (COMPBIO3)\nPI: Peter Coveney, University College London\nHOURS: ALCF: 1,625,000 Node-Hours\n\nEstablishing Digital Twins for High-Throughput Cellular Analysis in Whole Blood\nPI: Amanda Randles, Duke University\nHOURS: ALCF: 60,000 Node-Hours, OLCF: 240,000 Node-Hours\n\nExaCortex: Exascale Reconstruction of Human Cerebral Cortex\nPI: Nicola Ferrier, Argonne National Laboratory\nHOURS: ALCF: 250,000 Node-Hours\n\nFoundation Models for Predictive Molecular Epidemiology\nPI: Arvind Ramanathan, Argonne National Laboratory\nHOURS: ALCF: 320,000 Node-Hours\n\nOpenFold-Powered Machine Learning of Protein-Protein Interactions and Complexes\nPI: Mohammed AlQuraishi, Columbia University\nHOURS: ALCF: 200,000 Node-Hours\n\nCOMPUTER SCIENCE\n\nDemocratizing AI by Training Deployable Open-Source Language Models\nPI: Abhinav Bhatele, University of Maryland\nHOURS: ALCF: 150,000 Node-Hours, OLCF: 600,000 Node-Hours\n\nCHEMISTRY\n\nExascale Catalytic Chemistry\nPI: David Bross, Argonne National Laboratory\nHOURS: ALCF: 425,000 Node-Hours\n\nHeterogeneous Catalysis as a Collective Phenomenon within a Dynamic Ensemble of Sites\nPI: Anastassia Alexandrova, University of California, Los Angeles\nHOURS: ALCF: 250,000 Node-Hours\n\nHeteropolymer Design Harnessing New and Emerging Computing Technologies\nPI: Vikram Mulligan, Flatiron Institute\nHOURS: ALCF: 375,000 Node-Hours\n\nEARTH SCIENCE\n\nEnergy Exascale Earth System Model\nPI: Peter Caldwell, Lawrence Livermore National Laboratory\nHOURS: ALCF: 500,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nENGINEERING\n\nFlight-Scale Simulations of a Transport Aircraft in High-Lift Conditions\nPI: Eric Nielsen, NASA Langley Research Center\nHOURS: ALCF: 350,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nHigh Reynolds Number Hypersonic Transition Control via Porous Walls\nPI: Carlo Scalo, Purdue University\nHOURS: ALCF: 300,000 Node-Hours\n\nInterface-resolved Simulations of Scalar Transport in Turbulent Bubbly Flows]\nPI: Parisa Mirbod, University of Illinois at Chicago\nHOURS: ALCF: 50,000 Node-Hours\n\n[Online Machine Learning for Large-Scale Turbulent Simulations]https://www.alcf.anl.gov/science/projects/online-machine-learning-large-scale-turbulent-simulations-0)]\nPI: Kenneth Jansen, University of Colorado Boulder\nHOURS: ALCF: 375,000 Node-Hours\n\nMATERIALS SCIENCE\n\nCarbon at Extremes: Discovery Science with Exascale Computers\nPI: Ivan Oleynik, University of South Florida\nHOURS: ALCF: 300,000 Node-Hours, OLCF: 300,000 Node-Hours\n\nExascale Simulations of Quantum Materials\nPI: Paul Kent, Oak Ridge National Laboratory\nHOURS: ALCF: 590,000 Node-Hours, OLCF: 600,000 Node-Hours\n\nHeterogeneous Reaction Dynamics for Energy Storage and Hydrogen Production\nPI: Boris Kozinsky, Harvard University\nHOURS: ALCF: 150,000 Node-Hours, OLCF: 300,000 Node-Hours\n\nPHYSICS\n\n3D Imaging of Strong Interaction Nambu-Goldstone Bosons\nPI: Yong Zhao, Argonne National Laboratory\nHOURS: ALCF: 150,000 Node-Hours\n\nAb-initio Nuclear Structure and Nuclear Reactions\nPI: Gaute Hagen, Oak Ridge National Laboratory\nHOURS: ALCF: 500,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nAdvances in Quark and Lepton Flavor Physics with Lattice QCD\nPI: Andreas Kronfeld, Fermilab\nHOURS: ALCF: 1,200,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nExascale Models of Astrophysical Thermonuclear Explosions\nPI: Katrin Heitmann, Argonne National Laboratory\nHOURS: ALCF: 375,000 Node-Hours, OLCF: 800,000 Node-Hours\n\nExascale Gyrokinetic Study of ITER Challenge on Power-Exhaust and ELM-Free Edge\nPI: Choongseok Chang, Princeton Plasma Physics Laboratory\nHOURS: ALCF: 550,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nExascale Models of Astrophysical Thermonuclear Explosions\nPI: Michael Zingale, Stony Brook University\nHOURS: ALCF: 75,000 Node-Hours, OLCF: 400,000 Node-Hours\n\nNovel Calculation of Nucleon Generalized Form Factors in Lattice QCD at the Physical Point\nPI: Martha Constantinou, Temple University\nHOURS: ALCF: 315,000 Node-Hours\n\nRadiation-Dominated Black Hole Accretion\nPI: James Stone, Institute for Advanced Studies\nHOURS: ALCF: 425,000 Node-Hours, OLCF: 800,000 Node-Hours\n\nResolving Cosmic Ray Transport by Pushing the Frontier of MHD Turbulence\nPI: Drummond Fielding, Cornell University\nHOURS: ALCF: 200,000 Node-Hours, OLCF: 700,000 Node-Hours\n\nState-of-the-Art High-Resolution 3D Simulations of Core-Collapse Supernovae\nPI: Adam Burrows, Princeton\nHOURS: ALCF: 1,000,000 Node-Hours\n\nQCD under Extreme Conditions\nPI: Zoltan Fodor, Pennsylvania State University\nHOURS: ALCF: 400,000 Node-Hours\n\nALCC 2024–2025\n\nBIOLOGICAL SCIENCES\n\nBuilding Digital Twin of a Model Host-Pathogen System for Enhancing Biopreparedness\nPI: Margaret S. Cheung, Pacific Northwest National Laboratory\nHOURS: ALCF: 200,000 Node-Hours, OLCF, 50,000 Node-Hours, NERSC: 300,000 Node-Hours\n\nFoundation Neuroscience AI Model-NeuroX\nPI: Shinjae Yoo, Brookhaven National Laboratory\nHOURS: ALCF: 290,000 Node-Hours, OLCF: 152,000 Node-Hours, NERSC: 120,000 Node-Hours\n\nCOMPUTER SCIENCE\n\nScalable and Resilient Modeling for Federated Learning Systems and Applications\nPI: Xiaoyi Lu, University of California\nHOURS: ALCF: 44,800 Node-Hours, OLCF: 15,360 Node-Hours, NERSC: 207,520 Node-Hours\n\nCHEMISTRY\n\nExploring Exascale Quantum Chemical Methods for Transition Metal Chemistry\nPI: Daniel Mejia Rodriguez, Pacific Northwest National Laboratory\nHOURS: ALCF: 313,442 Node-Hours, OLCF: 145,628 Node-Hours\n\nHighly Scalable Ab Initio Simulations of N-Doped Porous Materials for Carbon Capture\nPI: Mark Gordon, Ames National Laboratory\nHOURS: ALCF: 2,000,000 Node-Hours\n\nENERGY TECHNOLOGIES\n\nDNS of Buoyancy Driven Flows for Developing NN-informed High-fidelity Turbulence Closures\nPI: Som Dutta, Utah State University\nHOURS: ALCF: 100,000 Node-Hours, OLCF: 300,000 Node-Hours, NERSC: 200,000 Node-Hours\n\nExascale Computing for Energy Applications\nPI: Misun Min, Argonne National Laboratory\nHOURS: ALCF: 250,000 Node-Hours, OLCF: 200,000 Node-Hours, NERSC: 50,000 Node-Hours\n\nHigh-Fidelity CFD Enabling Advanced Nuclear Power\nPI: Dillon Shaver, Argonne National Laboratory\nHOURS: ALCF: 150,000 Node-Hours, OLCF: 200,000 Node-Hours\n\nHigh Fidelity Numerical Analysis on Flow and Heat Transfer behavior in Involute Plate Research Reactor to Support the Conversion Program\nPI: Yiqi Yu, Argonne National Laboratory\nHOURS: ALCF: 200,000 Node-Hours, OLCF: 500,000 Node-Hours\n\nHigh-Fidelity Simulations of Helium-Air Mixing in High-Temperature Gas Reactor Cavities\nPI: Saumil Patel, Argonne National Laboratory\nHOURS: ALCF: 115,000 Node-Hours\n\nENGINEERING\n\nAutonomy for DOE Simulations\nPI: Allan Grosvenor, MSBAI\nHOURS: ALCF: 30,000 Node-Hours, OLCF: 100,000 Node-Hours\n\nMATERIALS SCIENCE\n\nHigh Energy Density Physics of Novel Inertial Fusion Energy Ablator Materials\nPI: Ivan Oleynik, University of South Florida\nHOURS: ALCF: 600,000 Node-Hours, OLCF: 900,000 Node-Hours\n\nMachine Learning Enabled Atomistic Simulation of Iron at Extreme Pressure\nPI: Robert Rudd, Lawrence Livermore National Laboratory\nHOURS: ALCF: 150,000 Node-Hours\n\nPredicting Heterogeneous Photocatalysts Using Large-scale Ab Initio Calculations\nPI: Felipe Jornada, Stanford University\nHOURS: ALCF: 100,000 Node-Hours, OLCF: 140,000 Node-Hours\n\nPHYSICS\n\nHadronic Contributions to the Muon g-2 from Lattice QCD\nPI: Thomas Blum, University of Connecticut\nHOURS: ALCF: 1,000,000 Node-Hours, OLCF: 3,846,000 Node-Hours\n\nSimulating Large-scale Long-lived Neutron Star Remnants from Binary Neutron Star Mergers\nPI: Ore Gottlieb, Flatiron Institute\nHOURS: ALCF: 400,000 Node-Hours\n\nALCC 2023–2024\n\nBIOLOGICAL SCIENCES\n\nProbabilistic Comparative Modeling of Colorectal Cancer Screening Strategies\nPI: Jonathan Ozik, Argonne National Laboratory\nHOURS: ALCF: 160,000 Node-Hours, NERSC: 100,000 Node-Hours\n\nScaling Genomic Variant Callers to Leadership-Class Systems: A Collaboration Between VA-MVP and DOE\nPI: Ravi Madduri, Argonne National Laboratory\nHOURS: ALCF: 210,000 Node-Hours\n\nCHEMISTRY\n\nMicroscopic Insight Into Transport Properties of Li-Battery Electrolytes\nPI: Wei Jiang, Argonne National Laboratory\nHOURS: ALCF: 710,000 Node-Hours\n\nRelativistic Quantum Dynamics in the Non-Equilibrium Regime\nPI: Eugene DePrince, Florida State University\nHOURS: ALCF: 700,000 Node-Hours\n\nENERGY TECHNOLOGIES\n\nAccelerating Deployment of Next-Generation Nuclear Power Using High-Fidelity CFD\nPI: Dillon Shaver, Argonne National Laboratory\nHOURS: ALCF: 500,000 Node-Hours\n\nHigh Energy Density Physics of Inertial Confinement Fusion Ablator Materials\nPI: Ivan Oleynik , University of South Florida\nHOURS: ALCF: 500,000 Node-Hours, OLCF: 1,500,000 Node-Hours\n\nLarge Eddy Simulation on Flow and Heat Transfer Behavior in Involute Plate Research Reactor Supporting the Needs of the Materials Management and Minimization (M3) Reactor Conversion Program\nPI: Yiqi Yu, Argonne National Laboratory\nHOURS: ALCF: 510,000 Node-Hours\n\nENGINEERING\n\nTwo-Phase Flow Interface Capturing Simulations\nPI: Igor Bolotnov, North Carolina State University\nHOURS: ALCF: 200,000 Node-Hours, NERSC: 300,000 Node-Hours\n\nMATERIALS SCIENCE\n\nComputational Design of Novel Semiconductors for Power and Energy Applications\nPI: Feliciano Giustino , The University of Texas at Austin\nHOURS: ALCF: 100,000 Node-Hours\n\nLarge Scale Simulations of Materials for Quantum Information Science\nPI: Giulia Galli , University of Chicago\nHOURS: ALCF: 600,000 Node-Hours, NERSC: 400,000 Node-Hours\n\nQuantum Accurate Large-Scale Atomistic Simulations of Advanced Fusion Reactor Materials\nPI: Aidan Thompson, Sandia National Laboratories\nHOURS: ALCF: 850,000 Node-Hours, OLCF: 500,000 Node-Hours, NERSC: 250,000 Node-Hours\n\nPHYSICS\n\nContinuum Limit Latice Calculation of Direct CP-violation in Kaon Decays\nPI: Christopher Kelly, Brookhaven National Laboratory\nHOURS: ALCF: 135,000 Node-Hours\n\nEnergy Partition and Particle Acceleration in Laser-Driven Laboratory Magnetized Shocks\nPI: Frederico Fiuza, SLAC National Accelerator Laboratory\nHOURS: ALCF: 300,000 Node-Hours, NERSC: 150,000 Node-Hours\n\nHadronic Contributions to the Muon G-2 from Lattice QCD\nPI: Thomas Blum, University of Connecticut\nHOURS: ALCF: 5,000 Node-Hours, OLCF: 3,283,000 Node-Hours\n\nUsing GPU to Reconstruct LHC Collisions Recorded with the CMS Detector\nPI: Dirk Hufnagel , Fermi National Accelerator Laboratory\nHOURS: ALCF: 70,000 Node-Hours\n\nALCF Data Science Program\n\nAdvanced Materials Characterization with AI-Informed Computation\nPI: Marco Govoni, Argonne National Laboratory\n\nAutonomous Molecular Design for Redox Flow Batteries\nPI: Logan Ward, Argonne National Laboratory\n\nDeep Learning at Scale for Multimessenger Astrophysics Through the NCSA-Argonne Collaboration\nPI: Eliu Huerta, University of Illinois at Urbana-Champaign\n\nDeveloping High-Performance-Computing Applications for Liquid Argon Neutrino Detectors\nPI: Andrzej Szelc, University of Manchester\n\nDynamic Compressed Sensing for Real-Time Tomographic Reconstruction\nPI: Robert Hovden, University of Michigan\n\nLearning Optimal Image Representations for Current and Future Sky Surveys\nPI: George Stein, Lawrence Berkeley National Laboratory\n\nMachine Learning for Data Reconstruction to Accelerate Physics Discoveries in Accelerator-Based Neutrino Oscillation Experiments\nPI: Marco Del Tutto, Fermi National Accelerator Laboratory\n\nMachine Learning Magnetic Properties of Van Der Waals Heterostructures\nPI: Trevor Rhone, Rensselaer Polytechnic Institute\n\nX-ray Microscopy of Extended 3D Objects: Scaling Towards the Future\nPI: Chris Jacobsen, Argonne National Laboratory and Northwestern University\n\nAURORA EARLY SCIENCE PROGRAM\n\nAccelerated Deep Learning Discovery in Fusion Energy Science\nPI: William Tang, Princeton Plasma Physics Laboratory\n\nDark Sky Mining\nPI: Salman Habib, Argonne National Laboratory\n\nData Analytics and Machine Learning for Exascale Computational Fluid Dynamics\nPI: Kenneth Jansen, University of Colorado Boulder\n\nEnabling Connectomics at Exascale to Facilitate Discoveries in Neuroscience\nPI: Nicola Ferrier, Argonne National Laboratory\n\nExascale Computational Catalysis\nPI: David Bross, Argonne National Laboratory\n\nExtending Moore’s Law Computing with Quantum Monte Carlo\nPI: Anouar Benali, Argonne National Laboratory\n\nExtreme-Scale Cosmological Hydrodynamics\nPI: Katrin Heitmann, Argonne National Laboratory\n\nExtreme-Scale In-Situ Visualization and Analysis of Fluid-Structure-Interaction Simulations\nPI: Amanda Randles, Duke University\n\nExtreme-Scale Unstructured Adaptive CFD\nPI: Kenneth Jansen, University of Colorado Boulder\n\nHigh-Fidelity Simulation of Fusion Reactor Boundary Plasmas\nPI: C.S. Chang, Princeton Plasma Physics Laboratory\n\nMachine Learning for Lattice Quantum Chromodynamics\nPI: William Detmold, Massachusetts Institute of Technology\n\nMany-Body Perturbation Theory Meets Machine Learning to Discover Singlet Fission Materials\nPI: Noa Marom, Carnegie Mellon University\n\nNWChemEx: Tackling Chemical, Materials, and Biochemical Challenges in the Exascale Era\nPI: Theresa Windus, Iowa State University and Ames Laboratory\n\nSimulating and Learning in the ATLAS Detector at the Exascale\nPI: Walter Hopkins, Argonne National Laboratory\n\nVirtual Drug Response Prediction\nPI: Rick Stevens, Argonne National Laboratory\n\nDIRECTOR’S DISCRETIONARY\n\nThe following list provides a sampling of the many Director’s Discretionary projects at the ALCF.\n\nBIOLOGICAL SCIENCES\n\nLarge Ensemble Model of Single-Cell 3D Genome Structures\nPI: Jie Liang, University of Illinois at Chicago\n\nTargeting Intrinsically Disordered Proteins Using Artificial Intelligence Driven Molecular Simulations\nPI: Arvind Ramanathan, Argonne National Laboratory\n\nCHEMISTRY\n\nMultimodal Imaging with Intense X-ray Pulses\nPI: Phay Ho, Argonne National Laboratory\n\nCOMPUTER SCIENCE\n\nAPS Beamline Data Processing and Analysis\nPI: Rafael Vescovi, Argonne National Laboratory\n\nAdvanced Photon Source (APS) Data Processing\nPI: Nicholas Schwarz, Argonne National Laboratory\n\nMaterials Science, Data¬Driven Molecular Engineering of Solar-powered Windows\nPI: Jacqueline Cole, University of Cambridge\n\nOptimizing Bayesian Neural Networks for Scientific Machine Learning Applications\nPI: Murali Emani, Argonne National Laboratory\n\nENERGY TECHNOLOGIES\n\nOptimizing Bayesian Neural Networks for Scientific Machine Learning Applications\nPI: Joshua New, Oak Ridge National Laboratory\n\nDevelopment of High-Fidelity Simulations of Gas Turbine Combustors for Sustainable Aviation Applications\nPI: Sicong Wu, Northwestern University\n\nInvestigation of Flow and Heat Transfer Behavior in Involute Plate Research Reactor with Large Eddy Simulation to Support the Conversion of Research Reactors to Low Enriched Uranium Fuel\nPI: Yiqi Yu, Argonne National Laboratory\n\nMultiphase Simulations of Nuclear Reactor Thermal Hydraulics\nPI: Igor A. Bolotnov, North Carolina State University\n\nENGINEERING\n\nTurbulent Rayleigh-Benard Convection in Suspensions of Bubbles\nPI: Parisa Mirbod, University of Illinois at Chicago\n\nMATERIALS SCIENCE\n\nAdsorptive CO2 Removal from Dilute Sources\nPI: John J. Low, Argonne National Laboratory\n\nMaterials Informatics Study of Two-Dimensional Magnetic Materials and Their Heterostructures\nPI: Trevor David Rhone, Rensselaer Polytechnic Institute\n\nPHYSICS\n\nAnalytic Continuation of Interacting Fermion Spectra\nPI: Adrian Giuseppe Del Maestro, University of Tennessee\n\nNeural Network Quantum States for Atomic Nuclei\nPI: Alessandro Lovato, Argonne National Laboratory\n\nReproducible, Interpretable, and Physics-Inspired AI Models in Astrophysics\nPI: Eliu Huerta, Argonne National Laboratory",
          "url": "http://localhost:4000//science/projects"
        },
  
    
      "science-publications": {
          "title": "Publications",
          "content": "JANUARY\n\nAkinsanola, A. A., C. Jung, J. Wang, and V. R. Kotamarthi. “Evaluation of Precipitation Across the Contiguous United States, Alaska, and Puerto Rico in Multi-Decadal Convection-Permitting Simulations,” Scientific Reports (January 2024), Springer Nature. doi: 10.1038/s41598-024-51714-3\n\nBechtel Amara, T., S. P. Smith, Z. A. Xing, S. S. Denk, A. Deshpande, A. O. Nelson, C. Simpson, E. W. DeShazer, T. F. Neiser, O. Antepara, C. M. Clark, J. Lestz, J. Colmenares, N. Tyler, P. Ding, M. Kostuk, E. D. Dart, R. Nazikian, T. Osborne, S. Williams, T. Uram, and D. Schissel. “Accelerating Discoveries at DIII-D with the Integrated Research Infrastructure,” Frontiers in Physics (January 2024), Frontiers Media SA. doi: 10.3389/fphy.2024.1524041\n\nCheng, D., A. N. Alexandrova, and P. Sautet. “H-Induced Restructuring on Cu(111) Triggers CO Electroreduction in an Acidic Electrolyte,” The Journal of Physical Chemistry Letters (January 2024), ACS. doi: 10.1021/acs.jpclett.3c03202\n\nChitty-Venkata, K. T., S. Raskar, B. Kale, F. Ferdaus, A. Tanikanti, K. Raffenetti, V. Taylor, M. Emani, and V. Vishwanath. “LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators,” SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (January 2024), Atlanta, GA, IEEE. doi: 10.1109/SCW63240.2024.00178\n\nDai, Z., C. Lian, J. Lafuente-Bartolome, and F. Giustino. “Excitonic Polarons and Self-Trapped Excitons from First-Principles Exciton-Phonon Couplings,” Physical Review Letters (January 2024), APS. doi: 10.1103/PhysRevLett.132.036902\n\nDai, Z., C. Lian, J. Lafuente-Bartolome, and F. Giustino. “Theory of Excitonic Polarons: From Models to First-Principles Calculations,” Physical Review B (January 2024), APS. doi: 10.1103/PhysRevB.109.045202\n\nFields, J., H. Zhu, D. Radice, J. M. Stone, W. Cook, S. Bernuzzi, and B. Daszuta. “Performance-Portable Binary Neutron Star Mergers with AthenaK,” The Astrophysical Journal Supplement Series (January 2024), IOP Publishing. doi: 10.3847/1538-4365/ad9687\n\nHammonds, J., S. Henke, P. R. Jemian, S. Kandel, H. Parraga L. Rebuffi, X. Shi, S. Veseli, M. Wolfman, M. Wyman, T. Zhou, R. Chard, B. Cote, W. Allcock, L. Assoufid, M. J. Cherukara, S. Kelly, A. Sandy, J. Sullivan, and N. Schwarz. “Advanced Computational Technologies for Experiment Control, Data Acquisition, and Data Analysis at the Advanced Photon Source,” Synchrotron Radiation News (January 2024), Informa UK Limited. doi: 10.1080/08940886.2023.2277136\n\nHirata, C. M., M. Yamamoto, K. Laliotis, E. Macbeth, M. A. Troxel, T. Zhang, K. Cao, A. Choi, J. Givans, K. Heitmann, M. Ishak, M. Jarvis, E. Kovacs, H. Long, R. Mandelbaum, A. Park, A. Porredon, C. W. Walter, and W. M. Wood-Vasey. “Simulating Image Coaddition with the Nancy Grace Roman Space Telescope - I. Simulation Methodology and General Results,” Monthly Notices of the Royal Astronomical Society (January 2024), The Royal Astronomical Society. doi: 10.1093/mnras/stae182\n\nHuang, D., and J. M. Cole. “A Database of Thermally Activated Delayed Fluorescent Molecules Auto-Generated from Scientific Literature with ChemDataExtractor,” Scientific Data (January 2024), Springer Nature. doi: 10.1038/s41597-023-02897-3\n\nHuang, J., J. Liu, S. Di, Y. Zhai, Z. Jian, S. Wu, K. Zhao, Z. Chen, Y. Guo, and F. Cappello. “Exploring Wavelet Transform Usages for Error-Bounded Scientific Data Compression,” 2023 IEEE International Conference on Big Data (BigData) (January 2024), Sorrento, Italy, IEEE. doi: 10.1109/bigdata59044.2023.10386386\n\nLi, Y., F. Zhang, V.-A. Ha, Y.-C. Lin, C. Dong, Q. Gao, Z. Liu, X. Liu, S. H. Ryu, H. Kim, C. Jozwiak, A. Bostwick, K. Watanabe, T. Taniguchi, B. Kousa, X. Li, E. Rotenberg, E. Khalaf, J. A. Robinson, F. Giustino, and C.-K. Shih. “Tuning Commensurability in Twisted van der Waals Bilayers,” Nature (January 2024), Springer Nature. doi: 10.1038/s41586-023-06904-w\n\nLi, Z., P. Chaturvedi, S. He, H. Chen, G. Singh, V. Kindratenko, E. A. Huerta, K. Kim, and R. Madduri. “FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler,” ICLR 2024 (January 2024), Vienna, Austria, ICLR.\n\nLiu, J., M. Liu, J.-P. Liu, Z. Ye, Y. Wang, Y. Alexeev, J. Eisert, and L. Jiang. “Towards Provably Efficient Quantum Algorithms for Large-Scale Machine-Learning Models,” Nature Communications (January 2024), Springer Nature. doi: 10.1038/s41467-023-43957-x\n\nMa, B., V. Nikitin, D. Li, and T. Bicer. “Accelerated Laminographic Image Reconstruction Using GPUs,” Symposium on Electronic Imaging (January 2024), San Fransisco, CA. doi: 10.2352/EI.2024.36.12.HPCI-188\n\nMaeno, T., A. Alekseev, F. H. B. Megino, K. De, W. Guan, E. Karavakis, A. Klimentov, T. Korchuganova, F. Lin, P. Nilsson, T. Wenaus, Z. Yang, and X. Zhao. “PanDA: Production and Distributed Analysis System,” Computing and Software for Big Science (January 2024), Springer Nature. doi: 10.1007/s41781-024-00114-3\n\nNguyen-Cong, K., J. T. Willman, J. M. Gonzalez, A. S. Williams, A. B. Belonoshko, S. G. Moore, A. P. Thompson, M. A. Wood, J. H. Eggert, M. Millot, L. A. Zepeda-Ruiz, and I. I. Oleynik. “Extreme Metastability of Diamond and its Transformation to the BC8 Post-Diamond Phase of Carbon,” The Journal of Physical Chemistry Letters (January 2024), ACS Publications. doi: 10.1021/acs.jpclett.3c03044\n\nNikolakopoulos, A., A. Lovato, and N. Rocco. “Relativistic Effects in Green’s Function Monte Carlo Calculations of Neutrino-Nucleus Scattering,” Physical Review C (January 2024), APS. doi: 10.1103/PhysRevC.109.014623\n\nRen, Z., S. Elhatisari, T. A. Lähde, D. Lee, and U.-G. Meißner. “Ab Initio Study of Nuclear Clustering in Hot Dilute Matter,” Physics Letters B (January 2024), Elsevier. doi: 10.1016/j.physletb.2024.138463\n\nRoa Perdomo, D. A., R. A. Herrera Guaitero, D. Fox, H. Yviquel, S. Raskar, X. Li, and J. M. M. Diaz. “Towards Fault Tolerance and Resilience in the Sequential Codelet Model,” High Performance Computing: CARLA 2023 (January 2024), Cartagena, Colombia, Springer Nature, pp. 77-94. doi: 10.1007/978-3-031-52186-7_6\n\nTramm, J., B. Allen, K. Yoshii, A. Siegel, and L. Wilson. “Efficient Algorithms for Monte Carlo Particle Transport on AI Accelerator Hardware,” Computer Physics Communications (January 2024), Elsevier. doi: 10.1016/j.cpc.2023.109072\n\nFEBRUARY\n\nBanik, S., D. Dhabal, H. Chan, S. Manna, M. Cherukara, V. Molinero, and S. K. R. S. Sankaranarayanan. “CEGANN: Crystal Edge Graph Attention Neural Network for Multiscale Classification of Materials Environment,” npj Computational Materials (February 2023), Springer Nature. doi: 10.1038/s41524-023-00975-z\n\nClyde, A., X. Liu, T. Brettin, H. Yoo, A. Partin, Y. Babuji, B. Blaiszik, J. Mohd-Yusof, A. Merzky, M. Turilli, S. Jha, A. Ramanathan, and R. Stevens. “AI-Accelerated Protein-Ligand Docking for SARS-CoV-2 Is 100-Fold Faster with No Significant Change in Detection,” Scientific Reports (February 2023), Springer Nature. doi: 10.1038/s41598-023-28785-9\n\nJaysaval, P., G. Hammond, and T. C. Johnson. “Massively Parallel Modeling and Inversion of Electrical Resistivity Tomography Data Using PFLOTRAN,” Geoscientific Model Development (February 2023), European Geosciences Union. doi: 10.5194/gmd-2022-66\n\nMusaelian, A., S. Batzner, A. Johansson, L. Sun, C. J. Owen, M. Kornbluth, and B. Kozinsky. “Learning Local Equivariant Representations for Large-Scale Atomistic Dynamics,” Nature Communications (February 2023), Springer Nature. doi: 10.1038/s41467-023-36329-y\n\nRaskar, S., T. Applencourt, K. Kumaran, and G. Gao. “Towards Maximum Throughput of Dataflow Software Pipeline under Resource Constraints,” PMAM’23: Proceedings of the 14th International Workshop on Programming Models and Applications for Multicores and Manycores (February 2023), ACM, pp. 20-28. doi: 10.1145/3582514.3582521\n\nRigo, M., B. Hall, M. Hjorth-Jensen, A. Lovato, and F. Pederiva. “Solving the Nuclear Pairing Model with Neural Network Quantum States,” Physical Review E (February 2023), APS. doi: 10.1103/PhysRevE.107.025310\n\nShao, X., C. Zhu, P. Kumar, Y. Wang, J. Lu, M. Cha, L. Yao, Y. Cao, X. Mao, H. Heinz, and N. A. Kotov. “Voltage-Modulated Untwist Deformations and Multispectral Optical Effects from Ion Intercalation into Chiral Ceramic Nanoparticles,” Advanced Materials (February 2023), John Wiley and Sons. doi: 10.1002/adma.202206956\n\nVallejo, J. L. G., G. M. Tow, E. J. Maginn, B. Q. Pham, D. Datta, and M. S. Gordon. “Quantum Chemical Modeling of Propellant Degradation,” The Journal of Physical Chemistry A (February 2023), ACS Publications. doi: 10.1021/acs.jpca.2c08722\n\nMARCH\n\nAllcroft, S., M. Metwaly, Z. Berg, I. Ghodgaonakar, F. Bordwell, X. Zhao, X. Liu, J. Xu, S. Chakraborty, V. Banna, A. Chinnakotla, A. Goel, C. Tung, G. Kao, W. Zakharov, D. A. Shoham, G. K. Thiruvathukal, and Y.-H. Lu. “Observing Human Mobility Internationally During COVID-19,” Computer (March 2023), vol. 56, IEEE, pp. 59-69. doi: 10.1109/MC.2022.3175751\n\nChitty-Venkata, K. T., M. Emani, V. Vishwanath, and A. K. Somani. “Neural Architecture Search Benchmarks: Insights and Survey,” IEEE Access (March 2023), IEEE. doi: 10.1109/ACCESS.2023.3253818\n\nCondon, L. E., A. Farley, S. Jourdain, P. O’leary, P. Avery, L. Gallagher, C. Chennault, and R. M. Maxwell. “ParFlow Sand Tank: A Tool for Groundwater Exploration,” The Journal of Open Source Education (March 2023), Open Source Initiative. doi: 10.21105/jose.00179\n\nDorier, M., Z. Wang, S. Ramesh, U. Ayachit, S. Snyder, R. Ross, and M. Parashar. “Towards Elastic In Situ Analysis for High-Performance Computing Simulations,” Journal of Parallel and Distributed Computing (March 2023), Elsevier. doi: 10.1016/j.jpdc.2023.02.014\n\nHausen, R., B. E. Robertson, H. Zhu, N. Y. Gnedin, P. Madau, E. E. Schneider, B. Villasenor, and N. E. Drakos. “Revealing the Galaxy–Halo Connection through Machine Learning,” The Astrophysical Journal Letters (March 2023), IOP Publishing. doi: 10.3847/1538-4357/acb25c\n\nHuang, B., O. Anatole von Lilienfeld, J. T. Krogel, and A. Benali. “Toward DMC Accuracy Across Chemical Space with Scalable Δ-QML,” Journal of Chemical Theory and Computation (March 2023), ACS Publications. doi: 10.1021/acs.jctc.2c01058\n\nHuang, J., Y.-F. Jiang, H. Feng, S. W. Davis, J M. Stone, and M. J. Middleton. “Global 3D Radiation Magnetohydrodynamic Simulations of Accretion onto a Stellar-Mass Black Hole at Sub- and Near-Critical Accretion Rates,” The Astrophysical Journal (March 2023), IOP Publishing. doi: 10.3847/1538-4357/acb6fc\n\nJoshi, A. V., S. G. Rosofsky, R. Haas, and E. A. Huerta. “Numerical Relativity Higher Order Gravitational Waveforms of Eccentric, Spinning, Nonprecessing Binary Black Hole Mergers,” Physical Review D (March 2023), APS. doi: 10.1103/PhysRevD.107.064038\n\nKumari, S., T. Masubuchi, H. S. White, A. Alexandrova, S. L. Anderson, and P. Sautet. “Electrocatalytic Hydrogen Evolution at Full Atomic Utilization over ITO-Supported Sub-nano-Ptn Clusters: High, Size-Dependent Activity Controlled by Fluxional Pt Hydride Species,” Journal of the American Chemical Society (March 2023), ACS Publications. doi: 10.1021/jacs.2c13063\n\nKumari, S., and P. Sautet. “Elucidation of the Active Site for the Oxygen Evolution Reaction on a Single Pt Atom Supported on Indium Tin Oxide,” The Journal of Physical Chemistry Letters (March 2023), ACS. doi: 10.1021/acs.jpclett.3c00160\n\nLi, H., A. Hu, and G. A. Meehl. “Role of Tropical Cyclones in Determining ENSO Characteristics,” Geophysical Research Letters (March 2023), American Geophysical Union. doi: 10.1029/2022gl101814\n\nMaris, P., H. Le, A. Nogga, R. Roth, and J. P. Vary. “Uncertainties in Ab Initio Nuclear Structure Calculations with Chiral Interactions,” Frontiers in Physics (March 2023), Frontiers Media SA. doi: 10.3389/fphy.2023.1098262\n\nNelson, J., T. K. Stanev, D. Lebedev, T. LaMountain, J. Tyler Gish, H. Zeng, H. Shin, O. Heinonen, K. Watanabe, T. Taniguchi, M. C. Hersam, and N. P. Stern. “Layer-Dependent Optically Induced Spin Polarization in InSe,” Physical Review B (March 2023), APS. doi: 10.1103/PhysRevB.107.115304\n\nPal, S., J. Wang, J. Feinstein, E. Yan, and V. R. Kotamarthi. “Projected Changes in Extreme Streamflow and Inland Flooding in the Mid-21st Century over Northeastern United States Using Ensemble WRF-Hydro Simulations,” Journal of Hydrology: Regional Studies (March 2023), Elsevier. doi: 10.1016/j.ejrh.2023.101371\n\nSeyitliyev, D., X. Qin, M. K. Jana, S. M. Janke, X. Zhong, W. You, D. B. Mitzi, V. Blum, and K. Gundogdu. “Coherent Phonon-Induced Modulation of Charge Transfer in 2D Hybrid Perovskites,” Advanced Functional Materials (March 2023), John Wiley and Sons. doi: 10.1002/adfm.202213021\n\nSharma, H., M. Shrivastava, and B. Singh. “Physics Informed Deep Neural Network Embedded in a Chemical Transport Model for the Amazon Rainforest,” npj Climate and Atmospheric Science (March 2023), Springer Nature. doi: 10.1038/s41612-023-00353-y\n\nShen, K., S. Kumari, Y.-C. Huang, J. Jang, P. Sautet, and C. G. Morales-Guio. “Electrochemical Oxidation of Methane to Methanol on Electrodeposited Transition Metal Oxides,” Journal of the American Chemical Society (March 2023), ACS Publications. doi: 10.1021/jacs.3c00441\n\nShepard, C., D. C. Yost, and Y. Kanai. “Electronic Excitation Response of DNA to High-Energy Proton Radiation in Water,” Physical Review Letters (March 2023), APS. doi: 10.1103/PhysRevLett.130.118401\n\nVan den Puttelaar, R., R. G. S. Meester, E. E. P. Peterse, A. G. Zauber, J. Zheng, R. B. Hayes, Y.-R. Su, J. K. Lee, M. Thomas, L. C. Sakoda, Y. Li, D. A. Corley, U. Peters, L. Hsu, and I. Lansdorp-Vogelaar. “Risk-Stratified Screening for Colorectal Cancer Using Genetic and Environmental Risk Factors: A Cost-Effectiveness Analysis Based on Real-World Data,” Clinical Gastroenterology and Hepatology (March 2023), Elsevier. doi: 10.1016/j.cgh.2023.03.003\n\nZhang, Z. T. Masubuchi, P. Sautet, S. L. Anderson, and A. N. Alexandrova. “Hydrogen Evolution on Electrode-Supported Ptn Clusters: Ensemble of Hydride States Governs the Size Dependent Reactivity,” Angewandte Chemie (March 2023), John Wiley and Sons. doi: 10.1002/anie.202218210\n\nAPRIL\n\nApplencourt, T., B. Videau, J. Le Quellec, A. Dufek, K. Harms, N. Liber, B. Allen, and A. Belton-Schure. “Standardizing Complex Numbers in SYCL,” IWOCL ‘23: Proceedings of the 2023 International Workshop on OpenCL (April 2023), ACM, pp. 1-6. doi: 10.1145/3585341.3585343\n\nClarke, R. W., T. Sandmeier, K. A. Franklin, D. Reich, X. Zhang, N. Vengallur, T. K. Patra, R. J. Tannenbaum, S. Adhikari, S. K. Kumar, T. Rovis, and E. Y.-X. Chen. “Dynamic Crosslinking Compatibilizes Immiscible Mixed Plastics,” Nature (April 2023), Springer Nature. doi: 10.1038/s41586-023-05858-3\n\nFedorov, D. G., and B. Q. Pham. “Multi-Level Parallelization of Quantum-Chemical Calculations,” The Journal of Chemical Physics (April 2023), AIP. doi: 10.1063/5.0144917\n\nFragola, N. R., B. M. Brems, M. Mukherjee, M. Cui, and R. G. Booth. “Conformationally Selective 2-Aminotetralin Ligands Targeting the alpha2A- and alpha2C-Adrenergic Receptors,” ACS Chemical Neuroscience (April 2023), ACS. doi: 10.1021/acschemneuro.3c00148\n\nGao, X., A. D. Hanlon, J. Holligan, N. Karthik, S. Mukherjee, P. Petreczky, S. Syritsyn, and Y. Zhao. “Unpolarized Proton PDF at NNLO from Lattice QCD with Physical Quark Masses,” Physical Review D (April 2023), APS. doi: 10.1103/PhysRevD.107.074509\n\nHolford, J. J., M. Lee, and Y. Hwang. “Optimal White-Noise Stochastic Forcing for Linear Models of Turbulent Channel Flow,” Journal of Fluid Mechanics (April 2023), Cambridge University Press. doi: 10.1017/jfm.2023.234\n\nIchibha, T., K. Saritas, J. T. Krogel, Y. Luo, P. R. C. Kent, and F. A. Reboredo. “Existence of La-site Antisite Defects in LaMO3 (M=Mn, Fe, and Co) Predicted with Many-Body Diffusion Quantum Monte Carlo,” Scientific Reports (April 2023), Springer Nature. doi: 10.1038/s41598-023-33578-1\n\nLavroff, R. H., J. Wang, M. G. White, P. Sautet, and A. N. Alexandrova. “Mechanism of Stoichiometrically Governed Titanium Oxide Brownian Tree Formation on Stepped Au(111),” The Journal of Physical Chemistry C (April 2023), ACS. doi: 10.1021/acs.jpcc.3c00715\n\nLi, K., and D. Qi. “Molecular Dynamics Simulation of Mechanical Properties of Carbon Nanotube Reinforced Cellulose,” Journal of Molecular Modeling (April 2023), Springer Nature. doi: 10.1007/s00894-023-05542-3\n\nLytle, A., C. DeTar, A. X. El-Khadra, E. Gámiz, S. Gottlieb, W. Jay, A. Kronfeld, J. N. Simone, and A. Vaquero. “B-meson Semileptonic Decays with Highly Improved Staggered Quarks,” The 39th International Symposium on Lattice Field Theory (LATTICE2022) (April 2023), Sissa Medialab. doi: 10.22323/1.430.0418\n\nMatthews, B., J. Hall, M. Batty, S. Blainey, N. Cassidy, R. Choudhary, D. Coca, S. Hallett, J. J. Harou, P. James, N. Lomax, P. Oliver, A. Sivakumar, T. Tryfonas, and L. Varga. “DAFNI: A Computational Platform to Support Infrastructure Systems Research,” Proceedings of the Institution of Civil Engineers - Smart Infrastructure and Constrution (April 2023), Emerald Publishing Limited. doi: 10.1680/jsmic.22.00007\n\nPham, B. Q., L. Carrington, A. Tiwari, S. S. Leang, M. Alkan, C. Bertoni, D. Datta, T. Sattasathuchana, P. Xu, and M. S. Gordon. “Porting Fragmentation Methods to GPUs Using an OpenMP API: Offloading the Resolution-of-the-Identity Second-Order Møller-Plesset Perturbation Method,” The Journal of Chemical Physics (April 2023), AIP Publishing. doi: 10.1063/5.0143424\n\nRhone, T. D., R. Bhattarai, H. Gavras, B. Lusch, M. Salim, M. Mattheakis, D. T. Larson, Y. Krockenberger, and E. Kaxiras. “Artificial Intelligence Guided Studies of van der Waals Magnets,” Advanced Theory and Simulations (April 2023), John Wiley and Sons. doi: doi.org/10.1002/adts.202300019\n\nYang, L., R. Jaramillo, R. K. Kalia, A. Nakano, and P. Vashishta. “Pressure-Controlled Layer-by-Layer to Continuous Oxidation of ZrS2(001) Surface,” ACS Nano (April 2023), ACS. doi: 10.1021/acsnano.2c12724\n\nYang, T. T., and W. A. Saidi. “Simple Approach for Reconciling Cyclic Voltammetry with Hydrogen Adsorption Energy for Hydrogen Evolution Exchange Current,” The Journal of Physical Chemistry Letters (April 2023), ACS. doi: 10.1021/acs.jpclett.3c00534\n\nMAY\n\nBazavov, A., C. DeTar, A. X. El-Khadra, E. Gámiz, Z. Gelzer, S. Gottlieb, W. I. Jay, H. Jeong, A. S. Kronfeld, R. Li, A. T. Lytle, P. B. Mackenzie, E. T. Neil, T. Primer, J. N. Simone, R. L. Sugar, D. Toussaint, R. S. Van de Water, and A. Vaquero. “D-meson Semileptonic Decays to Pseudoscalars from Four-Flavor Lattice QCD,” Physical Review D (May 2023), APS. doi: 10.1103/PhysRevD.107.094516\n\nBhati, A. P., A. Hoti, A. Potterton, M. K. Bieniek, and P. V. Coveney. “Long Time Scale Ensemble Methods in Molecular Dynamics: Ligand–Protein Interactions and Allostery in SARS-CoV-2 Targets,” Journal of Chemical Theory and Computation (May 2023), ACS. doi: 10.1021/acs.jctc.3c00020\n\nFearick, R. W., P. von Neumann-Cosel, S. Bacca, J. Birkhan, F. Bonaiti, I. Brandherm, G. Hagen, H. Matsubara, W. Nazarewicz, N. Pietralla, V. Y. Ponomarev, P.-G. Reinhard, X. Roca-Maza, A. Richter, A. Schwenk, J. Simonis, and A. Tamii. “Electric Dipole Polarizability of 40Ca,” Physical Review Research (May 2023), APS. doi: 10.1103/PhysRevResearch.5.L022044\n\nIbayashi, H., T. M. Razakh, L. Yang, T. Linker, M. Olguin, S. Hattori, Y. Luo, R. K. Kalia, A. Nakano, K. Nomura, and P. Vashishta. “Allegro-Legato: Scalable, Fast, and Robust Neural-Network Quantum Molecular Dynamics via Sharpness-Aware Minimization,” ISC High Performance 2023: High Performance Computing (May 2023), Springer Link, pp. 223-239. doi: 10.1007/978-3-031-32041-5_12\n\nKang, S., and E. M. Constantinescu. “Learning Subgrid-Scale Models with Neural Ordinary Differential Equations,” Computers and Fluids&lt;/i&gt; (May 2023), Elsevier. doi: 10.1016/j.compfluid.2023.105919\n\nNascimento de Lima, P., R. van den Puttelaar, A. I. Hahn, M. Harlass, N. Collier, J. Ozik, A. G. Zauber, I. Lansdorp-Vogelaar, and C. M. Rutter. “Projected Long-Term Effects of Colorectal Cancer Screening Disruptions Following the COVID-19 Pandemic,” eLife (May 2023), eLife Sciences. doi: 10.7554/eLife.85264\n\nRamos-Valle, A. N., A. F. Prein, M. Ge, D. Wang, and S. E. Giangrande. “Grid Spacing Sensitivities of Simulated Mid-Latitude and Tropical Mesoscale Convective Systems in the Convective Gray Zone,” Journal of Geophysical Research: Atmospheres (May 2023), American Geophysical Union. doi: 10.1029/2022jd037043\n\nRosofsky, S. G., H. Al Majed, and E. A. Huerta. “Applications of Physics Informed Neural Operators,” Machine Learning: Science and Technology (May 2023), IOP. doi: 10.1088/2632-2153/acd168\n\nShen, S., S. Elhatisari, T. A. Lähde, D. Lee, B.-N. Lu, and U.-G. Meißner. “Emergent Geometry and Duality in the Carbon Nucleus,” Nature Communications (May 2023), Springer Nature. doi: 10.1038/s41467-023-38391-y\n\nSu, Q., J. Larson, T. N. Dalichaouch, F. Li, W. An, L. Hildebrand, Y. Zhao, V. Decyk, P. Alves, S. M. Wild, and W. B. Mori. “Optimization of Transformer Ratio and Beam Loading in a Plasma Wakefield Accelerator with a Structure-Exploiting Algorithm,” Physics of Plasmas (May 2023), AIP Publishing. doi: 10.1063/5.0142940\n\nSwaminathan, B., J. Kang, K. Vaidya, A. Srinivasan, P. Kumar, S. Byna, and D. Barbarash. “Crowd Cluster Data in the USA for Analysis of Human Response to COVID-19 Events and Policies,” Scientific Data (May 2023), Springer Nature. doi: 10.1038/s41597-023-02176-1\n\nVartanyan, D., A. Burrows, T. Wang, M. S. B. Coleman, and C. J. White. “Gravitational-Wave Signature of Core-Collapse Supernovae,” Physical Review D (May 2023), APS. doi: 10.1103/PhysRevD.107.103015\n\nJUNE\n\nAfle, C., S. K. Kundu, J. Cammerino, E. R. Coughlin, D. A. Brown, D. Vartanyan, and A. Burrows. “Measuring the Properties of f−Mode Oscillations of a Protoneutron Star by Third-Generation Gravitational-Wave Detectors,” Physical Review D (June 2023), APS. doi: 10.1103/PhysRevD.107.123005\n\nAnaya, J. J., A. Tropina, R. Miles, and M. Grover. “Refractive Index of Diatomic Species for Nonequilibrium Flows,” AIAA AVIATION 2023 Forum (June 2023), San Diego, CA, AIAA. doi: 10.2514/6.2023-3478\n\nAyush, K., A. Seth, and T. K. Patra. “nanoNET: Machine Learning Platform for Predicting Nanoparticles Distribution in a Polymer Matrix,” Soft Matter (June 2023), Royal Society of Chemistry. doi: 10.1039/d3sm00567d\n\nBazavov, A., C. Davies, C. DeTar, A. X. El-Khadra, E. Gámiz, S. Gottlieb, W. I. Jay, H. Jeong, A. S. Kronfeld, S. Lahert, G. P. Lepage, M. Lynch, A. T. Lytle, P. B. Mackenzie, C. McNeile, E. T. Neil, C. T. Peterson, C. Ray, J. N. Simone, R. S. Van de Water, and A. Vaquero. “Light-Quark Connected Intermediate-Window Contributions to the Muon g − 2 Hadronic Vacuum Polarization from Lattice QCD,” Physical Review D (June 2023), APS. doi: 10.1103/physrevd.107.114514\n\nBera, M., Q. Zhang, X. Zuo, W. Bu, J. Strzalka, S. Weigand, J. Ilavsky, E. Dufresne, S. Narayanan, and B. Lee. “Opportunities of Soft Materials Research at Advanced Photon Source,” Synchrotron Radiation News (June 2023), Informa UK Limited. doi: 10.1080/08940886.2023.2204096\n\nChen, J., R. G. Edwards, and W. Mao. “Graph Contractions for Calculating Correlation Functions in Lattice QCD,” PASC ‘23: Proceedings of the Platform for Advanced Scientific Computing Conference (June 2023), ACM. doi: 10.1145/3592979.3593409\n\nCruz-Camacho, E., K. A. Brown, X. Wang, X. Xu, K. Shu, Z. Lan, R. B. Ross, and C. D. Carothers. “Hybrid PDES Simulation of HPC Networks Using Zombie Packets,” SIGSIM-PADS ‘23: Proceedings of the 2023 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation (June 2023), ACM. doi: 10.1145/3573900.3591122\n\nGiraurd, S., J. C. Zamora, R. G. T. Zegers, D. Bazin, Y. Ayyad, S. Bacca, S. Beceiro-Novo, B. A. Brown, A. Carls, J. Chen, M. Cortesi, M. DeNudt, G. Hagen, C. Hultquist, C. Maher, W. Mittig, F. Ndayisabye, S. Noji, S. J. Novario, J. Peneira, Z. Rahman, J. Schmitt, M. Serikow, L. J. Sun, J. Surbrook, N. Watwood, and T. Wheeler. “β+ Gamow-Teller Strengths from Unstable 14O via the (d, 2He) Reaction in Inverse Kinematics,” Physical Review Letters (June 2023), APS. doi: 10.1103/PhysRevLett.130.232301\n\nGrover, M. S., P. Valentini, N. J. Bisek, and A. M. Verhoff. “First Principle Simulation of CUBRC Double Cone Experiments,” AIAA AVIATION 2023 Forum (June 2023), San Diego, CA, AIAA. doi: 10.2514/6.2023-3735\n\nHamilton, A., J.-M. Qiu, and H. Zhang. “Scalable Riemann Solvers with the Discontinuous Galerkin Method for Hyperbolic Network Simulation,” PASC ‘23: Proceedings of the Platform for Advanced Scientific Computing Conference (June 2023), ACM, pp. 1-10. doi: 10.1145/3592979.3593421\n\nKale, B., A. Clyde, M. Sun, A. Ramanathan, R. Stevens, and M. E. Papka. “ChemoGraph: Interactive Visual Exploration of the Chemical Space,” Computer Graphics Forum (June 2023), John Wiley and Sons. doi: 10.1111/cgf.14807\n\nKale, B., M. Sun, and M. E. Papka. “The State of the Art in Visualizing Dynamic Multivariate Networks,” Computer Graphics Forum (June 2023), John Wiley and Sons. doi: 10.1111/cgf.14856\n\nKang, S., A. Dener, A. Hamilton, H. Zhang, E. M. Constantinescu, and R. L. Jacob. “Multirate Partitioned Runge–Kutta Methods for Coupled Navier–Stokes Equations,” Computers and Fluids (June 2023), Elsevier. doi: 10.1016/j.compfluid.2023.105964\n\nKorover, I., and the CLAS Collaboration. “Observation of Large Missing-Momentum (e, e′p) Cross-Section Scaling and the Onset of Correlated-Pair Dominance in Nuclei,” Physical Review C (June 2023), APS. doi: 10.1103/PhysRevC.107.L061301\n\nLiu, J., S. Di, K. Zhao, X. Liang, Z. Chen, and F. Cappello. “FAZ: A Flexible Auto-Tuned Modular Error-Bounded Compression Framework for Scientific Data,” ICS ‘23: Proceedings of the 37th International Conference on Supercomputing (June 2023), ACM, pp. 1-13. doi: 10.1145/3577193.3593721\n\nLiu, Q., W. Jiang, J. Xu, Y. Xu, Z. Yang, D.-J. Yoo, K. Z. Pupek, C. Wang, C. Liu, K. Xu, and Z. Zhang. “A Fluorinated Cation Introduces New Interphasial Chemistries to Enable High-Voltage Lithium Metal Batteries,” Nature Communications (June 2023), Springer Nature. doi: 10.1038/s41467-023-38229-7\n\nMadhyastha, M., R. Underwood, R. Burns, and B. Nicolae. “DStore: A Lightweight Scalable Learning Model Repository with Fine-Grain Tensor-Level Access,” ICS ‘23: Proceedings of the 37th International Conference on Supercomputing (June 2023), ACM, pp. 133-143. doi: 10.1145/3577193.3593730\n\nNicholson, G. L., L. Szajnecki, L. Duan, and N. J. Bisek. “Direct Numerical Simulation of High-Speed Boundary-Layer Separation Due to Backward Facing Curvature,” AIAA AVIATION 2023 Forum (June 2023), San Diego, CA, AIAA. doi: 10.2514/6.2023-3562\n\nPark, H., R. Zhu, E. A. Huerta, S. Chaudhuri, E. Tajkhorshid, and D. Cooper. “End-to-End AI Framework for Interpretable Prediction of Molecular and Crystal Properties,” Machine Learning: Science and Technology (June 2023), IOP Publishing. doi: 10.1088/2632-2153/acd434\n\nPurcell, T. A. R., M. Scheffler, L. M. Ghiringhelli, and C. Carbogno. “Accelerating Materials-Space Exploration for Thermal Insulators by Mapping Materials Properties via Artificial Intelligence,” npj Computational Materials (June 2023), Springer Nature. doi: 10.1038/s41524-023-01063-y\n\nScheld, W. S., K. Kim, C. Schwab, A. C. Moy, S.-K. Jiang, M. Mann, C. Dellen, Y. J. Sohn, S. Lobe, M. Ihrig, M. G. Danner, C.-Y. Chang, S. Uhlenbruck, E. D. Wachsman, B. J. Hwang, J. Sakamoto, L. F. Wan, B. C. Wood, M. Finsterbusch, and D. Fattakhova-Rohlfing. “The Riddle of Dark LLZO: Cobalt Diffusion in Garnet Separators of Solid-State Lithium Batteries,” Advanced Functional Materials (June 2023), John Wiley and Sons. doi: 10.1002/adfm.202302939\n\nShah, A., A. Ramanathan, V. Hayot-Sasson, and R. Stevens. “Causal Discovery and Optimal Experimental Design for Genome-Scale Biological Network Recovery,” PASC ‘23: Proceedings of the Platform for Advanced Scientific Computing Conference (June 2023), ACM, pp. 1-11. doi: 10.1145/3592979.3593400\n\nShah, M., X. Yu, S. Di, M. Becchi, and F. Cappello. “Lightweight Huffman Coding for Efficient GPU Compression,” ICS ‘23: Proceedings of the 37th International Conference on Supercomputing (June 2023), ACM, pp. 99-110. doi: 10.1145/3577193.3593736\n\nSingh, S., O. Ruwase, A. A. Awan, S. Rajbhandari, Y. He, and A. Bhatele. “A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training,” ICS ‘23: Proceedings of the 37th International Conference on Supercomputing (June 2023), ACM, pp. 203-214. doi: 10.1145/3577193.3593704\n\nWhite, C. J., P. D. Mullen, Y.-F. Jiang, S. W. Davis, J. M. Stone, V. Morozova, and L. Zhang. “An Extension of the Athena++ Code Framework for Radiation-Magnetohydrodynamics in General Relativity Using a Finite-Solid-Angle Discretization,” The Astrophysical Journal (June 2023), IOP Publishing. doi: 10.3847/1538-4357/acc8cf\n\nXu, X., X. Wang, E. Cruz-Camacho, C. D. Carothers, K. A. Brown, R. B. Ross, Z. Lan, and K. Shu. “Machine Learning for Interconnect Network Traffic Forecasting: Investigation and Exploitation,” SIGSIM-PADS ‘23: Proceedings of the 2023 ACM SIGSIM Conference on Principles of Advanced Discrete Simulation (June 2023), ACM. doi: 10.1145/3573900.3591123\n\nJULY\n\nBollweg, D., D. A. Clarke, J. Goswami, O. Kaczmarek, F. Karsch, S. Mukherjee, P. Petreczky, C. Schmidt, and S. Sharma. “Equation of State and Speed of Sound of (2 + 1)-Flavor QCD in Strangeness-Neutral Matter at Nonvanishing Net Baryon-Number Density,” Physical Review D (July 2023), APS. doi: 10.1103/PhysRevD.108.014510\n\nBolotnov, I. A. “Direct Numerical Simulation of Single- and Two-Phase Flows for Nuclear Engineering Geometries,” Nuclear Technology (July 2023), Informa UK Limited. doi: 10.1080/00295450.2023.2232222\n\nConroy, N. S., M. Bauböck, V. Dhruv, D. Lee, A. E. Broderick, C. Chan, B. Georgiev, A. V. Joshi, B. Prather, and C. F. Gammie. “Rotation in Event Horizon Telescope Movies,” The Astrophysical Journal (July 2023), IOP Publishing. doi: 10.3847/1538-4357/acd2c8\n\nDive, A., K. Kim, S. Kang, L. F. Wan, and B. C. Wood. “First-Principles Evaluation of Dopant Impact on Structural Deformability and Processability of Li7La3Zr2O12,” Physical Chemistry Chemical Physics (July 2023), Royal Society of Chemistry. doi: 10.1039/d2cp04382c\n\nFore, B., J. M. Kim, G. Carleo, M. Hjorth-Jensen, A. Lovato, and M. Piarulli. “Dilute Neutron Star Matter from Neural-Network Quantum States,” Physical Review Research (July 2023), APS. doi: 10.1103/PhysRevResearch.5.033062\n\nGalda, A., E. Gupta, J. Falla, X. Liu, D. Lykov, Y. Alexeev, and I. Safro. “Similarity-Based Parameter Transferability in the Quantum Approximate Optimization Algorithm,” Frontiers in Quantum Science and Technology (July 2023), Frontiers Media SA. doi: 10.3389/frqst.2023.1200975\n\nGuo, J., V. Woo, D. A. Andersson, N. Hoyt, M. Williamson, I. Foster, C. Benmore, N. E. Jackson, and G. Sivaraman. “AL4GAP: Active Learning Workflow for Generating DFT-SCAN Accurate Machine-Learning Potentials for Combinatorial Molten Salt Mixtures,” The Journal of Chemical Physics (July 2023), AIP Publishing. doi: 10.1063/5.0153021\n\nHong, Z., A. Ajith, J. Pauloski, E. Duede, K. Chard, and I. Foster. “The Diminishing Returns of Masked Language Models to Science,” Findings of the Association for Computational Linguistics: ACL 2023 (July 2023), ACL, pp. 1270-1283. doi: 10.18653/v1/2023.findings-acl.82\n\nHuerta, E. A., B. Blaiszik, L. C. Brinson, K. E. Bouchard, D. Diaz, C. Doglioni, J. M. Duarte, M. Emani, I. Foster, G. Fox, P. Harris, L. Heinrich, S. Jha, D. S. Katz, V. Kindratenko, C. R. Kirkpatrick, K. Lassila-Perini, R. K. Madduri, M. S. Neubauer, F. E. Psomopoulos, A. Roy, O. Rübel, Z. Zhao, and R. Zhu. “FAIR for AI: An Interdisciplinary and International Community Building Perspective,” Scientific Data (July 2023), Springer Nature. doi: 10.1038/s41597-023-02298-6\n\nJeong, H., A. K. Turner, A. F. Roberts, M. Veneziani, S. F. Price, X. S. Asay-Davis, L. P. Van Roekel, W. Lin, P. M. Caldwell, H.-S. Park, J. D. Wolfe, and A. Mametjanov. “Southern Ocean Polynyas and Dense Water Formation in a High-Resolution, Coupled Earth System Model,” The Cryosphere (July 2023), Copernicus Publications. doi: 10.5194/tc-17-2681-2023\n\nKayastha, M. B., C. Huang, J. Wang, W. J. Pringle, TC Chakraborty, Z. Yang, R. D. Hetland, Y. Qian, and P. Xue. “Insights on Simulating Summer Warming of the Great Lakes: Understanding the Behavior of a Newly Developed Coupled Lake-Atmosphere Modeling System,” Journal of Advances in Modeling Earth Systems (July 2023), John Wiley and Sons. doi: 10.1029/2023MS003620\n\nLiu, Z., R. Kettimuthu, M. E. Papka, and I. Foster. “FreeTrain: A Framework to Utilize Unused Supercomputer Nodes for Training Neural Networks,” 2023 IEEE/ACM 23rd International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (July 2023), Bangalore, India, IEEE. doi: 10.1109/ccgrid57682.2023.00036\n\nMahadevan, V., D. Lenz, I. Grindeanu, and T. Peterka. “Accelerating Multivariate Functional Approximation Computation with Domain Decomposition Techniques,” Computational Science – ICCS 2023: 23rd International Conference (July 2023), Prague, Czech Republic, ACM, pp. 89-103. doi: 10.1007/978-3-031-35995-8_7\n\nManassa, J., J. Schwartz, Y. Jiang, H. Zheng, J. A. Fessler, Z. W. Di, and R. Hovden. “Dose Requirements for Fused Multi-Modal Electron Tomography,” Microscopy and Microanalysis (July 2023), Oxford University Press. doi: 10.1093/micmic/ozad067.1019\n\nPeterka, T., D. Morozov, O. Yildiz, B. Nicolae, and P. E. Davis. “LowFive: In Situ Data Transport for High-Performance Workflows,” 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (July 2023), St. Petersburg, FL, IEEE. doi: 10.1109/IPDPS54959.2023.00102\n\nRosofsky, S. G., and E. A. Huerta. “Magnetohydrodynamics with Physics Informed Neural Operators,” Machine Learning: Science and Technology (July 2023), IOP Publishing. doi: 10.1088/2632-2153/ace30a\n\nSchanen, M., S. H. K. Narayanan, S. Williamson, V. Churavy, W. S. Moses, and L. Paehler. “Transparent Checkpointing for Automatic Differentiation of Program Loops Through Expression Transformations,” Computational Science – ICCS 2023: 23rd International Conference (July 2023), Prague, Czech Republic, ACM, pp. 483-497. doi: 10.1007/978-3-031-36024-4_37\n\nShovon, A. R., T. Gilray, K. Micinski, and S. Kumar. “Towards Iterative Relational Algebra on the GPU,” 2023 USENIX Annual Technical Conference (USENIX ATC 23) (July 2023), Boston, MA, USENIX Association, pp. 1009-1016,\n\nSun, Z. H., G. Hagen, and T. Papenbrock. “Coupled-Cluster Theory for Strong Entanglement in Nuclei,” Physical Review C (July 2023), APS. doi: 10.1103/PhysRevC.108.014307\n\nTsai, Y.-H. M., N. Beams, and H. Anzt. “Three-Precision Algebraic Multigrid on GPUs,” Future Generation Computer Systems (July 2023), Elsevier. doi: 10.1016/j.future.2023.07.024\n\nValentini, P., M. S. Grover, A. M. Verhoff, and N. J. Bisek. “Near-Continuum, Hypersonic Oxygen Flow over a Double Cone Simulated by Direct Simulation Monte Carlo Informed from Quantum Chemistry,” Journal of Fluid Mechanics (July 2023), Cambridge University Press. doi: 10.1017/jfm.2023.437\n\nVan den Berg, D. M. N., P. Nascimento de Lima, A. B. Knudsen, C. M. Rutter, D. Weinberg, I. Lansdorp-Vogelaar, A. G. Zauber, A. I. Hahn, F. A. Escudero, C. E. Maerzluft, A. Katsara, K. M. Kuntz, J. M. Inamodi, N. Collier, J. Ozik, L. A. van Duuren, R. van den Puttelaar, M. Harlass, C. L. Seguin, B. Davidi, C. Pineda-Antunez, E. J. Feuer, and L. de Jonge. “NordICC Trial Results in Line with Expected Colorectal Cancer Mortality Reduction after Colonoscopy: A Modeling Study,” Gastroentereology (July 2023), Elsevier. doi: 10.1053/j.gastro.2023.06.035\n\nVerma, G., S. Raskar, Z. Xie, A. M. Malik, M. Emani, and B. Chapman. “Transfer Learning Across Heterogeneous Features for Efficient Tensor Program Generation,” ExHET 23: Proceedings of the 2nd International Workshop on Extreme Heterogeneity Solutions (July 2023), ACM, pp. 1-6. doi: 10.1145/3587278.3595644\n\nVincenti, H., T. Clark, L. Fedeli, P. Martin, A. Sainte-Marie, and N. Zaim. “Plasma Mirrors as a Path to the Schwinger Limit: Theoretical and Numerical Developments,” The European Physical Journal Special Topics (July 2023), Springer Nature. doi: 10.1140/epjs/s11734-023-00909-2\n\nZhang, Z., I. Hermans, and A. N. Alexandrova. “Off-Stoichiometric Restructuring and Sliding Dynamics of Hexagonal Boron Nitride Edges in Conditions of Oxidative Dehydrogenation of Propane,” Journal of the American Chemical Society (July 2023), ACS. doi: 10.1021/jacs.3c04613\n\nZhao, J., C. Bertoni, J. Young, K. Harms, V. Sarkar, and B. Videau. “HIPLZ: Enabling Performance Portability for Exascale Systems,” Concurrency and Computation: Practice and Experience (July 2023), John Wiley and Sons. doi: 10.1002/cpe.7866\n\nAUGUST\n\nAli, S., S. Calvez, P. Carns, M. Dorier, P. Ding, J. Kowalkowski, R. Latham, A. Norman, M. Paterno, R. Ross, S. Sehrish, S. Snyder, and J. Soumagne. “HEPnOS: A Specialized Data Service for High Energy Physics Analysis,” 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (August 2023), St. Petersburg, FL, IEEE. doi: 10.1109/IPDPSW59300.2023.00108\n\nAzizi, K., M. Gori, U. Morzan, A. Hassanali, and P. Kurian. “Examining the Origins of Observed Terahertz Modes from an Optically Pumped Atomistic Model Protein in Aqueous Solution,” PNAS Nexus (August 2023), Oxford University Press. doi: 10.1093/pnasnexus/pgad257\n\nBrahlek, M., A. R. Mazza, A. Annaberdiyev, M. Chilcote, G. Rimal, G. B. Halász, A. Pham, Y.-Y. Pai, J. T. Krogel, J. Lapano, B. J. Lawrie, G. Eres, J. McChesney, T. Prokscha, A. Suter, S. Oh, J. W. Freeland, Y. Cao, J. S. Gardner, Z. Salman, R. G. Moore, P. Ganesh, and T. Z. Ward. “Emergent Magnetism with Continuous Control in the Ultrahigh-Conductivity Layered Oxide PdCoO2,” Nano Letters (August 2023), ACS. doi: 10.1021/acs.nanolett.3c01065\n\nChen, S., F. Browne, P. Doornenbal, J. Lee, A. Obertelli, Y. Tsunoda, T. Otsuka, Y. Chazono, G. Hagen, J. D. Holt, G. R. Jansen, K. Ogata, N. Shimizu, Y. Utsuno, K. Yoshida, N. L. Achouri, H. Baba, D. Calvet, F. Château, N. Chiga, A. Corsi, M. L. Cortés, A. Delbart, J.-M. Gheller, A. Giganon, A. Gillibert, C. Hilaire, T. Isobe, T. Kobayashi, Y. Kubota, V. Lapoux, H. N. Liu, T. Motobayashi, I. Murray, H. Otsu, V. Panin, N. Paul, W. Rodriguez, H. Sakurai, M. Sasano, D. Steppenbeck, L. Stuhl, Y. L. Sun, Y. Togano, T. Uesaka, K. Wimmer, K. Yoneda, O. Aktas, T. Aumann, L. X. Chung, F. Flavigny, S. Franchoo, I. Gasparic, R.-B. Gerst, J. Gibelin, K. I. Hahn, D. Kim, T. Koiwai, Y. Kondo, P. Koseoglou, C. Lehr, B. D. Linh, T. Lokotko, M. MacCormick, K. Moschner, T. Nakamura, S. Y. Park, D. Rossi, E. Sahin, P.-A. Söderström, D. Sohler, S. Takeuchi, H. Törnqvist, V. Vaquero, V. Wagner, S. Wang, V. Werner, X. Xu, H. Yamada, D. Yan, Z. Yang, M. Yasuda, and L. Zanetti. “Level Structures of 56,58Ca Cast Doubt on a Doubly Magic 60Ca,” Physics Letters B (August 2023), Elsevier. doi: 10.1016/j.physletb.2023.138025\n\nCollier, N., J. M. Wozniak, A. Stevens, Y. Babuji, M. Binois, A. Fadikar, A. Würth, K. Chard, and J. Ozik. “Developing Distributed High-Performance Computing Capabilities of an Open Science Platform for Robust Epidemic Analysis,” 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (August 2023), St. Petersburg, FL, IEEE. doi: 10.1109/ipdpsw59300.2023.00143\n\nDong, B., J. L. Bez, and S. Byna. “AIIO: Using Artificial Intelligence for Job-Level and Automatic I/O Performance Bottleneck Diagnosis,” HPDC ‘23: Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing (August 2023), ACM, pp. 155-167. doi: 10.1145/3588195.3592986\n\nGrover, M. S., A. M. Verhoff, P. Valentini, and N. J. Bisek. “First Principles Simulation of Reacting Hypersonic Flow over a Blunt Wedge,” Physics of Fluids (August 2023), AIP Publishing. doi: 10.1063/5.0161570\n\nHannon, S., B. C. Whitmore, J. C. Lee, D. A. Thilker, S. Deger, E. A. Huerta, W. Wei, B. Mobasher, R. Klessen, M. Boquien, D. A. Dale, M. Chevance, K. Grasha, P. Sanchez-Blazquez, T. Williams, F. Scheuermann, B. Groves, H. Kim, J. M. D. Kruijssen, and the PHANGS-HST Team. “Star Cluster Classification Using Deep Transfer Learning with PHANGS-HST,” Monthly Notices of the Royal Astronomical Society (August 2023), Oxford University Press. doi: 10.1093/mnras/stad2238\n\nHosseini, R., F. Simini, V. Vishwanath, R. Sivakumar, S. Shanmugavelu, Z. Chen, L. Zlotnik, M. Wang, P. Colangelo, A. Deng, P. Lassen, and S. Pathan. “Exploring the Use of Dataflow Architectures for Graph Neural Network Workloads,” High Performance Computing: ISC High Performance 2023 International Workshops (August 2023), Hamburg, Germany, ACM, pp. 648-661. doi: 10.1007/978-3-031-40843-4_48\n\nJohnson, M. S., M. Gierada, E. D. Hermes, D. H. Bross, K. Sargsyan, H. N. Najm, and J. Zádor. “Pynta─An Automated Workflow for Calculation of Surface and Gas–Surface Kinetics,” Journal of Chemical Information and Modeling (August 2023), ACS Publications. doi: 10.1021/acs.jcim.3c00948\n\nLee, H., S. Poncé, K. Bushick, S. Hajinazar, J. Lafuente-Bartolome, J. Leveillee, C. Lian, J.-M. Lihm, F. Macheda, H. Mori, H. Paudyal, W. H. Sio, S. Tiwari, M. Zacharias, X. Zhang, N. Bonini, E. Kioupakis, E. R. Margine, and F. Giustino. “Electron–Phonon Physics from First Principles Using the EPW Code,” npj Computational Materials (August 2023), Springer Nature. doi: 10.1038/s41524-023-01107-3\n\nLenard, B., E. Pershey, Z. Nault, and A. Rasin. “An Approach for Efficient Processing of Machine Operational Data,” Database and Expert Systems Applications: 34th International Conference, DEXA 2023 (August 2023), Penang, Malaysia, ACM, pp. 129-146. doi: 10.1007/978-3-031-39847-6_9\n\nLinker, T. M., K. Nomura, S. Fukushima, R. K. Kalia, A. Krishnamoorthy, A. Nakano, K. Shimamura, F. Shimojo, and P. Vashishta. “Induction and Ferroelectric Switching of Flux Closure Domains in Strained PbTiO3 with Neural Network Quantum Molecular Dynamics,” Nano Letters (August 2023), ACS Publications. doi: 10.1021/acs.nanolett.3c01885\n\nLovato, A., A. Nikolakopoulos, N. Rocco, and N. Steinberg. “Lepton–Nucleus Interactions within Microscopic Approaches,” Universe (August 2023), MDPI. doi: 10.3390/universe9080367\n\nMecham, N. J., I. A. Bolotnov, and E. L. Popov. “Quantifying HFIR Turbulence by Variable Curvature Channels,” 20th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-20) (August 2023), Washington, DC, American Nuclear Society, pp. 1194-1205. doi: 10.13182/nureth20-40044\n\nMonniot, J., F. Tessier, M. Robert, and G. Antoniu. “Supporting Dynamic Allocation of Heterogeneous Storage Resources on HPC Systems,” Concurrency and Computation: Practice and Experience (August 2023), John Wiley and Sons. doi: 10.1002/cpe.7890\n\nRoy, R. B., T. Patel, R. Liew, Y. N. Babuji, R. Chard, and D. Tiwari. “ProPack: Executing Concurrent Serverless Functions Faster and Cheaper,” HPDC ‘23: Proceedings of the 32nd International Symposium on High-Performance Parallel and Distributed Computing (August 2023), ACM, pp. 211-224. doi: 10.1145/3588195.3592988\n\nWan, L., K. Kim, A. M. Dive, B. Wang, T. W. Heo, M. Wood, and B. C. Wood. “Multiscale Modeling of Heterogeneous Interfaces in All Solid-State Batteries,” ECS Meeting Abstracts (August 2023), IOP Publishing. doi: 10.1149/ma2023-0161045mtgabs\n\nWang, T., and A. Burrows. “Neutrino-Driven Winds in Three-Dimensional Core-Collapse Supernova Simulations,” The Astrophysical Journal (August 2023), IOP Publishing. doi: 10.3847/1538-4357/ace7b2\n\nWard, L., J. G. Pauloski, V. Hayot-Sasson, R. Chard, Y. Babuji, G. Sivaraman, S. Choudhury, K. Chard, R. Thakur, and I. Foster. “Cloud Services Enable Efficient AI-Guided Simulation Workflows across Heterogeneous Resources,” 2023 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (August 2023), St. Petersburg, FL, IEEE. doi: 10.1109/IPDPSW59300.2023.00018\n\nXie, Z., S. Raskar, M. Emani, and V. Vishwanath. “TrainBF: High-Performance DNN Training Engine Using BFloat16 on AI Accelerators,” Euro-Par 2023: Parallel Processing (August 2023), Springer Nature, pp. 458-473. doi: 10.1007/978-3-031-39698-4_31\n\nSEPTEMBER\n\nAhn, J., I. Hong, G. Lee, H. Shin, A. Benali, J. T. Krogel, and Y. Kwon. “Structural Stability of Graphene-Supported Pt Layers: Diffusion Monte Carlo and Density Functional Theory Calculations,” The Journal of Physical Chemistry C (September 2023), ACS. doi: 10.1021/acs.jpcc.3c03160\n\nBlum, T., P. A. Boyle, M. Bruno, D. Giusti, V. Gülpers, R. C. Hill, T. Izubuchi, Y.-C. Jang, L. Jin, C. Jung, A. Jüttner, C. Kelly, C. Lehner, N. Matsumoto, R. D. Mawhinney, A. S. Meyer, and J. T. Tsang. “Update of Euclidean Windows of the Hadronic Vacuum Polarization,” Physical Review D (September 2023), APS. doi: 10.1103/physrevd.108.054507\n\nBoëzennec, R., F. Dufossé, and G. Pallez. “Optimization Metrics for the Evaluation of Batch Schedulers in HPC,” Job Scheduling Strategies for Parallel Processing: 26th Workshop, JSSPP 2023 (September 2023), St. Petersburg, FL, ACM, pp. 97-115. doi: 10.1007/978-3-031-43943-8_5\n\nChen, L., P.-H. Lin, T. Vanderbruggen, C. Liao, M. Emani, and B. de Supinski. “LM4HPC: Towards Effective Language Model Application in High-Performance Computing,” IWOMP 2023: OpenMP: Advanced Task-Based, Device and Compiler Programming (September 2023), Springer Nature, pp. 18-33. doi: 10.1007/978-3-031-40744-4_2\n\nChitty-Venkata, K. T., Y. Bian, M. Emani, V. Vishwanath, and A. K. Somani. “Differentiable Neural Architecture, Mixed Precision and Accelerator Co-Search,” IEEE Access (September 2023), IEEE. doi: 10.1109/ACCESS.2023.3320133\n\nChitty-Venkata, K. T., S. Mittal, M. Emani, V. Vishwanath, and A. K. Somani. “A Survey of Techniques for Optimizing Transformer Inference,” Journal of Systems Architecture (September 2023), Elsevier. doi: 10.1016/j.sysarc.2023.102990\n\nHaberlie, A. M., W. S. Ashley, V. A. Gensini, and A. C. Michaels. “The Ratio of Mesoscale Convective System Precipitation to Total Precipitation Increases in Future Climate Change Scenarios,” npj Climate and Atmospheric Science (September 2023), Springer Nature. doi: 10.1038/s41612-023-00481-5\n\nHimanshu, K. Chakraborty, and T. K. Patra. “Developing Efficient Deep Learning Model for Predicting Copolymer Properties,” Physical Chemistry Chemical Physics (September 2023), Royal Society of Chemistry. doi: 10.1039/d3cp03100d\n\nIsazawa, T., and J. M. Cole. “Automated Construction of a Photocatalysis Dataset for Water-Splitting Applications,” Scientific Data (September 2023), Springer Nature. doi: 10.1038/s41597-023-02511-6\n\nKönig, K., S. Fritzsche, G. Hagen, J. D. Holt, A. Klose, J. Lantis, Y. Liu, K. Minamisono, T. Miyagi, W. Nazarewicz, T. Papenbrock, S. V. Pineda, R. Powel, and P.-G. Reinhard. “Surprising Charge-Radius Kink in the Sc Isotopes at N=20,” Physical Review Letters (September 2023), APS. doi: 10.1103/PhysRevLett.131.102501\n\nNavrátil, P., K. Kravvaris, P. Gysbers, C. Hebborn, G. Hupin, and S. Quaglioni. “Ab Initio Investigations of A=8 Nuclei: α–α Scattering, Deformation in 8He, Radiative Capture of Protons on 7Be and 7Li and the X17 Boson,” Journal of Physics: Conference Series (September 2023), vol. 2586, IOP Publishing. doi: 10.1088/1742-6596/2586/1/012062\n\nNavrátil, P., and S. Quaglioni. “Ab Initio Nuclear Reaction Theory with Applications to Astrophysics,” Handbook of Nuclear Physics (September 2023), Springer, Singapore, pp. 1545-1590. doi: 10.1007/978-981-19-6345-2_7\n\nNicolae, B., T. Z. Islam, R. Ross, H. Van Dam, K. Assogba, P. Shpilker, M. Titov, M. Turilli, T. Wang, O. O. Kilic, S. Jha, and L. C. Pouchard. “Building the I (Interoperability) of FAIR for Performance Reproducibility of Large-Scale Composable Workflows in RECUP,” 2023 IEEE 19th International Conference on e-Science (e-Science) (September 2023), Limassol, Cyprus, IEEE. doi: 10.1109/e-Science58273.2023.10254808\n\nSaeedizade, E., R. Taheri, and E. Arslan. “I/O Burst Prediction for HPC Clusters Using Darshan Logs,” 2023 IEEE 19th International Conference on e-Science (e-Science) (September 2023), Limassol, Cyprus, IEEE. doi: 10.1109/e-Science58273.2023.10254871\n\nYalamanchi, K. K., S. Kommalapati, P. Pal, N. Kuzhagaliyeva, A. S. AlRamadan, B. Mohan, Y. Pei, S. M. Sarathy, E. Cenker, and J. Badra. “Uncertainty Quantification of a Deep Learning Fuel Property Prediction Model,” Applications in Energy and Combustion Science (September 2023), Elsevier. doi: 10.1016/j.jaecs.2023.100211\n\nZhang, C., F. Gygi, and G. Galli. “Engineering the Formation of Spin-Defects from First Principles,” Nature Communications (September 2023), Springer Nature. doi: 10.1038/s41467-023-41632-9\n\nOCTOBER\n\nBurrows, A., D. Vartanyan, and T. Wang. “Black Hole Formation Accompanied by the Supernova Explosion of a 40 M⊙ Progenitor Star,” The Astrophysical Journal (October 2023), IOP Publishing. doi: 10.3847/1538-4357/acfc1c\n\nChen, Y., E. M. Y. Lee, P. S. Gil, P. Ma, C. V. Amanchukwu, and J. J. de Pablo. “Molecular Engineering of Fluoroether Electrolytes for Lithium Metal Batteries,” Molecular Systems Design and Engineering (October 2023), Royal Society of Chemistry. doi: 10.1039/d2me00135g\n\nDatta, D., and M. S. Gordon. “Accelerating Coupled-Cluster Calculations with GPUs: An Implementation of the Density-Fitted CCSD(T) Approach for Heterogeneous Computing Architectures Using OpenMP Directives,” Journal of Chemical Theory and Computation (October 2023), ACS Publications. doi: 10.1021/acs.jctc.3c00876\n\nHarb, H., S. N. Elliott, L. Ward, I. T. Foster, S. J. Klippenstein, L. A. Curtiss, and R. S. Assary. “Uncovering Novel Liquid Organic Hydrogen Carriers: A Systematic Exploration of Chemical Compound Space Using Cheminformatics and Quantum Chemical Methods,” Digital Discovery (October 2023), Royal Society of Chemistry. doi: 10.1039/D3DD00123G\n\nHuang, S., and J. M. Cole. “ChemDataWriter: A Transformer-Based Toolkit for Auto-Generating Books That Summarise Research,” Digital Discovery (October 2023), Royal Society of Chemistry. doi: 10.1039/D3DD00159H\n\nKravvaris, K., P. Navrátil, S. Quaglioni, C. Hebborn, and G. Hupin. “Ab Initio Informed Evaluation of the Radiative Capture of Protons on 7Be,” Physics Letters B (October 2023), Elsevier. doi: 10.1016/j.physletb.2023.138156\n\nLiu, X., S. Jiang, A. Vasan, A. Brace, O. Gokdemir, T. Brettin, F. Xia, I. Foster, and R. Stevens. “DrugImprover: Utilizing Reinforcement Learning for Multi-Objective Alignment in Drug Optimization,” NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development (October 2023), New Orleans, LA, Neural Information Processing Systems Foundation.\n\nMinch, P., R. Bhattarai, and T. D. Rhone. “Data-Driven Study of Magnetic Anisotropy in Transition Metal Dichalcogenide Monolayers,” Solid State Communications (October 2023), Elsevier. doi: 10.1016/j.ssc.2023.115248\n\nVartanyan, D., and A. Burrows. “Neutrino Signatures of 100 2D Axisymmetric Core-Collapse Supernova Simulations,” Monthly Notices of the Royal Astronomical Society (October 2023), Oxford University Press. doi: 10.1093/mnras/stad2887\n\nWan, S., A. P. Bhati, and P. V. Coveney. “Comparison of Equilibrium and Nonequilibrium Approaches for Relative Binding Free Energy Predictions,” Journal of Chemical Theory and Computation (October 2023), ACS. doi: 10.1021/acs.jctc.3c00842\n\nZahariev, F., P. Xu, B. M. Westheimer, S. Webb, J. G. Vallejo, A. Tiwari, V. Sundriyal, M. Sosonkina, J. Shen, G. Schoendorff, M. Schlinsog, T. Sattasathuchana, K. Ruedenberg, L. B. Roskop, A. P. Rendell, D. Poole, P. Piecuch, B. Q. Pham, V. Mironov, J. Mato, S. Leonard, S. S. Leang, J. Ivanic, J. Hayes, T. Harville, K. Gururangan, E. Guidez, I. S. Gerasimov, C. Friedl, K. N. Ferreras, G. Elliott, D. Datta, D. Del Angel Cruz, L. Carrington, C. Bertoni, G. M. J. Barca, M. Alkan, and M. S. Gordon. “The General Atomic and Molecular Electronic Structure System (GAMESS): Novel Methods on Novel Architectures,” Journal of Chemical Theory and Computation (October 2023), ACS. doi: 10.1021/acs.jctc.3c00379\n\nZyvagin, M., A. Brace, K. Hippie, Y. Deng, B. Zhang, C. O. Bohorquez, A. Clyde, B. Kale, D. Perez-Rivera, H. Ma, C. M. Mann, M. Irvin, J. G. Pauloski, L. Ward, V. Hayot-Sasson, M. Emani, S. Foreman, Z. Xie, D. Lin, M. Shukla, W. Nie, J. Romero, C. Dallago, A. Vahdat, C. Xiao, T. Gibbs, I. Foster, J. J. Davis, M. E. Papka, T. Brettin, R. Stevens, A. Anandkumar, V. Vishwanath, and A. Ramanathan. “GenSLMs: Genome-Scale Language Models Reveal SARS-CoV-2 Evolutionary Dynamics,” The International Journal of High Performance Computing Applications (October 2023), SAGE Publications. doi: 10.1177/10943420231201154\n\nNOVEMBER\nAbbott, R., M. S. Albergo, A. Botev, D. Boyda, K. Cranmer, D. C. Hackett, A. G. D. G. Matthews, S. Racanière, A. Razavi, D. J. Rezende, F. Romero-López, P. E. Shanahan, and J. M. Urban. “Aspects of Scaling and Scalability for Flow-Based Sampling of Lattice QCD,” The European Physical Journal A (November 2023), Springer Nature. doi: 10.1140/epja/s10050-023-01154-w\n\nAntepara, O., S. Williams, H. Johansen, T. Zhao, S. Hirsch, P. Goyal, and M. Hall. “Performance Portability Evaluation of Blocked Stencil Computations on GPUs,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1007-1018. doi: 10.1145/3624062.3624177\n\nAntepara, O., S. Williams, S. Kruger, T. Bechtel, J. McClenaghan, and L. Lao. “Performance-Portable GPU Acceleration of the EFIT Tokamak Plasma Equilibrium Reconstruction Code,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1939-1948. doi: 10.1145/3624062.3624607\n\nBabu, A. V., T. Zhou, S. Kandel, T. Bicer, Z. Liu, W. Judge, D. J. Ching, Y. Jiang, S. Veseli, S. Henke, R. Chard, Y. Yao, E. Sirazitdinova, G. Gupta, M. V. Holt, I. T. Foster, A. Miceli, and M. J. Cherukara. “Deep Learning at the Edge Enables Real-Time Streaming Ptychographic Imaging,” Nature Communications (November 2023), Springer Nature. doi: 10.1038/s41467-023-41496-z\n\nBarik, R., S. Raskar, M. Emani, and V. Vishwanath. “Characterizing the Performance of Triangle Counting on Graphcore’s IPU Architecture,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1949-1957. doi: 10.1145/3624062.3624608\n\nBaughman, M., N. Hudson, R. Chard, A. Bauer, I. Foster, and K. Chard. “Tournament-Based Pretraining to Accelerate Federated Learning,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 109-115. doi: 10.1145/3624062.3626089\n\nBrace, A., R. Vescovi, R. Chard, N. D. Saint, A. Ramanathan, N. J. Zaluzec, and I. Foster. “Linking the Dynamic PicoProbe Analytical Electron-Optical Beam Line / Microscope to Supercomputers,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM. doi: 10.1145/3624062.3624614\n\nCendejas, M. C., O. A. P. Mellone, U. Kurumbail, Z. Zhang, J. H. Jansen, F. Ibrahim, S. Dong, J. Vinson, A. N. Alexandrova, D. Sokaras, S. R. Bare, and I. Hermans. “Tracking Active Phase Behavior on Boron Nitride during the Oxidative Dehydrogenation of Propane Using Operando X-ray Raman Spectroscopy,” Journal of the American Chemical Society (November 2023), ACS. doi: 10.1021/jacs.3c08679\n\nChakraborty, TC, J. Wang, Y. Qian, W. Pringle, Z. Yang, and P. Xue. “Urban Versus Lake Impacts on Heat Stress and Its Disparities in a Shoreline City,” GeoHealth (November 2023), John Wiley and Sons. doi: 10.1029/2023GH000869\n\nChen, L., X. Ding, M. Emani, T. Vanderbruggen, P.-H. Lin, and C. Liao. “Data Race Detection Using Large Language Models,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 215-223. doi: 10.1145/3624062.3624088\n\nChowdhury, S., F. Li, A. Stubbings, J. New, A. Garg, S. Correa, and K. Bacabac. “Bias Correction in Urban Building Energy Modeling for Chicago Using Machine Learning,” 2023 Fourth International Conference on Intelligent Data Science Technologies and Applications (IDSTA) (November 2023), Kuwai, Kuwait, IEEE. doi: 10.1109/idsta58916.2023.10317837\n\nDharuman, G., L. Ward, H. Ma, P. V. Setty, O. Gokdemir, S. Foreman, M. Emani, K. Hippe, A. Brace, K. Keipert, T. Gibbs, I. Foster, A. Anandkumar, V. Vishwanath, and A. Ramanathan. “Protein Generation via Genome-scale Language Models with Bio-physical Scoring,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 95-101. doi: 10.1145/3624062.3626087\n\nDing, X., L. Chen, M. Emani, C. Liao, P.-H. Lin, T. Vanderbruggen, Z. Xie, A. Cerpa, and W. Du. “HPC-GPT: Integrating Large Language Model for High-Performance Computing,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 951-960. doi: 10.1145/3624062.3624172\n\nDitte, M., M. Barborini, L. M. Sandonas, and A. Tkatchenko. “Molecules in Environments: Toward Systematic Quantum Embedding of Electrons and Drude Oscillators,” Physical Review Letters (November 2023), APS. doi: 10.1103/physrevlett.131.228001\n\nFox, D., J. M. M. Diaz, and X. Li. “A gem5 Implementation of the Sequential Codelet Model: Reducing Overhead and Expanding the Software Memory Interface,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 839-846. doi: 10.1145/3624062.3624152\n\nGrassi, A., H. G. Rinderknecht, G. F. Swadling, D. P. Higginson, H.-S. Park, A. Spitkovsky, and F. Fiuza. “Electron Injection via Modified Diffusive Shock Acceleration in High-Mach-Number Collisionless Shocks,” The Astrophysical Journal Letters (November 2023), IOP Publishing. doi: 10.3847/2041-8213/ad0cf9\n\nGu, C., Z. H. Sun, G. Hagen, and T. Papenbrock. “Entanglement Entropy of Nuclear Systems,” Physical Review C (November 2023), APS. doi: 10.1103/PhysRevC.108.054309\n\nGueroudji, A., J. Bigot, B. Raffin, and R. Ross. “Dask-Extended External Tasks for HPC/ML in Transit Workflows,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 831-838. doi: 10.1145/3624062.3624151\n\nHossain, K., R. Balin, C. Adams, T. Uram, K. Kumaran, V. Vishwanath, T. Dey, S. Goswami, J. Lee, R. Ramer, and K. Yamada. “Demonstration of Portable Performance of Scientific Machine Learning on High Performance Computing Systems,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 644-647. doi: 10.1145/3624062.3624138\n\nHuang, Y., S. Di, X. Yu, G. Li, and F. Cappello. “cuSZp: An Ultra-Fast GPU Error-Bounded Lossy Compression Framework with Optimized End-to-End Performance,” SC ‘23: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (November 2023), ACM, pp. 1-13. doi: 10.1145/3581784.3607048\n\nKanhaiya, K., M. Nathanson, P. J. in ‘t Veld, C. Zhu, I. Nikiforov, E. B. Tadmor, Y. K. Choi, W. Im, R. K. Mishra, and H. Heinz. “Accurate Force Fields for Atomistic Simulations of Oxides, Hydroxides, and Organic Hybrid Materials up to the Micrometer Scale,” Journal of Chemical Theory and Computation (November 2023), ACS. doi: 10.1021/acs.jctc.3c00750\n\nKéruzoré, F., L. E. Bleem, M. Buehlmann, J. D. Emberson, N. Frontiere, S. Habib, K. Heitmann, and P. Larsen. “Optimization and Quality Assessment of Baryon Pasting for Intracluster Gas using the Borg Cube Simulation,” The Open Journal of Astrophysics (November 2023), Maynooth Academic Publishing. doi: 10.21105/astro.2306.13807\n\nKumari, S., A. N. Alexandrova, and P. Sautet. “Nature of Zirconia on a Copper Inverse Catalyst Under CO2 Hydrogenation Conditions,” Journal of the American Chemical Society (November 2023), ACS. doi: 10.1021/jacs.3c09947\n\nLiu, M., C. Oh, J. Liu, L. Jiang, and Y. Alexeev. “Simulating Lossy Gaussian Boson Sampling with Matrix-Product Operators,” Physical Review A (November 2023), APS. doi: 10.1103/physreva.108.052604\n\nLykov, D., R. Shaydulin, Y. Sun, Y. Alexeev, and M. Pistoia. “Fast Simulation of High-Depth QAOA Circuits,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM. doi: 10.1145/3624062.3624216\n\nMartin, A., G. Liu, W. Ladd, S. Lee, J. Gounley, J. Vetter, S. Patel, S. Rizzi, V. Mateevitsi, J. Insley, and A. Randles. “Performance Evaluation of Heterogeneous GPU Programming Frameworks for Hemodynamic Simulations,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1126-1137. doi: 10.1145/3624062.3624188\n\nMateevitsi, V. A., M. Bode, N. Ferrier, P. Fischer, J. H. Göbbert, J. A. Insley, Y.-H. Lan, M. Min, M. E. Papka, S. Patel, S. Rizzi, and J. Windgassen. “Scaling Computational Fluid Dynamics: In Situ Visualization of NekRS using SENSEI,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 862-867. doi: 10.1145/3624062.3624159\n\nNarykov, O., Y. Zhu, T. Brettin, Y. Evrard, A. Partin, M. Shukla, P. Vasanthakumari, J. Doroshow, and R. Stevens. “Entropy-Based Regularization on Deep Learning Models for Anti-Cancer Drug Response Prediction,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 121-122. doi: 10.1145/3624062.3624080\n\nParraga, H., J. Hammonds, S. Henke, S. Veseli, W. Allcock, B. Côté, R. Chard, S. Narayanan, and N. Schwarz. “Empowering Scientific Discovery through Computing at the Advanced Photon Source,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 2126-2132. doi: 10.1145/3624062.3624612\n\nPauloski, J. G., V. Hayot-Sasson, L. Ward, N. Hudson, C. Sabino, M. Baughman, K. Chard, and I. Foster. “Accelerating Communications in Federated Applications with Transparent Object Proxies,” SC ‘23: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (November 2023), ACM. doi: 10.1145/3581784.3607047\n\nPautsch, E., J. Li, S. Rizzi, G. K. Thiruvathukal, and M. Pantoja. “Optimized Uncertainty Estimation for Vision Transformers: Enhancing Adversarial Robustness and Performance Using Selective Classification,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 391-394. doi: 10.1145/3624062.3624106\n\nPrince, M., D. Gürsoy, D. Sheyfer, R. Chard, B. Côté, H. Parraga, B. Frosik, J. Tischler, and N. Schwarz. “Demonstrating Cross-Facility Data Processing at Scale With Laue Microdiffraction,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 2133-2139. doi: 10.1145/3624062.3624613\n\nRangel, E. M., S. J. Pennycook, A. Pope, N. Frontiere, Z. Ma, and V. Madanath. “A Performance-Portable SYCL Implementation of CRK-HACC for Exascale,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1114-1125. doi: 10.1145/3624062.3624187\n\nRutter, C. M., P. N. de Lima, C. E. Maerzluft, F. P. May, and C. C. Murphy. “Black-White Disparities in Colorectal Cancer Outcomes: A Simulation Study of Screening Benefit,” JNCI Monographs (November 2023), Oxford University Press. doi: 10.1093/jncimonographs/lgad019\n\nShepard, C., and Y. Kanai. “Ion-Type Dependence of DNA Electronic Excitation in Water under Proton, α-Particle, and Carbon Ion Irradiation: A First-Principles Simulation Study,” The Journal of Physical Chemistry B (November 2023), ACS Publications. doi: 10.1021/acs.jpcb.3c05446\n\nSiefert, C. M., C. Pearson, S. L. Olivier, A. Prokopenko, J. Hu, and T. J. Fuller. “Latency and Bandwidth Microbenchmarks of US Department of Energy Systems in the June 2023 Top 500 List,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1298-1305. doi: 10.1145/3624062.3624203\n\nTräff, J. L., S. Hunold, I. Vardas, and N. M. Funk. “Uniform Algorithms for Reduce-Scatter and (Most) Other Collectives for MPI,” 2023 IEEE International Conference on Cluster Computing (CLUSTER) (November 2023), Santa Fe, NM, IEEE. doi: 10.1109/cluster52292.2023.00031\n\nUnderwood, R. R., S. Di, S. Jin, M. H. Rahman, A. Khan, and F. Cappello. “LibPressio-Predict: Flexible and Fast Infrastructure for Inferring Compression Performance,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 272-280. doi: 10.1145/3624062.3625124\n\nVasan, A., T. Brettin, R. Stevens, A. Ramanathan, and V. Vishwanath. “Scalable Lead Prediction with Transformers using HPC Resources,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 123. doi: 10.1145/3624062.3624081\n\nVeseli, S., J. Hammonds, S. Henke, H. Parraga, and N. Schwarz. “Streaming Data from Experimental Facilities to Supercomputers for Real-Time Data Processing,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 2110-2117. doi: 10.1145/3624062.3624610\n\nWan, S., A. P. Bhati, A. D. Wade, and P. V. Coveney. “Ensemble-Based Approaches Ensure Reliability and Reproducibility,” Journal of Chemical Information and Modeling (November 2023), ACS. doi: 10.1021/acs.jcim.3c01654\n\nWilkins, M., H. Wang, P. Liu, B. Pham, Y. Guo, R. Thakur, P. Dinda, and N. Hardavellas. “Generalized Collective Algorithms for the Exascale Era,” 2023 IEEE International Conference on Cluster Computing (CLUSTER) (November 2023), Santa Fe, NM, IEEE. doi: 10.1109/cluster52292.2023.00013\n\nZhang, C., B. Sun, X. Yu, Z. Xie, W. Zheng, K. A. Iskra, P. Beckman, and D. Tao. “Benchmarking and In-Depth Performance Study of Large Language Models on Habana Gaudi Processors,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1759-1766. doi: 10.1145/3624062.3624257\n\nZubair, M., A. Walden, G. Nastac, E. Nielsen, C. Bauinger, and X. Zhu. “Optimization of Ported CFD Kernels on Intel Data Center GPU Max 1550 using oneAPI ESIMD,” SC-W ‘23: Proceedings of the SC ‘23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis (November 2023), ACM, pp. 1705-1712. doi: 10.1145/3624062.3624251\n\nDECEMBER\nBabbar, A., S. Ragunathan, D. Mitra, A. Dutta, and T. K. Patra. “Explainability and Extrapolation of Machine Learning Models for Predicting the Glass Transition Temperature of Polymers,” Journal of Polymer Science (December 2023), John Wiley and Sons. doi: 10.1002/pol.20230714\n\nBarwey, S., V., Shankar, V. Viswanathan, and R. Maulik, “Multiscale Graph Neural Network Autoencoders for Interpretable Scientific Machine Learning,” Journal of Computational Physics (December 2023), Elsevier. doi: 10.1016/j.jcp.2023.112537\n\nChen, J. L., J. L. Prelesnik, B. Liang, Y. Sun, M. Bhatt, C. Knight, K. Mahesh, and J. I. Siepmann. “Large-Scale Molecular Dynamics Simulations of Bubble Collapse in Water: Effects of System Size, Water Model, and Nitrogen,” The Journal of Chemical Physics (December 2023), AIP Publishing. doi: 10.1063/5.0181781\n\nDing, H. T., X. Gao, A. D. Hanlon, S. Mukherjee, P. Petreczky, Q. Shi, S. Syritsyn, and Y. Zhao. “Lattice QCD Predictions of Pion and Kaon Electromagnetic Form Factors at Large Momentum Transfer,” The 40th International Symposium on Lattice Field Theory (LATTICE2023) (December 2023), Batavia, IL, Sissa Medialab. doi: 10.22323/1.453.0320\n\nDuarte, J., H. Li, A. Roy, R. Zhu, E. A. Huerta, D. Diaz, P. Harris, R. Kansal, D. S. Katz, I. H. Kavoori, V. V. Kindratenko, F. Mokhtar, M. S. Neubauer, S. E. Park, M. Quinnan, R. Rusack, and Z. Zhao. “FAIR AI Models in High Energy Physics,” Machine Learning: Science and Technology (December 2023), IOP Publishing. doi: 10.1088/2632-2153/ad12e3\n\nForeman, S., X.-Y. Jin, and J. C. Osborn. “MLMC: Machine Learning Monte Carlo for Lattice Gauge Theory,” The 40th International Symposium on Lattice Field Theory (LATTICE2023) (December 2023), Batavia, IL, Sissa Medialab. doi: 10.22323/1.453.0036\n\nHackett, D. C., P. R. Oare, D. A. Pefkou, and P. E. Shanahan. “Gravitational Form Factors of the Pion from Lattice QCD,” Physical Review D (December 2023), APS. doi: 10.1103/physrevd.108.114504\n\nNarykov, O., Y. Zhu, T. Brettin, Y. A. Evrard, A. Partin, M. Shukla, F. Xia, A. Clyde, P. Vasanthakumari, J. H. Doroshow, and R. L. Stevens. “Integration of Computational Docking into Anti-Cancer Drug Response Prediction Models,” Cancers (December 2023), MDPI. doi: 10.3390/cancers16010050\n\nSarkar, A., D. Lee, and U.-G. Meißner. “Floating Block Method for Quantum Monte Carlo Simulations,” Physical Review Letters (December 2023), APS. doi: 10.1103/PhysRevLett.131.242503\n\nTian, M., E. A. Huerta, and H. Zheng. “AI Ensemble for Signal Detection of Higher Order Gravitational Wave Modes of Quasi-Circular, Spinning, Non-Precessing Binary Black Hole Mergers,” 2023 Workshop on Machine Learning and the Physical Sciences (December 2023), New Orleans, LA, Neural Information Processing Systems Foundation. doi: 10.48550/arXiv.2310.00052\n\nWallace, B. C., A. M. Haberlie, W. S. Ashley, V. A. Gensini, and A. C. Michaelis. “Decomposing the Precipitation Response to Climate Change in Convection Allowing Simulations over the Conterminous United States,” Earth and Space Science (December 2023), John Wiley and Sons. doi: 10.1029/2023ea003094\n\nWang, H.-H., S.-Y. Moon, H. Kim, G. Kim, W.-Y. Ah, Y. Y. Joo, and J. Cha. “Early Life Stress Modulates the Genetic Influence on Brain Structure and Cognitive Function in Children,” Heliyon (December 2023), Cell Press. doi: 10.1016/j.heliyon.2023.e23345\n\nWildenberg, G., H. Li, V. Sampathkumar, A. Sorokina, and N. Kasthuri. “Isochronic Development of Cortical Synapses in Primates and Mice,” Nature Communications (December 2023), Springer Nature. doi: 10.1038/s41467-023-43088-3\n\nYe, Z., F. Gygi, and G. Galli. “Raman Spectra of Electrified Si–Water Interfaces: First-Principles Simulations,” The Journal of Physical Chemistry Letters (December 2023), ACS. doi: 10.1021/acs.jpclett.3c03122",
          "url": "http://localhost:4000//science/publications"
        },
  
    
      "search-html": {
          "title": "Search Results",
          "content": "Back to Top &uarr;",
          "url": "http://localhost:4000//search.html"
        },
  
    
      "features": {
          "title": "Features",
          "content": "",
          "url": "http://localhost:4000//features"
        },
  
    
      "expertise-and-resources": {
          "title": "Expertise and Resources",
          "content": "",
          "url": "http://localhost:4000//expertise-and-resources"
        },
  
    
      "science": {
          "title": "Science",
          "content": "",
          "url": "http://localhost:4000//science"
        },
  
    
      "community-and-outreach": {
          "title": "Growing the HPC Community",
          "content": "",
          "url": "http://localhost:4000//community-and-outreach"
        },
  
    
      "expertise-and-resources-staff-news": {
          "title": "Staff News",
          "content": "ALCF Researchers Contribute to Gordon Bell Prize Finalist Study\n\nAn Argonne-led team was selected as a finalist for the 2024 ACM Gordon Bell Prize for their work to develop MProt-DPO, an AI-driven protein design framework. The team’s study, “MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization,” leveraged five of the world’s leading HPC systems, including the ALCF’s Aurora exascale supercomputer, to develop and demonstrate a scalable, end-to-end workflow for accelerating the discovery of new proteins for medicine, catalysis, and other applications. The team includes Argonne researchers Gautham Dharuman, Kyle Hippe, Alexander Brace, Sam Foreman, Väinö Hatanpää, Varuni Sastry, Huihuo Zheng, Logan Ward, Servesh Muralidharan, Archit Vasan, Bharat Kale, Carla Mann, Heng Ma, Murali Emani, Michael Papka, Ian Foster, Rick Stevens, and Venkatram Vishwanath. Additional contributors include Yun-Hsuan Cheng, Yuliana Zamora, and Tom Gibbs (NVIDIA); Shengchao Liu (University of California, Berkeley); Chaowei Xiao (University of Wisconsin-Madison); Mahidhar Tatineni (San Diego Supercomputing Center); and Deepak Canchi, Jerome Mitchell, Koichi Yamada, and Maria Garzaran (Intel).\n\nALCF Team Receives Best Paper Award at ISAV 2024\n\nA team including ALCF researchers received the Best Paper Award at the SC24 conference’s In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization (ISAV 2024) Workshop. The paper, “Bridging Gaps in Simulation Analysis through a General Purpose, Bidirectional Steering Interface with Ascent,” was authored by Argonne’s Victor Mateevitsi, Silvio Rizzi, Joseph Insley, Michael Papka, Thomas Marrinan, and Dimitrios Fytanidis, along with Utah State University’s Andres Sewell and Steve Petruzza, and Cyrus Harrison and Nicole Marsaglia of Lawrence Livermore National Laboratory. Sewell, the lead author, has been a summer student at the ALCF for the past two years. The paper introduces a framework for adding interactive, human-in-the-loop steering controls to existing simulation codes. This capability allows scientists to pause, adjust, and resume large-scale simulations without starting over.\n\nALCF-Led Team Receives Best Paper Award at PMBS24\n\nA team of researchers led by ALCF staff received the Best Paper Award at SC24’s Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS24) Workshop. The paper, “Ponte Vecchio Across the Atlantic: Single-Node Benchmarking of Two Intel GPU Systems,” provides micro-benchmarking data from applications running on the ALCF’s Aurora supercomputer and another Intel GPU-powered system, housed at the University of Cambridge. Argonne’s Thomas Applencourt, Servesh Muralidharan, Colleen Bertoni, JaeHyuk Kwack, Ye Luo, Esteban Rangel, John Tramm, and Yasaman Ghadar authored the paper in collaboration with University of Bristol’s Aditya Sadawarte and Tom Deakin, and University of Cambridge’s Christopher Edsall.\n\nAurora Install Team Recognized for Outstanding Safety Performance\n\nAs part of the 2024 Argonne Board of Governors Awards, the Aurora installation team received the James B. Porter, Jr. Team Award for Outstanding Safety Performance, which recognizes teams that embody the principles of integrated safety management and contribute to a positive safety culture. Honored for their efforts in safely installing and preparing Argonne’s Aurora exascale system, team members included ALCF’s William Allcock, Jonathan Bouvet, Susan Coghlan, Joseph Crawford, Gregory Cross, Jeff Goetz, Michael Hogan, Carissa Holohan, Ti Leggett, and Haseebuddin Syed in collaboration with contributors from across the laboratory, including Christopher Baltas, Adena Banas, Erika Gutierrez, Cari Helberg, William Lucnik, Mitchell McClellan, Lisa Polowy, Raihan Rahman, Dillon Roark, Dana Silvestri, Michael Talamonti, and Jeremy Young.\n\nPapka Named UIC Warren S. McCulloch Professor of Computer Science, Director of Electronic Visualization Laboratory\n\nALCF Director Michael Papka was named the Warren S. McCulloch Professor of Computer Science at the University of Illinois Chicago, an endowed professorship recognizing leadership and impact in the field while supporting continued research and teaching. In addition to this honor, Papka was also appointed director of UIC’s Electronic Visualization Laboratory (EVL), an interdisciplinary research space focused on collaborative visualization, virtual reality, and advanced computing and networking infrastructure.\n\nECP Team Earns DOE Secretary of Energy Achievement Award\n\nThe U.S. Department of Energy (DOE) recognized the Exascale Computing Project (ECP) with the Secretary of Energy Honor Achievement Award for its role in delivering an exascale computing ecosystem for the nation. The multi-lab collaboration involved several key staff from Argonne, including Lois Curfman McInnes, David Martin, Susan Coghlan, Todd Munson, and Yasaman Ghadar, who were on hand to receive the award as part of the ECP leadership team.\n\nShilpika Earns Recognition in Argonne’s Postdoc Research Slam\n\nALCF postdoctoral researcher Shilpika tied for third place in Argonne’s 2024 Postdoc Research Slam with her presentation, “Alice in the Data Labyrinth: Solving Supercomputer Mysteries Through Intelligent Visuals.” The event showcased research from postdoctoral scholars in three-minute pitches before a live audience and panel of judges. Shilpika later presented her work as part of the Argonne OutLoud lecture series event, “Global Problem Solvers: Early-Career Scientists Explore New Frontiers to Unleash Discoveries for Today and Tomorrow.”\n\nMateevitsi Named Senior Member of IEEE and ACM\n\nIn 2024, Victor Mateevitsi, ALCF assistant computer scientist, was named a senior member of both the Institute of Electrical and Electronics Engineers (IEEE) and the Association for Computing Machinery (ACM), recognizing his professional experience and contributions to the field.\n\nPapka Honored with IEEE Chicago Section Award\n\nALCF Director Michael Papka received the Institute of Electrical and Electronics Engineers (IEEE) Chicago Section’s Distinguished Senior Research and Development Award, recognizing his contributions to high-performance computing. Argonne National Laboratory was also honored with the Friends of the IEEE Chicago Section Award, highlighting the lab’s collaborative efforts with the organization.\n\nPapka Receives NIU Distinguished Alumni Award\n\nALCF Director Michael Papka received the Distinguished Alumni Award from Northern Illinois University’s College of Liberal Arts and Sciences. The award honors alumni who have made significant contributions in their professional field or through civic, cultural, or charitable service.\n\nResearch Team Using ALCF Resources Wins HPCwire Award for Best Use of HPC in Physical Sciences\n\nA team with researchers from Argonne National Laboratory, as well as University of Chicago, University of Illinois-Urbana Champaign, National Center for Supercomputing Applications, and University of Minnesota, received the HPCwire Editors’ Choice Award for Best Use of HPC in Physical Sciences for work utilizing ALCF resources including the Polaris supercomputer and the Globus data management platform. This work was aimed at developing a physics-informed transformer model to predict gravitational wave evolution for spinning binary black hole mergers. The team’s AI approach dramatically reduces simulation time from days to seconds, handling terabyte-scale datasets with high accuracy.\n\nALCF-APS Pipeline Honored with HPCwire Readers’ Choice Award\n\nAn automated pipeline integrating ALCF supercomputers and Advanced Photon Source instruments to enable near-real-time data analysis was named the Best HPC in the Cloud Use Case, winning an HPCwire Readers’ Choice Award. The pipeline allows scientists to adjust experiments on the fly, potentially accelerating scientific breakthroughs by delivering rapid results while researchers still have facility access.",
          "url": "http://localhost:4000//expertise-and-resources/staff-news"
        },
  
    
      "expertise-and-resources-staff-spotlights": {
          "title": "Staff Spotlights",
          "content": "Riccardo Balin, Assistant Computational Scientist\n\nRiccardo Balin joined the ALCF in 2021 as a postdoc under the Aurora Early Science Program (ESP) working on methods to incorporate machine learning with computational fluid dynamics (CFD) and turbulence modeling simulations. In 2024, he joined the Data Services and Workflows team as an assistant computational scientist. In this role, Riccardo works with ALCF users on developing, scaling and applying novel workflows which aim to accelerate traditional simulations with AI methods. He also works closely with HPC vendors, ensuring the necessary software tools as well as new innovative solutions are available to our users.\n\nIn the last year, Riccardo continued his involvement in the Aurora ESP project aimed at developing ML-based closure models from ongoing simulations of turbulent aerodynamic flows. Thanks to his efforts, the project demonstrated INCITE readiness on the new Aurora supercomputer. Additionally, Riccardo spearheaded an Argonne led effort to incorporate AI methods into the nekRS CFD code called nekRS-ML. This collaboration now includes researchers from four divisions at Argonne and led to a new ALCC proposal for an AI-powered toolkit to accelerate simulation-based design space exploration. Riccardo also fostered new collaborations spanning across DOE labs to develop benchmarks for coupled simulation and AI workflows in order to better understand how these workloads can take full advantage of current and future HPC systems.\n\nMurali Emani, Assistant Computational Scientist\n\nMurali Emani is a Computer Scientist in the Artificial Intelligence and Machine Learning (AIML) group with the Argonne Leadership Computing Facility.\n\nMurali develops performance models to identify and address bottlenecks while scaling machine learning and deep learning frameworks on emerging supercomputers for scientific applications. He also co-designs emerging hardware architectures to scale up machine learning algorithms.\n\nMurali co-leads the AI Testbed where he explores the performance, efficiency of AI accelerators for scientific machine learning applications. He works in close collaboration with existing vendors to influence their future product offerings and also potential vendors the help procurement.  He also organized various training sessions and  tutorials on programming the AI Testbed accelerators at various venues such as SC that gained tremendous response from the community.\n\nMurali is also a core member of the  Model Architecture and Performance Evaluation working group in the Trillion Parameter Consortium (TPC) and organized tutorials and hackathon sessions at TPC event. He is actively engaged in the AuroraGPT project that aims to develop open science foundation model across various science disciplines. As a technical committee member of the first NAIRR workshop, he chaired break-out sessions to understand the gaps and goals for performance of AI applications.\n\nMurali was also a part of SC24 GordonBell paper finalist (MProt-DPO). He serves as a co-chair for MLPerf HPC group at MLCommons to benchmark large scale ML on HPC systems overseeing group of scientist and engineers from HPC centers, industry and academia to identify state-of-art applications to help benchmark diverse supercomputers. He also has been actively engaged in Aurora exascale machine where I am the POC for two benchmark applications, and work with DL frameworks team at Intel to evaluate the software stack and work to optimize performance at scale.\n\n\n\nVaruni Sastry, Assistant Computational Scientist\n\nVaruni Sastry joined Argonne in the Data Science and Learning Division in 2021 as a predoctoral appointee, primarily working on AI Testbeds with a focus on evaluating and benchmarking different AI and ML workloads on next generation dataflow based hardware. She also assisted ALCF users in deploying different scientific AI workloads on these accelerators. In 2023, she officially joined the ALCF as an Assistant Computer scientist and continues to lead several efforts in enabling AI for Science workloads on AI Testbed and other supercomputers at the ALCF.\n\nIn 2024, Varuni joined the AuroraGPT pre-training team, setting up the data processing pipeline, and developing several key features for distributed training framework for scalable language and vision models. Varuni contributed to three different Gordon Bell submissions for 2024, and the “MProt-DPO” work on multimodal protein design workflow was selected as a Gordon Bell finalist. She was awarded an Impact Argonne Award for the Enhancement of Argonne’s Reputation for her contribution. She was also honored with an Impact Argonne Award for Extraordinary Efforts for her contribution to “AuroraGlimmer”, an effort to build a scalable pipeline for the AuroraGPT project. In collaboration with the CNM and APS teams, she developed a RAG based LLM based chat framework tailored for scientific facilities and this work was published at the Nature Partner Journal (NPJ) computational materials.\n\nAs part of outreach activities, Varuni co-organized several tutorial and workshops on AI Testbeds including sessions at SuperComputing’24, Argonne Training Program on Extreme-Scale Computing (ATPESC) and other ALCF events. She also delivered an invited talk at NNSA Emergent Technology Seminar. In addition, she co-organized INCITE GPU hackathon and served as a reviewer for both ATPESC’24 and INCITE’25 committees.\n\nChristine Simpson, Assistant Computational Scientist\n\nChristine Simpson joined the ALCF in 2022 as an Assistant Computational Scientist in the Datascience group. Now part of the Data Services and Workflows group, she primarily focuses on workflows on ALCF systems, which involves development and testing of workflow tools and user support and training. She is currently the lead developer for Balsam, an ALCF-developed workflow package that has enabled users to deploy high-throughput workflows on ALCF systems. She also works closely with the Parsl and Globus teams to help optimize their tools and services for ALCF users.\n\nIn the past year, Christine has been heavily involved in ALCF efforts in Integrated Research Infrastructure (IRI), an initiative to connect DOE’s experimental and computational facilities. She has been ALCF’s point person in a collaboration with the DIII-D National Fusion Facility and NERSC. She led efforts to run a production IRI workflow on Polaris analyzing the dynamics of neutral beams within the DIII-D tokamak during its fall campaign. This workflow was demonstrated on Polaris analyzing live DIII-D experiments during SC24 along with a concurrently running IRI workflow at NERSC. In addition to DIII-D, Christine works with a number of other IRI users analyzing data on ALCF systems.\nChristine has also worked on exploring new approaches for coupling simulation and AI/ML codes on ALCF systems. She has co-led efforts to explore a new workflow and data management tool called Dragon developed by HPE. She has worked to port a drug discovery workflow from the CANDLE project to Dragon and presented on this effort at PASC24. She has worked closely with HPE developers to improve Dragon performance and features for ALCF applications.\n\nChristine is also the postdoc hiring lead for ALCF. Prior to joining ALCF, she received her PhD in Astronomy studying galaxy formation and evolution with numerical simulations.\n\n\n\nSheeja Susan, Software Development Specialist\n\nSheeja Susan joined the ALCF in 2019 as a Software Development Specialist under the Advanced Integration Group. She was a contractor for 5 years and became a full-time employee in 2024.\n\nAt Argonne, Sheeja is responsible for developing and maintaining frontend screens within the ALCF portal and Allocation Request Management websites. She also creates test automation scripts to ensure the website pages run correctly. Sheeja was part of the team for the migration process from AngularJS to Angular for ALCF. As an individual player, she has developed ALCF screens such as Director’s Discretionary Allocation Request, View Systems and many pages for the administrators, and helped with the page routing. She works closely with UX/design teams and effectively translates design concepts into functional web pages. Coordinating with the backend team, Sheeja creates modular, reusable code components for the development of user-friendly, responsive pages across various screen sizes. She has written numerous test scripts in Cypress to test the functionality of webpages and helped to reduce the number of failing ALCF test scripts significantly.\n\nIn 2024, Sheeja mainly worked on the frontend development of Allocation Request Management (ARM) website and the new version of Director’s Discretionary Allocation Request form (DD Allocation Request form) for ALCF. The ALCF Allocations Committee reviews requests from the DD Allocation Request form through the Allocation Request Management site. The ARM site is the internal clearinghouse for ALCF staff to view, analyze, and ultimately approve allocation requests. Within the site, the Committee reviews a project’s goals and needs to discern whether the project fits with the ALCF resources and mission. Sheeja also participated in the ALCF portal development - to replace UB3 home page and login section with that of Portal, to create functionalities for the redirection of UB3 specific URLs from email links, and to modify UB3 menu with the same look and feel of the Portal menu. Lastly, she was also able to successfully implement Angular Route Guards to activate/deactivate the navigation to specific routes in ALCF.\n\nPeter Upton, Systems Integration Administrator/Support\n\nPeter Upton is a Systems Integration Administrator/Support at the ALCF.  Peter manages, supports, and updates ALCF GitLab installations. He also assist in managing DNS and Salt systems, providing peer work reviews, troubleshoot GitLab user issues, and supported Aurora work by creating a new node type for administrative tasks.\n\nPeter assist vendors in using Aurora to improve functionality and stability, as well as assisting and tracking work around a new ALCF testbed. He assist his coworkers with HPCM-related tasks, and participates in miscellaneous data-center physical activities (e.g., remote troubleshooting, cable routing). Peter also addresses security vulnerabilities promptly, keeping documentation up-to-date, and coordinating GitLab usage, licensing, and upgrades.\n\nIn 2024, Upton  upgraded GitLab instances to RHEL9 and GitLab runners to RHEL9. He configured GitLab JLSE container registry. Peter also reworked the GitLab upgrade process to minimize downtime and increase consistency. He also automated Jacamar-CI RPM import.",
          "url": "http://localhost:4000//expertise-and-resources/staff-spotlights"
        },
  
    
      "expertise-and-resources-systems": {
          "title": "ALCF Systems",
          "content": "Supercomputing Resources\n\nALCF supercomputing resources support large-scale, computationally intensive projects aimed at solving some of the world’s most complex and challenging scientific problems.\n\n\n\n\n  \n    \n      System Name\n      Purpose\n      Architecture\n      Peak Performance\n      Processors per Node\n      GPUs per Node\n      Nodes\n      Cores\n      Memory\n      Interconnect\n      Racks\n    \n  \n  \n    \n      Aurora\n      Purpose Science Campaigns\n      Architecture HPE Cray EX\n      Peak Performance 2 EF\n      Processors per Node 2 Intel Xeon CPU Max Series processors\n      GPUs per Node 6 Intel Data Center GPU Max Series\n      Nodes 10,624CPUs: 21,248GPUs: 63,744\n      Cores 9,264,128\n      Memory 20.4 PB\n      Interconnect HPE Slingshot 11 with Dragonfly Configuration\n      Racks 166\n    \n    \n      Polaris\n      Purpose Science Campaigns\n      Architecture HPE Apollo 6500 Gen10+\n      Peak Performance 34 PF; 44 PF of Tensor CoreFP64 performance\n      Processors per Node 3rd Gen AMD EPYC\n      GPUs per Node 4 NVIDIA A100 Tensor Core\n      Nodes 560\n      Cores 17,920\n      Memory 280 TB (DDR4); 87.5 TB (HBM)\n      Interconnect HPE Slingshot 11 with Dragonfly configuration\n      Racks 40\n    \n    \n      Sophia\n      Purpose Science Campaigns\n      Architecture NVIDIA DGX A100\n      Peak Performance 3.9 PF\n      Processors per Node 2 AMD EPYC 7742\n      GPUs per Node 8 NVIDIA A100 Tensor Core\n      Nodes 24\n      Cores 3,072\n      Memory 26 TB (DDR4); 8.32 TB (GPU)\n      Interconnect NVIDIA HDR IniniBand\n      Racks 7\n    \n  \n\n\nALCF AI Testbed\n\nThe ALCF AI Testbed provides an infrastructure of next-generation AI-accelerator machines for research campaigns at the intersection of AI and science. AI testbeds include:\n\n\n  \n    \n      System Name\n      System Size\n      Compute Units per Accelerator\n      Estimated Performance of a Single Accelerator (TFlops)\n      Software Stack Support\n      Interconnect\n    \n  \n  \n    \n      Cerebras CS-2\n      2 Nodes (Each with a Wafer-Scale Engine) Including MemoryX and SwarmX\n      850,000 Cores\n      &gt; 5,780 (FP16)\n      Cerebras SDK, TensorFlow, PyTorch\n      Ethernet-based\n    \n    \n      SambaNova Cardinal SN30\n      64 Accelerators (8 Nodes and 8 Accelerators per Node)\n      1,280 Programmable Compute Units\n      &gt;660 (BF16)\n      SambaFlow, PyTorch\n      Ethernet-based\n    \n    \n      GroqRack\n      72 Accelerators (9 Nodes and 8 Accelerators per Node)\n      5,120 Vector ALUs\n      &gt;188 (FP16) &gt;750 (INT8)\n      GroqWare SDK, ONNX\n      RealScale™\n    \n    \n      Graphcore Bow Pod-64\n      64 Accelerators (4 Nodes and 16 Accelerators per Node)\n      1,472 Independent Processing Units\n      &gt;250 (FP16)\n      PopART, TensorFlow, PyTorch, ONNX\n      IPU Link\n    \n    \n      Habana Gaudi-1\n      16 Accelerators (2 Nodes and 8 Accelerators per Node)\n      8 TPC + GEMM Engine\n      &gt;150 (FP16)\n      SynapseAI, TensorFlow, PyTorch\n      Ethernet-based\n    \n  \n\n\nData Storage Systems\n\nALCF disk storage systems provide intermediate-term storage for users to access, analyze, and share computational and experimental data. Tape storage is used to archive data from completed projects.\n\n\n\n\n  \n    \n      System Name\n      File System\n      Storage System\n      Usable Capacity\n      Sustained Data Transfer Rate\n      Disk Drives\n    \n  \n  \n    \n      Eagle\n      File System Lustre\n      Storage System HPE ClusterStor E1000\n      Usable Capacity 100 PB\n      Sustained Data Transfer Rate 650 GB/s\n      Disk Drives 8,480\n    \n    \n      Grand\n      File System Lustre\n      Storage System HPE ClusterStor E1000\n      Usable Capacity 100 PB\n      Sustained Data Transfer Rate 650 GB/s\n      Disk Drives 8,480\n    \n    \n      Swift\n      File System Lustre\n      Storage System All NVMe Flash Storage Array\n      Usable Capacity 123 TB\n      Sustained Data Transfer Rate 48 GB/s\n      Disk Drives 24\n    \n    \n      Tape Storage\n      File System –\n      Storage System LT06 and LT08 Tape Technology\n      Usable Capacity 300 PB\n      Sustained Data Transfer Rate –\n      Disk Drives –\n    \n  \n\n\nNetworking\n\nNetworking is the fabric that ties all of the ALCF’s computing systems together. InfiniBand enables communication between system I/O nodes and the ALCF’s various storage systems. The Production HPC SAN is built upon NVIDIA Mellanox High Data Rate (HDR) InfiniBand hardware. Two 800-port core switches provide the backbone links between 80 edge switches, yielding 1600 total available host ports, each at 200 Gbps, in a non-blocking fat-tree topology. The full bisection bandwidth of this fabric is 320 Tbps. The HPC SAN is maintained by the NVIDIA Mellanox Unified Fabric Manager (UFM), providing Adaptive Routing to avoid congestion, as well as the NVIDIA Mellanox Self-Healing Interconnect Enhancement for InteLligent Datacenters (SHIELD) resiliency system for link fault detection and recovery.\n\nWhen external communications are required, Ethernet is the interconnect of choice. Remote user access, systems maintenance and management, and high-performance data transfers are all enabled by the Local Area Network (LAN) and Wide Area Network (WAN) Ethernet infrastructure.\n\nThis connectivity is built upon a combination of Extreme Networks SLX and MLXe routers and NVIDIA Mellanox Ethernet switches. \nALCF systems connect to other research institutions over multiple 100 Gbps connections that link to many high-performance research networks, including regional networks like the Metropolitan Research and Education Network (MREN), as well as national and international networks like the Energy Sciences Network (ESnet) and Internet2.\n\nJoint Laboratory for System Evaluation\n\nArgonne’s Joint Laboratory for System Evaluation (JLSE) provides access to leading-edge testbeds for research aimed at evaluating future extreme-scale computing systems, technologies, and capabilities. Here is a partial listing of the novel technology that make up the JLSE.\n\n\n  Florentia: Test and development system equipped with early versions of the Sapphire Rapids CPUs and Ponte Vecchio GPUs that power Aurora\n  Arcticus, DevEP, Iris: Intel discrete and integrated GPU testbeds to\nsupport the development, optimization, and scaling of applications and software for Aurora\n  Aurora Software Development Kit: Frequently updated version of the publicly available Intel oneAPI toolkit for Aurora development\n  Arm Ecosystem: Apollo 80 Fujitsu A64FX Arm system, NVIDIA Ampere Arm and A100 test kits, and an HPE Comanche with Marvell ARM64 CPU platform provide an ecosystem for porting applications and measuring performance on next-generation systems\n  Presque: Intel DAOS nodes for testing the Aurora storage system\n  Edge Testbed: NVIDIA Jetson Xavier and Jetson Nano platforms provide a resource for testing and developing edge computing applications\n  NVIDIA and AMD GPUs: Clusters of NVIDIA V100, A100, and A40 GPUs, and AMD MI50 and MI100 GPUs for preparing applications for heterogeneous computing architectures\n  NVIDIA Bluefield-2 DPU SmartNICs: Platform used for confidential computing, MPICH offloading, and APS data transfer acceleration\n  NextSilicon Maverick: First-generation product being tested by Argonne researchers\nAtos Quantum Learning Machine: Platform for testing and developing quantum algorithms and applications",
          "url": "http://localhost:4000//expertise-and-resources/systems"
        },
  
    
      "expertise-and-resources-team": {
          "title": "ALCF Team",
          "content": "Operations\nThe ALCF’s HPC systems administrators manage and support all ALCF computing systems, ensuring users have stable, secure, and highly available resources to pursue their scientific goals. This includes the ALCF’s production supercomputers, AI accelerators, supporting system environments, storage systems, and network infrastructure. The team’s software developers create tools to support the ALCF computing environment, including software for user account and project management, job failure analysis, and job scheduling. User support specialists provide technical assistance to ALCF users and manage the workflows for user accounts and projects. In the business intelligence space, staff data architects assimilate and verify ALCF data to ensure accurate reporting of facility information.\n\nScience\nComputational scientists with multidisciplinary domain expertise work directly with ALCF users to maximize and accelerate their research efforts. In addition, the ALCF team applies broad expertise in data science, machine learning, data visualization and analysis, and mathematics to help application teams leverage ALCF resources to pursue data-driven discoveries. With a deep knowledge of the ALCF computing environment and experience with a wide range of numerical methods, programming models, and computational approaches, staff scientists and performance engineers help researchers optimize the performance and productivity of simulation, data, and learning applications on ALCF systems.\n\nTechnology\nThe ALCF team plays a key role in designing and validating the facility’s next-generation supercomputers. By collaborating with compute vendors and the performance tools community, staff members ensure the requisite programming models, tools, debuggers, and libraries are available on ALCF platforms. The team also helps manage Argonne’s Joint Laboratory for System Evaluation, which houses next-generation testbeds that enable researchers to explore and prepare for emerging computing technologies. ALCF computer scientists, performance engineers, and software engineers develop and optimize new tools and capabilities to facilitate science on the facility’s current and future computing resources. This includes the deployment of scalable machine learning frameworks, in-situ visualization and analysis capabilities, data management services, workflow packages, and container technologies. In addition, the ALCF team is actively involved in programming language standardization efforts and contributes to cross-platform libraries to further enable the portability of HPC applications.\n\nOutreach\nALCF staff members organize and participate in training events that prepare researchers for efficient use of leadership computing systems. They also participate in a wide variety of educational activities aimed at cultivating a diverse and skilled HPC community and workforce in the future. In addition, staff outreach efforts include facilitating partnerships with industry and academia, and communicating the impactful research enabled by ALCF resources to external audiences.",
          "url": "http://localhost:4000//expertise-and-resources/team"
        },
  
    
      "community-and-outreach-user-training-activities": {
          "title": "Training ALCF Users",
          "content": "ALCF AI Testbed Training Workshops\nStarting April of 2024, the ALCF hosted a series of training workshops that introduced researchers to the novel AI accelerators deployed at the ALCF AI Testbed. The four individual workshops demonstrated to participants the architecture and software of the SambaNova DataScale SN30 system, the Cerebras CS-2 system, the Graphcore Bow Pod system, and the GroqRack system.\n\nALCF Hands-on HPC Workshop\nHeld in October at Argonne, the ALCF Hands-on HPC Workshop is designed to help attendees boost application performance on ALCF systems. The three-day workshop provided an opportunity for hands-on time on Polaris and AI Testbeds focusing on porting applications to heterogeneous architectures (CPU + GPU), improving code performance, and exploring AI/ML applications development on ALCF systems. For a recap of the 2024 event, read the article on our website.\n\n\n\nALCF INCITE GPU Hackathon\nIn May, the ALCF hosted its GPU Hackathon for the fourth time, a hybrid event designed to help developers accelerate their codes on ALCF resources and prepare for the INCITE call for proposals. The multi-day hackathon gave attendees access to ALCF’s Polaris system. Out of the 20 teams that participated this year, 11 teams were part of ongoing INCITE projects, and out of the 9 remaining teams, six teams applied for 2025.  Teams involved were researching a vast array of topics including genome structure and function, deep protein design for synthetic biology, and genetics data. For a recap of the 2024 event, read the article on our website.\n\nATPESC 2024\nThe annual Argonne Training Program on Extreme-Scale Computing (ATPESC) marked its 12th year in 2024. The two-week event offers training on key skills, approaches, and tools needed to design, implement, and execute computational science and engineering applications on high-end computing systems, including exascale supercomputers. The program features talks from leading computer scientists and HPC experts; and hands-on training using DOE leadership-class systems at ALCF, OLCF, and NERSC. In 2024, ATPESC attracted 75 attendees from 54 different institutions worldwide.\n\nAurora Early Science Program Workshops (ESP)\nThe Intel Center of Excellence (COE), in collaboration with ALCF’s Early Science Program, held multiday events where select ESP and ECP project teams worked on developing, porting, and profiling their codes on Sunspot with help from Intel and Argonne experts. The events were geared toward developers and emphasized using the Intel software development kit to get applications running on testbed hardware. Teams were also given the opportunity to consult with ALCF staff and provide feedback. ALCF staff also held dedicated office hours on a range of topics from programming models to profiling tools.\n\n2024 INCITE Proposal Writing Webinars\nIn spring, the INCITE program, ALCF, and the Oak Ridge Leadership Computing Facility (OLCF) jointly hosted two webinars on effective strategies for writing an INCITE proposal.\n\nMonthly ALCF Webinars\nThe ALCF continued to host the monthly ALCF Developer Sessions, aimed at training researchers and increasing the dialogue between HPC users and the developers of leadership-class systems and software. Speakers in the series included developers from Argonne, covering topics such as deep learning frameworks on Aurora, Intel performance profiling tools on Aurora, remote workflows at the ALCF, and the QMCPACK team’s journey to exascale on Aurora.",
          "url": "http://localhost:4000//community-and-outreach/user-training-activities"
        },
  
    
      "community-and-outreach-building-an-hpc-workforce": {
          "title": "Building the Computing Workforce of the Future",
          "content": "Scientific breakthroughs in HPC and AI are driven by the ingenuity and expertise of the people who design, optimize, and support these powerful technologies. At the ALCF, computational scientists, software engineers, HPC system administrators, and research support staff work to maximize the capabilities of the facility’s HPC and AI resources, ensuring they can drive scientific discoveries across a wide range of disciplines. These efforts include collaborating with ALCF users to help maximize their computing time and research outcomes; deploying and maintaining cutting-edge HPC and AI systems for peak scientific performance; optimizing codes for ALCF resources; advancing open-source software; and developing tools and services to enhance data management, workflows, and computational efficiency.\n\nAttracting and training the next generation of computing professionals is crucial to sustaining innovation in the use of HPC and AI for scientific research. The ALCF is committed to expanding the HPC and AI talent pipeline by engaging with students, educators, and researchers through a variety of outreach, training, and internship programs.\n\nStudent Outreach\n\nThe ALCF is actively working to inspire future computer scientists through its contributions to educational programs that introduce students to coding, computational thinking, and scientific research. Each December, ALCF staff volunteer in Chicagoland schools for the Hour of Code initiative, a global effort to teach students to the basics of programming and broaden participation in computer science. The facility also engages students through programs like CodeGirls@Argonne Camp, which encourages them to explore coding, and annual outreach events such as Introduce a Girl to Engineering Day and Science Careers in Search of Women, where they connect with Argonne researchers and learn about careers in STEM.\n\nFor high school students, the ALCF contributes to immersive learning experiences that introduce them to computational science and data-driven research. Argonne’s annual Coding for Science Camp and Big Data Camp provide hands-on training in programming, data science, and problem-solving techniques used in scientific research. Additionally, ALCF staff mentor students participating in the ACT-SO (Afro-Academic, Cultural, Technological &amp; Scientific Olympics) High School Research Program, guiding them through independent research projects that use Argonne’s computing resources.\n\nHands-On Training and Career Development\n\nAt the college level, the ALCF provides hands-on learning experiences through its annual summer student program. Through programs like DOE’s Science Undergraduate Laboratory Internship (SULI) and Argonne’s research aide appointments, undergraduate and graduate students have the opportunity to work alongside ALCF staff mentors on real-world research projects in areas such as HPC system administration, data analytics, computational science, and AI-driven workflows.\n\nBeyond summer internships, the ALCF extends its educational outreach with training opportunities throughout the year. One such program is the annual “Intro to AI-driven Science on Supercomputers” training series, which provides undergraduate and graduate students with an introduction to AI and supercomputing for scientific research. Launched in 2021, the virtual series has successfully reached over 700 participants from across the nation.\n\nFor those seeking more advanced training, the facility also hosts the Argonne Training Program on Extreme-Scale Computing (ATPESC), an intensive, two-week program that teaches attendees the key skills and tools needed to use the world’s powerful supercomputers. Since its launch in 2013, ATPESC has trained more than 800 participants, equipping them with knowledge in areas such as programming methodologies, numerical algorithms, HPC architectures, scientific machine learning, and data analysis. The program has been a pivotal experience for many attendees, helping them build valuable connections and advance their careers in computational science.\n\nIn 2024, the ALCF launched the Lighthouse Initiative, a program designed to establish long-term partnerships with academic institutions. This initiative aims to broaden the facility’s user base while fostering connections with the next generation of computing professionals. As part of its workforce development efforts, the ALCF offers internship opportunities to students from partner institutions, providing them with hands-on experience in scientific computing.\n\nRecruitment and Community Engagement\n\nTo connect with new talent and share career opportunities, the ALCF maintains a presence at major computing conferences and industry events. Staff members engage with students and professionals at events such as SC (Supercomputing), the Grace Hopper Celebration, and computing workshops to highlight the impact of HPC in advancing research and innovation.\n\nThe ALCF also supports efforts to build a stronger professional network for women in HPC. In collaboration with the University of Illinois Chicago, the facility helped establish Chicago Women in High Performance Computing (WHPC), a chapter dedicated to broadening participation in the field. The group provides mentorship, resources, and networking opportunities to support members pursuing careers in HPC.\n\nThrough these outreach and engagement efforts, the ALCF is working to grow the community of researchers and professionals who use and develop HPC and AI technologies, ensuring that these fields continue to evolve and drive scientific breakthroughs.",
          "url": "http://localhost:4000//community-and-outreach/building-an-hpc-workforce"
        },
  
    
      "year-in-review-year-in-review": {
          "title": "ALCF Leadership",
          "content": "Bill Allcock, ALCF Director of Operations\n\nFor the operations team, and much of ALCF, 2024 was the year of Aurora. It was the single largest effort, involving every aspect of the team, and culminated in the completion of acceptance testing in December. We plan on opening Aurora to users early next year. This concluded an effort that spanned many years and while it was a challenge, I am proud of the operations team for the part they played in bringing Aurora to fruition.\n\nAurora brought many challenges. From the operations side, one of the challenges was managing the sheer volume of log and monitoring data. To address this, we created a “data lake” that ingests data into Parquet files and then incrementally improves and filters them. It also enables SQL-like queries against the files on disk. We are currently running this in parallel with the ETL (Extract, Transform, Load) processes, but we are likely going to shift to ingesting everything into the data lake and then having our Business Intelligence system perform ETL from there. Many of the AI frameworks have native support for the Parquet files, and this should be a valuable tool for AI researchers moving forward.\n\nWhile Aurora sometimes felt like the whole of the ALCF universe, it certainly was not the only focus. Polaris, the ALCF’s primary production machine, had a very good year. Although the official numbers are not yet in, we exceeded all our performance metrics, some by a significant margin. Our Integrated Resource Infrastructure (IRI) work also continued apace. With the Advanced Photon Source coming up from their upgrade, we have begun doing production processing of beamline data and expanded the scope of this work to other facilities and science communities. As long-time users are undoubtedly aware, we typically have a weekly or bi-weekly cadence of regular preventative maintenance. To better support the IRI concept, we put significant effort into improving our systems and processes by adding redundancy, enabling maintenance while systems are live, and implementing other measures to minimize disruption. Our target was a maintenance cadence of four months, and we successfully accomplished that on Polaris. We completed a software upgrade on Aurora using an improved process, where the upgrade was treated much like a software development project. This involved using Sirius as the stage system, making all changes and conducting all testing there, before pushing them up to Aurora. It went well, but there is also room for improvement, and we will continue refining this process.\n\nWe also brought some new resources online this year. Crux, a CPU-only system originally developed as a testbed for evaluating system software for Aurora, was deployed to users late this year. It is small relative to other ALCF systems at about 1 PF, but it is a non-trivial resource and there is a small, but important, set of science applications that can take advantage of a CPU-only machine. Additionally, we deployed Sophia, a 24-node Nvidia A100 DGX system, in July. Previously part of ThetaGPU, Sophia became a standalone system after Theta’s retirement. It is targeted at smaller workloads that need near-immediate turnaround, such as Jupyter notebooks and interactive visualizations.\n\nThe ALCF AI Testbed did not add new systems in 2024, but several upgrades were implemented, and new systems are planned for next year. Resources were also made available to the National Artificial Intelligence Research Resource (NAIRR) pilot, and we supported multiple workshops and an SC24 tutorial.\n\nOn the storage front, the most significant change for users was that we enabled the ability for users to share their data on the Eagle and Grand storage systems by default. Users still must create shares, but they no longer need to explicitly request this capability be enabled. Another significant effort on the storage side, that was, by design, completely transparent to the users, was the migration of all data in the Grand filesystem path onto the same hardware as Eagle so we could repurpose the hardware to support Aurora during testing. This isolated the storage for Aurora from the rest of ALCF to avoid the ongoing changes disrupting other production resources. We also completed routine upgrades of the Lustre storage and HPSS.\n\nWe also released a significant upgrade to the ALCF account and project management website. Named myALCF, the new platform greatly expands web tools for ALCF users to manage their projects and resources. It offers a dashboard with real-time data graphs to show allocation usage, allocation on-track trends, daily/14-day/monthly job activity, and node usage through a project all at a glance. Additionally, myALCF includes a web tool to help new users learn our sbank accounting tool. The sbank tool also helps seasoned ALCF users to be aware of options in sbank that they may not have seen or used before. With myALCF, users still have all the functionality of the previous accounts and project management system (requesting allocation, managing projects and Unix groups, etc.) in a cleaner user interface. We plan to continue expanding myALCF with an improved cluster accounting (sbank) web presence, web UI for job management, and a user notification framework in the future.\n\n\n\nKalyan Kumaran, ALCF Director of Technology\n\nOver the past year, our team made significant strides in advancing the ALCF-3 project, successfully facilitating the transition of the Aurora supercomputer into production early next year. At the same time, we laid the groundwork for the ALCF-4 project, contributing extensively to the early stages of our next system acquisition.\n\nThe Aurora efforts were multifaceted, with our team collaborating closely with Intel on Non-Recurring Engineering (NRE) tasks for key software components, including those integrated within the oneAPI framework. Additionally, we focused on stabilizing and optimizing system performance by running a diverse array of workloads designed to stress-test Aurora, pinpointing performance bottlenecks and addressing system failures. In parallel, our team worked diligently to optimize and scale several science applications from the Aurora Early Science Program and the Exascale Computing Project. A major accomplishment was the preparation and execution of an extensive set of tests and applications that were essential for the functional, performance, and stability assessments during Aurora’s acceptance at the end of the year.\n\nFor the ALCF-4 project, the team played an instrument role in drafting several sections of the Request for Proposal (RFP), actively enaging in internal reviews, and visiting vendors to gain a deeper understanding of their roadmaps while effectively communicating the facility’s evolving needs.\nThroughout the year, the team also made impressive contributions in showcasing Aurora’s extraordinary scientific capabilities. Notable achievements include:\n\n\n  Performance Engineering Excellence: The performance engineering team partnered with Intel to run a comprehensive suite of benchmarks, including the High-Performance Linpack (HPL) benchmark for the TOP500 list and the mixed-precision variant (HPL MxP) to evaluate Aurora’s AI processing capabilities. The team also executed the Graph 500 benchmark, which evaluates large-scale data analytics performance, and the High-Performance Conjugate Gradient (HPCG) benchmark, which measures computational efficiency in scientific simulations. These results placed Aurora among the most powerful supercomputers in the world, establishing its place at the top of the global rankings.\n  AI/ML Innovations: The AI/ML team, in collaboration with other research groups, earned widespread recognition for their groundbreaking work in scaling and fine-tuning a large language model on Aurora. Their efforts were recognized as a finalist for Gordon Bell Prize, underscoring their significant contributions to advancing  AI and machine learning capabilities in supercomputing.\n  Visualization and Data Analytics Milestone: In partnership with Intel, the visualization and data analytics team ensured that Intel’s oneAPI Render Kit fully leveraged Aurora’s GPU ray-tracing cores. As a result, Aurora became the first supercomputer in the world to use GPU-accelerated ray tracing as its primary rendering method, pushing the boundaries of high-performance visualization and data analytics.\n  Data Services and Workflows for Advanced AI: The data services and workflows continues to work at the forefront of developing scalable inference services to meet the growing demand for large language model prompting on ALCF systems. These services support a broad spectrum of applications, from querying complex scientific phenomena to protein sequence analysis and even code generation. The team’s innovative work has resulted in the creation of user-friendly web applications and robust programmable interfaces for both individual queries and large-batch prompt processing, further expanding the accessibility to cutting-edge AI capabilities.\n  Commitment to Standards and User Empowerment: The team remains actively engaged in various standards committees, advocating for the integration of new features into programming languages and models that are driven by ALCF user needs and the technical requirements of our systems. The team’s commitment to user empowerment is reflected in their ongoing development of comprehensive technical documentation and workshop presentations designed to educate and train the user community. Moreover, the team continues to evaluate emerging systems at Argonne’s Joint Laboratory for System Evaluation and the ALCF AI Testbed, ensuring the ALCF stays ahead of the curve and remains at the forefront of technological innovation in anticipation of future acquisitions.\n\n\n\n\nJini Ramprakash, ALCF Deputy Director\n\nIn 2024, we ramped up our efforts to plan for the facility’s next-generation supercomputer via the ALCF-4 project. While our latest system, Aurora, is just about ready to make its debut, it’s critical to lay the groundwork for future supercomputers well in advance. We formally launched the ALCF-4 effort in 2023 with a target deployment in the 2028–2029 timeframe. Over the past year, we’ve reached several key milestones: releasing the draft technical requirements in June, conducting a successful technical design review in August, and engaging with potential vendors to assess their technology roadmaps and timelines. The project will gain further momentum next year with major reviews, including Critical Decision-1 (CD-1), which evaluates the project’s technical approach, design, and implementation while establishing preliminary cost and schedule estimates.\n\nOur work to plan for future computing resources also extended beyond ALCF-4. We contributed to a charge from the DOE Advanced Scientific Computing Advisory Committee (ASCAC) to develop a 10-year outlook for the DOE computing facilities. This effort culminated in a report outlining the investments and upgrades needed to ensure DOE remains at the forefront of scientific computing.\n\nWorkforce development has remained a priority as well. Our annual “Intro to AI” training series continues to provide hundreds of college students with hands-on experience in using AI and supercomputers for science. In addition to training, we offer students the opportunity to apply their skills in real research environments through various appointments and internships at the ALCF. This year, we partnered with the University of Illinois Chicago’s Sprintership program, welcoming five students for three-week internships at the ALCF in the spring—four of whom returned for summer internships to work in their areas of interest. These efforts are part of our broader vision to inspire the next generation of STEM professionals by providing opportunities to explore careers in computing.\n\n\n\nKatherine Riley, ALCF Director of Science\n\nIn 2024, our user community once again pushed the boundaries of scientific computing, achieving breakthroughs in fields ranging from biology and materials science to cosmology and fusion energy. At the same time, we continued to see more and more projects with complex workflows that integrate simulation, data science, and AI methods, reflecting the growing convergence of these approaches in scientific computing. The year was marked by several high-impact projects, including a collaboration between DOE and NASA to perform simulations that are helping prepare for future observations of the cosmos, and the development of an innovative AI-driven protein design framework that was recognized as a finalist for the Gordon Bell Prize. We also made strides in integrating our supercomputing resources with experimental facilities to speed up data-intensive discoveries, while early science teams on Aurora had some promising pre-production results that hint at the transformative advances to come.\n\nOne of the year’s most exciting moments was seeing David Baker, a longtime ALCF user from the University of Washington, receive the 2024 Nobel Prize in Chemistry for his pioneering work in computational protein design. Baker was among the first researchers to use ALCF systems nearly 20 years ago and has since led multiple INCITE projects aimed at designing novel proteins and peptides for medical and industrial applications.\n\nWhile the Nobel Prize underscores the lasting impact of our community’s efforts, computational protein design represents just one area of the dynamic research portfolio supported by the ALCF. In 2024, we provided computing resources to 31 INCITE projects, 31 ALCC projects (spanning two award cycles), and numerous Director’s Discretionary projects. We also began supporting some of the initial projects awarded through the National AI Research Resource (NAIRR) pilot, providing them with access to the ALCF AI Testbed.\n\nTo kick off the year, a multi-institutional team from DOE, NASA, and academia used our now-retired Theta system for a final run before it was decommissioned. The team generated nearly 4 million images of the cosmos to help researchers prepare for upcoming observations from NASA’s Nancy Grace Roman Space Telescope and the NSF-DOE Vera C. Rubin Observatory. By publicly releasing the simulation data, they are enabling the scientific community to refine processing pipelines, improve analysis codes, and prepare to interpret real observations as soon as they start coming in.\n\nThis year also saw major progress toward building an Integrated Research Infrastructure (IRI), a DOE initiative designed to accelerate data-intensive science by connecting supercomputers with large-scale experimental and observational facilities. One of our key partners in this effort is the Advanced Photon Source (APS), a DOE user facility at Argonne. In a demonstration of IRI capabilities, the team used a fully automated pipeline between ALCF and APS to rapidly process data from an X-ray experiment. The researchers reconstructed the experimental data on Polaris and returned results to APS within 15 minutes, highlighting an approach for experiment-time data analysis that can be applied at other facilities.\n\nWe also saw some innovative work being carried out by early science team using pre-production time on Aurora. This included the Gordon Bell Prize finalist project that used Aurora and four other powerful supercomputers to develop an AI-driven framework for protein design. In December, another team ran a pair of cosmological simulations on Aurora to explore recent findings from the Dark Energy Spectroscopic Instrument, which hint at new physics beyond the standard model of cosmology. While these early examples offer a preview of Aurora’s impact, we can’t wait to see the discoveries the system will enable when it is released to the broader scientific community next year.",
          "url": "http://localhost:4000//year-in-review/year-in-review"
        },
  
    
  
    
      "feed-xml": {
          "title": "",
          "content": "&lt;?xml version=”1.0” encoding=”utf-8”?&gt;&lt;?xml-stylesheet type=”text/xml” href=””?&gt;&lt;feed xmlns=”http://www.w3.org/2005/Atom” xml:lang=””&gt;Jekyll&lt;link href=”” rel=”self” type=”application/atom+xml” /&gt;&lt;link href=”” rel=”alternate” type=”text/html” hreflang=”” /&gt;&lt;/updated&gt;&lt;/id&gt;&lt;/title&gt;&lt;/subtitle&gt;&lt;/name&gt;&lt;/email&gt;&lt;/uri&gt;&lt;/author&gt;&lt;entryxml:lang=””&gt;&lt;link href=”” rel=”alternate” type=”text/html” title=”” /&gt;&lt;/published&gt;&lt;/updated&gt;&lt;/id&gt;&lt;content type=”html” xml:base=””&gt;&lt;![CDATA[]]&gt;&lt;/content&gt;&lt;/name&gt;&lt;/email&gt;&lt;/uri&gt;&lt;/author&gt;&lt;category term=”” /&gt;&lt;category term=”” /&gt;&lt;category term=”” /&gt;&lt;summary type=\"html\"&gt;&lt;![CDATA[]]&gt;&lt;/summary&gt;&lt;media:thumbnail xmlns:media=”http://search.yahoo.com/mrss/” url=”” /&gt;&lt;media:content medium=”image” url=”” xmlns:media=”http://search.yahoo.com/mrss/” /&gt;&lt;/entry&gt;&lt;/feed&gt;",
          "url": "http://localhost:4000//feed.xml"
        }
  
  };

  window.store = {...highlights_json, ...performance_json, ...pages_json}
</script>
<script>
  const queryString = window.location.search;
  const urlParams = new URLSearchParams(queryString);
  const searchTerm = urlParams.get('query');
  document.getElementById('search-title').innerHTML = 'Results for: ' + searchTerm;
</script>
<script src="https://unpkg.com/lunr/lunr.js"></script>
<script src="http://localhost:4000/assets/js/search.js"></script>



  <button class="to-top js-to-top">
    Back to Top &uarr;
  </button>
</main>



    </div><footer>

	<div class="content-wrapper">


		<div class="alcf-logo">
			<img src="http://localhost:4000/assets/images/logo-h.png" />
		</div>

		<div class="alcf-info">
			<h2>CONTACT</h2>
			<p>
				<a href="mailto:media@alcf.anl.gov">media@alcf.anl.gov</a><br>
				<a href="https://alcf.anl.gov/">alcf.anl.gov</a>
			</p>
		</div>

		<div class="smallprint">
			<ul>
				<li><a href="http://localhost:4000/about">About</a></li>
				<li><a href="http://localhost:4000/disclaimer">Disclaimer</a></li>
				<li><a href="http://localhost:4000/credits">Credits</a></li>
			</ul>
		</div>




		<div class="bottom">
			<hr>

			<!-- <div class="llc">
				<img src="http://localhost:4000/assets/images/llc-logo.png" />
			</div> -->

			<div class="doe-info">
				<img class='llc' src="http://localhost:4000/assets/images/llc-logo.png" />
				<img class='doe' src="http://localhost:4000/assets/images/doe.png" />
				<p>Argonne National Laboratory is a U.S. Department of Energy laboratory<br>managed by UChicago Argonne, LLC.</p>
			</div>

		</div>
	
  </div>

  <script src="http://localhost:4000/assets/js/plugin--totop.js"></script>
  <script src="http://localhost:4000/assets/js/plugin--dropdown.js"></script>
  <script src="http://localhost:4000/assets/js/app.js"></script>

</footer>
</body>

</html>
