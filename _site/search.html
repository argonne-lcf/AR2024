<!DOCTYPE html>
<html lang='en'><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search Results | 2024 ALCF Annual Report</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Search Results" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="2024 ALCF Annual Report" />
<meta property="og:description" content="2024 ALCF Annual Report" />
<link rel="canonical" href="http://localhost:4000/search.html" />
<meta property="og:url" content="http://localhost:4000/search.html" />
<meta property="og:site_name" content="2024 ALCF Annual Report" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search Results" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"2024 ALCF Annual Report","headline":"Search Results","url":"http://localhost:4000/search.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://use.typekit.net/vpa7ous.css">
  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="shortcut icon" href="http://localhost:4000/favicon.ico"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="2024 ALCF Annual Report" /><meta property="og:title" content="ALCF 2023 Annual Report" />
  <meta property="og:image" content="http://localhost:4000/assets/images/og-ar2023.jpg" />
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-V7JP2R5JPH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-V7JP2R5JPH');
</script>

    <body class="search-results">
  <header class="header" role="banner">

  <div class="nav-wrapper">
    <div class="nav-desktop">

      <div class="masthead">
        <h1><a href="http://localhost:4000/index.html">ALCF Annual Report</a></h1>
      </div>


      <div class="nav">
        
          <h2>
            
              <a href="http://localhost:4000/year-in-review/directors-letter">Year in Review</a>
            
            <!-- <a href="http://localhost:4000/year-in-review/directors-letter">Year in Review</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/features">Features</a>
            
            <!-- <a href="http://localhost:4000/features">Features</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/community-and-outreach">Growing the HPC Community</a>
            
            <!-- <a href="http://localhost:4000/community-and-outreach">Growing the HPC Community</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/expertise-and-resources">Expertise and Resources</a>
            
            <!-- <a href="http://localhost:4000/expertise-and-resources">Expertise and Resources</a> -->
          </h2>
        
          <h2>
            
              <a href="http://localhost:4000/science">Science</a>
            
            <!-- <a href="http://localhost:4000/science">Science</a> -->
          </h2>
        
        <div class='search-trig' id='js-search-trig'>
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
        </div>

        <form class='searchbar' id='js-searchbar' action="http://localhost:4000/search.html" method="get">
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
          <input 
            aria-label="Search the annual report"
            autocomplete="off"
            inputmode="search"
            placeholder="Search"
            type="search"
            id="search-box" 
            name="query"
          >
          <div class='x-box' id='js-searchbar-close'>
            <svg class='x' fill="currentColor" viewBox="0 0 512 512">
              <g>
                <polygon points="512,59.076 452.922,0 256,196.922 59.076,0 0,59.076 196.922,256 0,452.922 59.076,512 256,315.076 452.922,512 512,452.922 315.076,256"/>
              </g>
            </svg>
          </div>
        </form>

      </div>

    </div>
  </div>


  <div class="nav-wrapper">
    <div class="nav-mobile">

      <div class="masthead">
        <h1><a href="http://localhost:4000/index.html">ALCF Annual Report</a></h1>
      </div>


      <div class="nav">
        <div class="js-drop menuitem-wrapper" id="toggle-menu" data-menu="menu">
          <h1>Menu <!--&nbsp;&blacktriangledown;--></h1> 
          <div id="menu" class="dropdown js-dropdown-hidden">

            <div class="nav-wrapper">
               

                <div class="chunk">
                  <h2>
                    <a href="/year-in-review/directors-letter" class="menuitem">
                      Year in Review
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/year-in-review/directors-letter">
                            Director&rsquo;s Letter
                          </a>
                        </li>
                      
                        <li>
                          <a href="/year-in-review/year-in-review">
                            ALCF Leadership
                          </a>
                        </li>
                      
                        <li>
                          <a href="/year-in-review/about-alcf">
                            About ALCF
                          </a>
                        </li>
                      
                        <li>
                          <a href="/year-in-review/about-alcf#2024-by-the-numbers">
                            2024 by the Numbers
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/features" class="menuitem">
                      Features
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/features/aurora">
                            Aurora Nears Full Deployment
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/aurora-performance-highlights">
                            Aurora Performance Highlights
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/nexus-iri">
                            Linking Experimental Facilities and Leadership Computing
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/alcf-ai-testbed">
                            ALCF Continues to Expand AI Testbed Systems Deployed for Open Science
                          </a>
                        </li>
                      
                        <li>
                          <a href="/features/ai-training">
                            Preparing a new generation of AI-ready researchers
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/community-and-outreach" class="menuitem">
                      Growing the HPC Community
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/community-and-outreach/industry">
                            Partnering with Industry
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/building-an-hpc-workforce">
                            Building an HPC Workforce
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/hpc-community-activities">
                            Shaping the Future of Supercomputing
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/user-training-activities">
                            Training ALCF Users
                          </a>
                        </li>
                      
                        <li>
                          <a href="/community-and-outreach/educational-outreach-activities">
                            Inspiring Students
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/expertise-and-resources" class="menuitem">
                      Expertise and Resources
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/expertise-and-resources/systems">
                            ALCF Systems
                          </a>
                        </li>
                      
                        <li>
                          <a href="/expertise-and-resources/team">
                            ALCF Team
                          </a>
                        </li>
                      
                        <li>
                          <a href="/expertise-and-resources/staff-spotlights">
                            Staff Spotlights
                          </a>
                        </li>
                      
                        <li>
                          <a href="/expertise-and-resources/staff-news">
                            Staff News
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
               

                <div class="chunk">
                  <h2>
                    <a href="/science" class="menuitem">
                      Science
                    </a>
                  </h2>

                  
                    <ul>
                      
                        <li>
                          <a href="/science/allocation-programs">
                            Allocation Programs
                          </a>
                        </li>
                      
                        <li>
                          <a href="/science/highlights">
                            2024 Science Highlights
                          </a>
                        </li>
                      
                        <li>
                          <a href="/science/projects">
                            2024 Award List
                          </a>
                        </li>
                      
                        <li>
                          <a href="/science/publications">
                            Publications
                          </a>
                        </li>
                      
                    </ul>
                   
                </div>
                
            </div>   
          </div>
        </div>
        <div class='search-trig' id='js-search-trig-mobile'>
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
        </div>
        <form class='searchbar' id='js-searchbar-mobile' action="http://localhost:4000/search.html" method="get">
          <svg class='mag' aria-hidden="true" viewBox="0 0 24 24">
            <path
              fill="currentColor"
              d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"
            />
          </svg>
          <input 
            aria-label="Search the annual report"
            autocomplete="off"
            inputmode="search"
            placeholder="Search"
            type="search"
            id="search-box" 
            name="query"
          >
          <div class='x-box' id='js-searchbar-close-mobile'>
            <svg class='x' fill="currentColor" viewBox="0 0 512 512">
              <g>
                <polygon points="512,59.076 452.922,0 256,196.922 59.076,0 0,59.076 196.922,256 0,452.922 59.076,512 256,315.076 452.922,512 512,452.922 315.076,256"/>
              </g>
            </svg>
          </div>
        </form>
      </div>

    </div>
  </div>

</header>




<nav class="sidenav">

  

    
    
    
    
  

    
    
    
    
  

    
    
    
    
  

    
    
    
    
  

    
    
    
    
  


</nav>

<div class="page-intro-text">
    	<!-- 

TEXT, BREADCRUMBS

-------------------------------------------------------------------------------

available parameters:

* required

-------------------------------------------------------------------------------

-->

<div id="breadcrumbs">
 
<a href="http://localhost:4000/index">Home</a>

  
    / Search Results
  

</div>
    	<h1 class="page-title">Search Results</h1>

    	
    </div>

    <div class="page-intro-media">
      
    </div>
    
    

    <div class="height-wrapper">
    	<main class="page-content layout-wrapper page" aria-label="Content">

<!-- Based on tutorial: https://learn.cloudcannon.com/jekyll/jekyll-search-using-lunr-js/ -->
<!-- ------------------------------------------------------------------------------------ -->



<h2 id='search-title'></h2>
<ul id="search-results"></ul>



<script>
  
  
  

  const highlights_json = {
    
      "science-highlights-ramanathan": {
        "title": "MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization",
        "content": "The ability to design and discover new proteins drives advances in medicine, catalysis, and numerous other applications. Leveraging AI and some of the world’s most powerful supercomputers, a multi-institutional team led by researchers at Argonne National Laboratory has developed a multimodal framework to accelerate the design of novel proteins.\n\nChallenge\n\nMapping a protein’s amino acid sequence to its structure and function is a long-standing research challenge. Each unique arrangement of amino acids—the building blocks of proteins—can yield different properties and behaviors. The sheer volume of potential variations makes it impractical to test them all through experiments alone. AI models offer a way to analyze and predict promising protein structures, but they must integrate diverse data sources to generate reliable designs. By combining AI with high-performance computing, researchers can overcome these challenges and accelerate the discovery process.\n\nApproach\n\nThe team developed MProt-DPO, a scalable, end-to-end workflow for protein design that integrates protein sequences, structural and functional information, and natural language descriptions of biochemical properties while incorporating feedback from experiments and molecular simulations. A key component of their framework is Direct Preference Optimization (DPO), which refines AI-generated protein designs based on fitness landscapes to improve their effectiveness. By leveraging multimodal data, the workflow enables AI to learn directly from experimental and computational feedback, enhancing the reliability of designed proteins. The framework was developed and deployed on five leading HPC systems: ALCF’s Aurora, OLCF’s Frontier, the Swiss National Supercomputing Centre’s ALPS, Cineca’s Leonardo, and NVIDIA’s PDX machine. The team achieved over one exaflop of sustained performance (mixed precision) on each machine, with a peak performance of 5.57 exaflops on Aurora.\n\nResults\n\nThe team tested MProt-DPO on two tasks to demonstrate its ability to handle complex protein design challenges: enhancing the yeast protein HIS7 based on deep mutational scanning data and improving the catalytic efficiency of malate dehydrogenase using molecular simulations. The framework demonstrated the ability to integrate diverse data sources, refine AI-generated sequences, and produce biologically plausible protein variants with high predicted fitness scores. The team is collaborating with Argonne biologists to validate the AI-generated designs in a laboratory, where initial tests have shown they are performing as expected. Their innovative approach was named a finalist for the 2024 Gordon Bell Prize, highlighting its impact in advancing AI-driven science.\n\nImpact\n\nThe team’s development of MProt-DPO marks a major advance in AI-driven protein design, with the potential to greatly accelerate the discovery of new proteins for applications ranging from biodegradation and rare earth extraction to biofuels and biomedicine. By integrating experimental validation into the AI training process, MProt-DPO improves the reliability of generative protein design models. The framework establishes a new benchmark for multimodal AI-driven protein design and contributes to broader AI for science initiatives, including Argonne’s development of AuroraGPT, a foundation model designed to aid in autonomous scientific exploration across disciplines.",
        "url": "http://localhost:4000//science/highlights/ramanathan"
      }
      ,
    
      "science-highlights-jiang": {
        "title": "Microscopic Insight into Transport Properties of Li-Battery Electrolytes",
        "content": "To advance high-energy-density lithium battery chemistries, researchers are working to develop protective interphases that enhance stability and performance. With this project, a team from Argonne National Laboratory is using ALCF supercomputers to perform large-scale molecular dynamics (MD) simulations to uncover the mechanisms behind electrolyte fluorination, a key process for creating such interphases. By identifying how fluorination affects interphase formation and lithium-ion transport, the team’s research provides key insights for designing next-generation electrolytes.\n\nChallenge\n\nCreating stable, high-performance lithium batteries requires a robust interphase formed by electrolyte decomposition, but identifying the optimal electrolyte formulations is costly and time-consuming. Electrolyte fluorination, the process of introducing fluorine atoms to the electrolyte molecules, has shown to be a promising solution for creating durable protective layers and improving battery stability. However, developing an effective fluorination strategy without relying on extensive lab work remains a significant obstacle. HPC enables researchers to simulate these complex electrochemical interactions at the atomic scale, providing detailed insights into the effects of fluorination on electrolyte behavior.\n\nApproach\n\nWith an ALCC award, the Argonne team employed the NAMD code on the ALCF’s Theta and Polaris supercomputers to conduct extensive MD simulations of electrolyte fluorination effects on protective interphase formation and charge conduction under realistic experimental conditions. The researchers used novel all-atom models for high-capacity Li-metal anodes and Mn-Ni-O cathodes to study atomic-scale interface structural changes induced by fluorination. Enhanced sampling techniques were applied to efficiently explore the dynamics of viscous electrolytes, requiring large timescales beyond brute-force simulations. Multiple replicas of each simulated system were run in parallel, allowing for comprehensive configurational sampling to accurately capture the effects of fluorination on electrolyte-electrode interactions.\n\nResults\n\nThe team’s simulations provide new insights into the fundamental mechanism of fluorination and its effects on battery stability, lifetime, and capacity. Their research identified how fluorination facilitates the formation of a robust protective interphase, which enhances electrolyte stability and improves charge conduction. The work also demonstrated that the electrolytes exhibit distinct behaviors based on their molecular structure—fluorinated diluents with cyclic structures improved lithium-ion mobility, while those with linear structures restricted charge transport. Statistical analysis revealed that fluorination must be applied to specific positions on electrolyte molecules or particular molecular structures of diluents to achieve optimal performance, paving the way for the rational design of future fluorinated electrolytes.\n\nImpact\n\nThis research accelerates electrolyte discovery by reducing reliance on extensive wet-lab experiments, improving safety, and enhancing the rational design of fluorinated electrolytes. The insights gained from the team’s simulations open new pathways for optimizing lithium battery performance, supporting the development of more efficient and longer-lasting energy storage solutions.",
        "url": "http://localhost:4000//science/highlights/jiang"
      }
      ,
    
      "science-highlights-prince": {
        "title": "Demonstrating Cross-Facility Data Processing at Scale with Laue Microdiffraction",
        "content": "The ongoing Advanced Photon Source (APS) upgrade project will offer users x-ray beams 500 times more powerful than previously available, with commensurate increases in the amounts of experimental data generated for processing. To meet the demands of the upgrade, a team of researchers led by Argonne National Laboratory developed high-performance software tools and data infrastructure to integrate APS data processing and analysis in near-real time with ALCF computing resources.\n\nChallenge\n\nThe co-location of the ALCF and the APS offers an ideal proving ground for methods to closely integrate supercomputers and experiments for near-real-time data analysis. The research team deployed a fully automated pipeline that uses ALCF resources to rapidly process data obtained from x-ray experiments at the APS. To demonstrate the capabilities of the pipeline, the team carried out a study using a technique called Laue microdiffraction, which is employed at the APS and other light sources to analyze materials with crystalline structures but requires significant computational resources.\n\nApproach\n\nAs a focused x-ray beam passes through a material, individual crystallites along the path of the beam diffract at different angles depending on their orientation. To obtain a full 3D map of the structure, the angle and position of each diffracted beam must be resolved. A new coded aperture Laue reconstruction algorithm is used instead of using a time-intensive scan to complete the analysis, necessitating supercomputers.\nThe automatic pipeline the team built for processing APS data leveraged infrastructure and tools being deployed between APS and ALCF as part of the Argonne Nexus effort. Globus handles much of the cross-facility data management. The APS Data Management System integrates with Globus Gladier/FuncX workflow tools to provide a single end-to-end data pipeline.\n\nResults\n\nWith ALCF resources, the team demonstrated the on-demand reconstruction of data obtained from the APS beamline, returning reconstructed scans to the APS within 15 minutes of them being sent to the ALCF. Near-ideal scaling for the workflow has been shown on as many as 100 nodes. Continuously using up to 50 nodes on Polaris, it was able to keep up with the data generation rate processing scans, which came in every 1 to 2 minutes throughout 6-to-12 hour runs. The work resulted in a Best Paper Award at the SC23 conference’s XLOOP workshop.\n\nImpact\n\nThe team’s results carry implications for future software development, engineering and beamline science. The parallelization, optimization, and deployment onto Polaris of the Laue microdiffraction technique has enabled full-scale analysis of microdiffraction data. The automated near-real-time reconstruction of coded aperture Laue datasets will enable users of ALCF supercomputing resources to collect data at speeds ten times faster than is currently possible, thereby accelerating the pace of scientific discovery. Furthermore, the full-scale reconstructions produced with Polaris are being used to improve the underlying beamline technique.",
        "url": "http://localhost:4000//science/highlights/prince"
      }
      ,
    
      "science-highlights-tramm": {
        "title": "Efficient Algorithms for Monte Carlo Particle Transport on AI Accelerator Hardware",
        "content": "In this study, researchers led by Argonne National Laboratory examined the feasibility of performing continuous energy Monte Carlo (MC) particle transport on the Cerebras Wafter-Scale Engine 2 (WSE-2). The researchers ported a key kernel from the MC transport algorithm to the Cerebras Software Language (CSL) programming model and evaluated the performance of the kernel on the Cerebras WSE-2.\n\nChallenge\n\nBeyond the challenge of porting the kernel into the low-level CSL programming model, the team proposed and tested various new algorithms to handle the decomposition of neutron cross-sectional data (which is used to generate random samples for particle behavior) into the small local memory domains contained in each of some 750,000 WSE-2 units.\n\nApproach\n\nThe researchers ported a simplified version of the MC cross-section lookup kernel (a kernel used by the MC neutral particle transport algorithm) using the Cerebras SDK and the Cerebras CSL programming model. Their decomposition and communication scheme involved three stages: (1) the sorting of particles into energy bands within each column of compute cores; (2) an iterative diffusion-based load balancing stage for balancing starting particle loads within each row; and (3) an exchange of particles to allow particles to accumulate nuclide information from each column in the row. All communication patterns had to be developed to avoid any concept of global synchronization or point-to-point message passing, given the limitations of the WSE-2 hardware. Additionally, the team developed an architecture-specific optimization to leverage the capabilities of the WSE-2 and a highly optimized CUDA kernel for testing on an NVIDIA A100 graphics processing unit to provide a baseline to contextualize the performance of the WSE-2.\n\nResults\n\nA single WSE-2 was found to run 130 times faster than the highly optimized CUDA version of the kernel deployed on a single NVIDIA A100—significantly outpacing expected performance increase, given the difference in transistor counts between the architectures. However, the performance gains came at a cost—namely, increases in both software programming and algorithmic complexity. Considering how AI accelerators such as the WSE-2 were designed almost exclusively around deep learning AI tasks, it is noteworthy that the WSE-2 is already able to exceed performance expectations relative to GPUs—an architecture that has had several decades to mature and is now quite friendly to HPC simulation applications. A follow-up study saw the WSE-2 achieve a 182x speedup over the A100.\n\nImpact\n\nThe team’s analysis suggests the potential for a wide variety of complex and irregular simulation methods to be mapped efficiently onto AI accelerators like the Cerebras WSE-2. Such accelerators could offer significant advantages to traditional simulation workloads, and the development of higher-level programming models to more readily enable software development and exploration could greatly benefit HPC simulations. MC simulations themselves offer the potential to fill in crucial gaps in experimental and operational nuclear reactor data.",
        "url": "http://localhost:4000//science/highlights/tramm"
      }
      ,
    
      "science-highlights-grover": {
        "title": "From Quantum Mechanics to Hypersonic Aerothermodynamics",
        "content": "The development of hypersonic flight—the ability to fly at five times the speed of sound—and related capabilities is now of interest across the globe, with the potential to revolutionize technologies for national security, aviation, and space exploration. At the core of this research is the pursuit of accuracy in the prediction of the environment surrounding the hypervelocity vehicle.  This work, led by a research team from the University of Dayton Research Institute and Air Force Research Laboratory, replicated a hypervelocity ground-test experiment using the novel direct molecular simulation (DMS) method.\n\nChallenge\n\nAt high Mach numbers, the shock encapsulating a vehicle is strong enough to cause chemical reactions and excitation of internal energy modes in the shock-heated gas. A precise description of these processes is essential to many aspects of flight, especially the heat loads experienced by the vehicle. DMS methods were used to model a reactive Mach 8.2 oxygen flow over a double-cone geometry. The method relies on molecular dynamics guided by interaction potentials grounded in quantum mechanics to construct this complex flow-field. The free-stream conditions and the article configuration generate a flow with thermal, chemical, and mechanical nonequilibrium.\n\nApproach\n\nThe researchers used a custom, massively parallel version of the SPARTA DSMC code to carry out the DMS simulation on ALCF computing resources. Due to the robust grid adaption and efficient parallelization of the code, the team was able to carry out a DMS of a continuum-scale flow.\n\nResults\n\nAs detailed in Science Advances, the team used ALCF computing resources to simulate, for the first time, using a method grounded in quantum mechanics, a full-scale reactive hypersonic flow studied experimentally. Due to the fundamental nature of the simulation technique, this work provides a molecular-level description of internal energy excitation and reaction mechanisms throughout the system. Building on direct simulation Monte Carlo (DSMC) methods, DMS replaces stochastic collision models for particles in the flow with molecular dynamics calculations using interaction potentials derived from quantum mechanics. These ab-initio interaction potentials are the only modeling input in the flow; therefore, all flow features can be attributed the quantum-mechanically guided molecular interactions.\n\nImpact\n\nThis was the first-ever quantum mechanically guided simulation of a hypersonic ground test. The team’s research is helping to advance our understanding of the complex aerothermodynamics of hypersonic flight, providing insights that could help inform the design of safer and more efficient technologies for space travel and defense. The simulation performed is of sufficiently great fidelity and detail that it can be used as a benchmark solution to verify computational fluid mechanics (CFD) codes and assess fundamental gaps and opportunities of improvement for the physics simulated by lower fidelity models. This, in turn, enables robust and accurate CFD solutions to mission-critical scenarios where the flow field may be in thermal and chemical nonequilibrium.",
        "url": "http://localhost:4000//science/highlights/grover"
      }
      ,
    
      "science-highlights-pal": {
        "title": "Robust Gas Turbine Film Cooling Under Manufacturing Uncertainty for Improved Jet Engine Lifecycle Energy Efficiency",
        "content": "Gas turbines in aircraft operate at extreme temperatures, making efficient cooling strategies essential for preventing thermal damage and extending component lifetimes. As next-generation aircraft engines become more compact and run at higher pressures, managing increased heat loads becomes even more critical. To address this challenge, researchers from RTX Technology Research Center and Argonne National Laboratory are using DOE supercomputers to better understand cooling flow physics, providing insights that can help inform more effective turbine cooling designs.\n\nChallenge\n\nFilm cooling is a widely used technology for protecting turbine components from intense heat by injecting cooling air through small holes to create a protective layer. Optimizing these designs is challenging due to the complex interactions between flow dynamics, heat transfer, and variations in cooling hole geometry. Surface roughness caused by manufacturing processes or material deposition can degrade cooling performance, often leading to conservative design choices that prioritize durability at the cost of efficiency. To improve turbine cooling, researchers need predictive models that accurately capture near-wall cooling flow physics, heat transfer, and the impact of manufacturing deviations like in-hole surface roughness. While computational fluid dynamics (CFD) simulations are essential for modeling these effects, the computational resources needed to generate fine-grained models of the flow near the turbine walls are often beyond the capabilities of most manufacturers. By leveraging HPC and AI, researchers can enhance modeling accuracy to inform the development of more efficient cooling strategies.\n\nApproach\n\nWith support from DOE’s High-Performance Computing for Energy Innovation (HPC4EI) program, the RTX-Argonne team continues its work to use ALCF and OLCF supercomputers to optimize turbine cooling designs. In the first phase of the project, researchers used NekRS, a high-fidelity CFD solver optimized for DOE supercomputers, to analyze how surface roughness inside cooling holes affects coolant flow and heat transfer. Their simulations achieved resolutions unattainable with conventional computing, enabling a more precise assessment of roughness effects. Now in phase two of the project, the team is integrating high-fidelity CFD simulations with machine learning to develop data-driven surrogate models for predicting the impact of in-hole surface roughness on film cooling effectiveness. This approach aims to develop reduced-order models to quantify the impact of manufacturing variability introduced by surface roughness on the performance of film cooling schemes, with implications for energy efficiency and durability of gas turbine engine components.\n\nResults\n\nIn a paper presented at the 2024 AIAA Aviation Forum and ASCEND conference, the team detailed how increased in-hole roughness reduces film cooling effectiveness, with roughness element height having the most significant impact. Comparisons with experimental data revealed that while simulations captured overall trends, discrepancies remained, particularly in cases with higher roughness. Moreover, it was observed that increasing the length of the roughness unit, while keeping the other parameters constant, significantly increases the thermal cooling effectiveness.\n\nImpact\n\nThe team’s research is advancing our understanding of film cooling and its impact on gas turbine performance. By refining predictive high-fidelity CFD models and developing reduced-order surrogate models , their work will enhance turbine cooling designs to maximize efficiency, reduce thermal stress, and extend the lifespan of aircraft engine components. The ongoing effort to integrate machine learning methods with high-fidelity simulations and supercomputing will further pave the way for shrinking design cycles, making advanced cooling designs more efficient and accessible to manufacturers.",
        "url": "http://localhost:4000//science/highlights/pal"
      }
      ,
    
      "science-highlights-larsson": {
        "title": "Three-Dimensional Shock Boundary Layer Interactions over Flexible Walls",
        "content": "To design safe and efficient hypersonic aircraft, engineers must understand how shockwaves and turbulence affect the aircraft’s performance and structural integrity. Recently, a team from the University of Southern California and the University of Maryland used ALCF supercomputing resources to develop predictive 3D simulations of shock wave and turbulent interactions over flexible walls.\n\nChallenge\n\nHigh-speed airflows create a thin boundary layer along solid surfaces. When a shock hits the boundary layer forcefully enough, it may create high-amplitude, low-frequency oscillations that can damage the aircraft. To address this, we need to understand the mechanics of shockwave and turbulent boundary-layer interactions (STBLI) and the fluid-structure interactions (FSI). Researchers have not extensively studied the fluid-structural coupling of STBLIs. While teams have led efforts to investigate the relationships of STBLI with flexible panels, it is extremely challenging to characterize this class of interactions experimentally. Numerical simulations are crucial to provide these fundamental insights. Several simulation approaches have been tested, but many suffer from high associated computational costs. Approaches that have lower computational costs suffer from lower accuracy.\n\nApproach\n\nThe team used the ALCF’s Theta supercomputer to perform fully coupled fluid-structural 3D simulations of STBLI over flexible walls to replicate and complement wind-tunnel experiments. They studied the interactions’ characteristic low-frequency motions on flexible panels using wall-modeled large-eddy simulations (WMLES). This method models rather than resolves the inner boundary layer, reducing the computational cost of the simulation and maintaining the physical fidelity of flow features like separation and reattachment. This approach allows for sufficiently long integration times needed to capture the low-frequency motions of interest. The team coupled the WMLES with a finite-element solid mechanics (FEM) solver to incorporate structural damping. This is the first time researchers have combined these approaches to study such interactions on flexible panels.\n\nResults\n\nTo validate the high-fidelity simulation methodology, the team used WMLES to replicate experiments at different strengths of the incident shock on the turbulent boundary layer. Based on these results, the team assessed the importance of the 3D effects in those interactions by conducting reduced-span simulations with imposed periodicity in the spanwise direction. The simulations replicated the coupled interactions observed experimentally with better accuracy than prior numerical studies, while also providing additional insights into the wind-tunnel experiments.\n\nImpact\n\nThe ultimate goal of the team’s research is to develop improved modeling techniques for the prediction of fluid-thermal-structural interactions through coupled specialized domain-specific solvers. These techniques will reduce the uncertainty factored into designing hypersonic vehicles and propulsion systems, leading to safer and more precise aircraft designs. This work, directly validated by x-ray free electron laser, ultrafast electron diffraction, and neutron experiments at DOE facilities, will enable future production of high-quality custom quantum material architectures for broad and critical applications for continued U.S. leadership in technology development, including that for sustainable ammonia, thereby addressing DOE basic research needs for transformative manufacturing and quantum materials. The Allegro–Legato model exhibits excellent computational scalability and GPU acceleration in carrying out NNQMD simulations, with strong promise for emerging exascale systems.",
        "url": "http://localhost:4000//science/highlights/larsson"
      }
      ,
    
      "science-highlights-chang": {
        "title": "Exascale Gyrokinetic Study of ITER Challenge on Power-Exhaust and ELM-Free Edge",
        "content": "This project aims to significantly advance our understanding of fundamental edge plasma physics in fusion reactors, answering questions critical to the successful operation of ITER and to the design of fusion power plants (FPPs).\n\nChallenge\n\nThe goal of this project is to perform two-pronged, inter-related fundamental edge physics studies of critical importance to the successful operation of ITER and to the design of FPPs. The first prong is the mitigation of high stationary heat-flux densities that will damage material walls while maintaining the high edge plasma pedestal within a safe operational window. The second prong is avoiding explosive transient power flow to material walls caused by edge localized mode crash.\n\nApproach\n\nAchieving their goals necessitates that the researchers employ DOE supercomputing resources. The team uses the electromagnetic edge gyrokinetic particle-in-cell code XGC, which enables the inclusion of two important but computationally expensive components: (a) the addition of tungsten impurity particles that are sputtered from ITER’s material wall as a third species along with deuterium and tritium fuel particles, and (b) the capability for plasma detachment from the divertor plates. Tungsten impurity particles, beyond their deleterious effect of radiating away plasma energy in the core, are known to significantly impact the edge physics, and the detached plasma is known to significantly reduce the divertor heat load.\n\nResults\n\nA paper published in Physics of Plasmas demonstrated the application of a bundling technique to model the diverse charge states of tungsten impurity species in total-f gyrokinetic simulations. XGC was used to simulate a JET H-mode-like plasma across an entire plasma volume, spanning from the magnetic axis to the divertor. Tungsten impurities were found to affect the deuterium fluxes of particles and heat.\n\nImpact\n\nAs fusion power represents a paradigm-shifting breakthrough, success of the ITER project is a high-priority challenge for the DOE mission, which will ultimately lead to more economical FPPs. To accomplish this goal, simulations based on first principles must be deployed to solve issues with power exhaust, including mitigating stationary heat-flux densities and avoiding unacceptably high transient power flow to material walls.",
        "url": "http://localhost:4000//science/highlights/chang"
      }
      ,
    
      "science-highlights-heitmann": {
        "title": "Simulating the Cosmos for the Roman and Rubin Telescopes",
        "content": "The Vera C. Rubin Observatory and Nancy Grace Roman Space Telescope are state-of-the-art telescopes set to revolutionize our understanding of the universe when they begin operations in 2025 and 2027, respectively. To prepare for the vast amounts of observational data they will generate, researchers from DOE, NASA, and U.S. universities joined forces to use the ALCF’s Theta supercomputer to produce nearly 4 million simulated images that depict the cosmos as the telescopes will see it.\n\nChallenge\n\nNASA’s space-based Roman telescope and the ground-based Rubin telescope, jointly funded by the National Science Foundation (NSF) and DOE, will provide unprecedented views of the cosmos to help advance research on dark energy, dark matter, and the evolution of the universe. Because the two surveys will produce highly complementary datasets, joint data processing and analysis has been identified as a critical step for achieving the most robust and powerful cosmological and astrophysical results. This approach, however, requires powerful computing resources and advanced simulation techniques to develop methods that take full advantage of the overlapping survey data.\n\nApproach\n\nResearchers from DOE (Argonne, SLAC), NASA, and academia partnered to use the ALCF’s Theta supercomputer to produce a set of overlapping joint synthetic Roman-Rubin time-domain surveys. Carried out as part of the broader NASA-led project, OpenUniverse, the team’s simulations account for the telescopes’ unique instrument performances, making them the most accurate predictions to date of what the telescopes will observe.\n\nResults\n\nThe team leveraged Theta’s processing power to generate approximately 4 million simulated images for the Rubin and Roman telescopes in about nine days—a task that would have taken around 300 years on a personal computer. The simulations cover the same patch of the sky, spanning 70 square degrees (roughly equivalent to the sky area covered by 350 full Moons). The team has released an initial 10-terabyte subset of the simulation data for the community to explore, with the remaining 390 terabytes to follow once all the data has been processed. The simulated images give researchers the opportunity to exercise their data processing pipelines, better understand their analysis codes, and accurately interpret the results, ensuring they are prepared to work with real observational data as soon as it becomes available.\n\nImpact\n\nThe team’s simulations will help inform future research with the Roman and Rubin telescopes to advance our understanding of dark energy, dark matter, and the evolution of the universe. By overlapping the simulation data for each survey, scientists can learn how to use the best aspects of each telescope—Rubin’s broader view and Roman’s sharper, deeper vision. The combination will provide more precise and accurate insights than could be achieved from either observatory alone.",
        "url": "http://localhost:4000//science/highlights/heitmann"
      }
      ,
    
      "science-highlights-burrows": {
        "title": "State-of-the-Art High-Resolution 3D Simulations of Core-Collapse Supernovae",
        "content": "Supernovae shape the universe and life as we know it, but the physical mechanisms that cause a star to explode remain a mystery. Using ALCF supercomputing resources, a team from Princeton University has been generating one of the largest collections of 3D supernova simulations to shed light on the physics behind these cosmic events.\n\nChallenge\n\nModeling the physics of supernovae has posed a persistent challenge to astrophysicists for decades. Models must not only be shown to explode, but must reach the asymptotic state of the blast to determine many of the observables. Many relevant simulations have been developed. However, the necessary sophisticated and expensive 3D simulations typically have not run long enough after bounce to successfully capture asymptotic kick speeds. Even the few existing longer-term studies do not explore the systematics with the broad range of progenitor masses to determine the relationships between the kick speed and progenitor mass or the initial core structure.\n\nApproach\n\nThe team carried out 20 state-of-the-art 3D long-term core-collapse simulations generated using the code FORNAX. They performed these simulations on multiple supercomputers, including the ALCF’s Theta and Polaris systems. The researchers focused their study on the kicks during the simultaneous accretion and explosion phase, with attention towards the crucial first few seconds post-bounce. They then complemented their kick study with a study of the associated induced spins. The ALCF Catalyst team provided support to transition the code to Polaris (NVIDIA GPUs) and has worked with the researchers to port the FORNAX GPU version to Aurora to enable exascale simulations.\n\nResults\n\nFor the first time, using a large and uniform collection of 3D supernova models ranging from 9- to 60-solar-mass stars, the researchers asymptoted the kicks or came within 20 percent of doing so. They obtained an integrated and wide-angle perspective of the overall dependence of the recoil kicks and induced spins upon progenitor mass and their Chandrasekhar-like core structures, the latter indexed approximately by compactness. The team found that the mass and compactness of the progenitors directly correlated to the size of the neutron star’s kicks. These two classes can be correlated to the gravitational mass of the residual neutron star, which suggests the survival of binary neutron star systems may be due to their lower observed kick speeds. Their new 3D model suite provides a greatly expanded perspective and appears to explain some observed pulsar properties by default.\n\nImpact\n\nThe team’s simulations represent the largest set of long-term 3D state-of-the-art core-collapse simulations ever created. These simulations lay the groundwork for more comprehensive research that will address other aspects and outcomes of core collapse and their dependence upon progenitors. These simulations provide a qualitative picture, paving the way to develop a quantitative explanation of the survival of binary neutron star systems.",
        "url": "http://localhost:4000//science/highlights/burrows"
      }
      
    
  };
  const performance_json = {
    
      "features-aurora-performance-highlights-connectomics": {
        "title": "A Large-Scale Foundation Model for Advancing Science: AuroraGPT",
        "content": "The highly collaborative AuroraGPT project leverages DOE supercomputing resources to develop and enhance understanding of powerful foundation models (FMs)— such as large language models (LLMs)—for science. In creating FMs for science—while developing underlying capabilities, tools and workflows, data resources, and other processes and artifacts—Argonne aims to significantly improve how science is conducted, by fostering a deeper integration of AI capabilities into research workflows. To this end, Argonne’s AuroraGPT project is creating and evaluating a series of increasingly powerful FMs, each with more parameters and/or trained on more data than its predecessors, designed to assist researchers in making more informed and efficient discoveries. The AuroraGPT research program focuses on producing this sequence of models while ensuring that each provides scientifically useful capabilities as well as scientific and computational performance knowledge to guide the design of the next model in the sequence\n\nChallenge\nThe main tasks in the project include collecting and refining large-scale scientific datasets; building models at 8 billion to 400 billion or more parameter scales using general texts, code, and specific scientific data, and evaluating their performance on the Aurora and Polaris supercomputers; refining the models for deployment and introducing post-processing techniques such as instruct tuning and reinforcement learning for aligned chat-based interfaces; and evaluating the effectiveness of the models on scientific tasks.\n\nPretraining of AuroraGPT models has accounted for the bulk of the project’s compute resources. For each model, the developers aim to train some 2 trillion tokens, requiring many extensive, large-scale runs on Aurora. Initially, the developers found that the processing time necessary to establish indexing function for 2 trillion tokens was approximately 1 hour. Improvement of training performance has therefore been priority. The developers maintain a GitHub repository, Megatron DeepSpeed, which can train LLMs efficiently at scale on Aurora and Polaris. It has also been important to create new architecture-agnostic tools for LLMs and to adapt existing LLM tools for architecture agnosticism.\n\nPerformance Results\nAfter debugging and implementing certain logic enhancements, the developers successfully reduced preprocessing time from approximately 1 hour to 2 minutes. This vastly improved both the AuroraGPT code and the developers’ ability to improve the code. Considerable effort has been put into making checkpoints convertible between the AuroraGPT software stack frameworks, particularly Megatron-DeepSpeed and Hugging Face.\n\nThe developers have successfully trained a model on 2 trillion tokens. After adapting the Meta-created LLM Llama for use on Aurora—involving the modification of the number of model layers and their sizes—the developers determined which data set to train it on and then optimized the implementation to get as close as possible to 100 percent utilization of compute resources. Initial training runs of the models helped the team identify instabilities to overcome. With the Llama LLM running smoothly, the developers were then able train it on 2 trillion tokens, effectively creating a ready, pretrained model.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nAuroraGPT represents a transformative opportunity to leverage AI for scientific discovery, potentially redefining problem-solving across various domains critical to the DOE’s mission. AuroraGPT will enable Aurora users to both train LLMs from scratch and finetune existing models. One pretrained model—trained on 2 trillion tokens—is already complete, providing Aurora users with a framework for deploying off-the-shelf LLMs for science campaigns and helping accelerate the pace of research while expanding the technology’s accessibility and deepening AI implementation in workflows.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/connectomics"
      }
      ,
    
      "features-aurora-performance-highlights-copper": {
        "title": "Cooperative Caching Layer for Scalable Parallel Data Movement in Exascale Supercomputing: Copper",
        "content": "Users of exascale computers like the ALCF’s Aurora—which feature more than 10,000 compute nodes—can experience as much as 20 minutes of idle time for a single line of code during application initialization. Copper, a scalable data loading library developed at the ALCF by an Argonne research team, addresses this problem—reducing load times by as much as 95 percent—by freeing up energy and compute resources. This is achieved without any changes to application codes, by means of a cooperative caching layer.\n\nChallenge\nThe ALCF’s file system struggled to accommodate requests commensurately with the rate at which users made them, leading to frequent crashes. The researchers responded to this I/O bottleneck by introducing a data loader capable of reducing storage-side congestion: Copper, a read-only cooperative caching layer.\n\nWithout the use of algorithms, Copper reduces initialization and loading times by creating a remote procedure call (RPC) overlay network tree with local cache on each node. The root node performs the data loading from the underlying storage system and then distributes to requesting nodes over the ALCF’s Slingshot network.\n\nPerformance Results\nCopper users merely need to append a prefix to the data path without any application code changes. This simple path change has enabled scaling with near constant data-loading on many thousands of nodes of the Aurora system, as demonstrated in a paper presented at the 2024 Supercomputing Conference (SC24). By freeing up highly contended computing resources and network infrastructure with vastly reduced I/O demands, Copper improves performance in numerous subtle ways.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nCopper has not only drastically reduced the total time to load data and software libraries, but has also freed up such previously contended, highly demanded resources such as filesystem responders and storage network bandwidth. The savings in time and energy have enabled the ALCF to permit more science-campaign jobs with more efficient utilization of resources. Copper also provides an effective means to adopt and reuse existing software libraries developed within the DOE laboratory system. The researchers are currently assisting other exascale computing facilities with the deployment of Copper.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/copper"
      }
      ,
    
      "features-aurora-performance-highlights-harvey": {
        "title": "Extreme-Scale Visualization and Analysis of Fluid-Structure Interactions: HARVEY",
        "content": "HARVEY, a massively parallel computational fluid dynamics code that predicts and simulates how blood cells flow through the human body, is used to study the mechanisms driving disease development, inform treatment planning, and improve clinical care.A team of researchers led by Duke University aims to repurpose HARVEY to improve our understanding of the physical mechanisms driving tumor metastasis in cancer.\n\nChallenge\n\nOne in four deaths in the United States is due to cancer, and metastasis is responsible for more than 90 percent of these deaths. The metastatic patterns of circulating tumor cells (CTCs) are strongly influenced by both a favorable microenvironment and mechanical factors such as blood flow.\n\nAdvancing the use of data science to drive in situ analysis of extreme-scale fluid-structure- interaction (FSI) simulations, this work aims to leverage the ALCF’s exascale Aurora system to model and analyze the movement of CTCs through the complex geometry of the human vasculature and thereby lay the groundwork for a predictive model of cancer metastasis. Simulating the rare CTCs, nearby red blood cells, and underlying fluid of the arterial network presents not only a computationally challenging simulation but also a large data problem for posterior analysis. Scalable and in situ analysis of massively parallel FSI models, including cellular-level flow, will be critical for enabling new scientific insights into the mechanisms driving cancer progression.\n\nHARVEY is based on the lattice Boltzmann method (LBM) for fluid dynamics. Advantages of the LBM over other numerical solvers of the Navier-Stokes equations include its amenability to parallelization due to its underlying stencil structure and the local availability of physical quantities, eliminating the need for global communication among processors required of Poisson solvers.\n\nHARVEY adopts the MPI+X parallelization model for execution on diverse accelerator architectures, where X was originally OpenMP for manycore CPUs and CUDA for GPUs. The code consists of highly optimized kernels tailored to exascale system architectures for solving the main components of the LBM and handling FSI interactions among millions of finite element cells. Over the last few years, HARVEY has been ported to SYCL, HIP, OpenMP Target, and Kokkos to enable functionality and performance on heterogeneous hardware across a variety of supercomputing systems.\n\nThe researchers have successfully built the code’s framework on Aurora, on top of which they integrated in-situ visualization in the Kokkos. This integration has been verified by rendering the fluid velocity field generated by the in-situ visualization of red blood cells flowing through complex vascular geometries.\n\nPerformance Results\nAs of 2024, the researchers have demonstrated HARVEY’s functionality on Aurora for two distinct cases. First, researchers scaled fluid-only simulations on full-body human vasculature anatomies. Next, HARVEY demonstrated weak scaling performance of millions of red blood cells in FSI simulations. Both cases were performed on as many as 2048 nodes of the Aurora supercomputer.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tScreening ~40-60B of the most synthesizable compounds made possible using the system capabilities and software stack on Aurora. * Simplified Molecular Input Line Entry System (SMILES) - Representation for Molecules.\n\t\t\t\n\t\t\n\t\n\n\nImpact\nThe exascale-optimized HARVEY application will offer the ability to create personalized models for individual patients. Blood flow simulations have the potential to greatly benefit the diagnosis and treatment of patients suffering from vascular disease. By simulating how cancer cells interact with different vascular environments, HARVEY helps uncover what makes certain regions of the body more susceptible to secondary tumor formation and what characteristics of the cancer cells contribute to metastasis. Empowering models of the full arterial tree can provide insight into diseases such as arterial hypertension while enabling the study of how local factors impact global hemodynamics.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/harvey"
      }
      ,
    
      "features-aurora-performance-highlights-phasta": {
        "title": "Machine Learning-Guided Computational Fluid Dynamics at Extreme Scales",
        "content": "This University of Colorado-led effort is aimed at developing a scalable framework to enable in-situ machine learning (ML) capabilities from exascale simulations, with the goal of using high-fidelity data from direct numerical simulations (DNS) of complex, wall-bounded turbulent flows to train accurate and generalizable sub-grid stress models for large eddy simulations (LES). The LES performed using the PHASTA application—with up to a billion degrees of freedom—will help elucidate the physics of flow separation at high Reynolds numbers, aiding the design of more aerodynamic and efficient aircraft. The project also aims to address I/O bottlenecks accentuated by the rise of exascale computing. Aurora enables simulations to be conducted at unprecedented scales, producing more data than ever before—but also producing bottlenecks via the traditional practice of saving the simulation data to disk. By performing ML model training concurrently with LES, researchers can avoid the file system altogether and efficiently stream data between simulation and ML training.\n\nChallenge\nA main challenge the developers face was translating the legacy, FORTRAN-based PHASTA application code—optimized for central processing unit- (CPU-) powered computing systems—to run on exascale graphics processing unit- (GPU-) based architectures. PHASTA’s compilers and code structures did not readily lend themselves to GPUs, leading the developers to leverage the libCEED application library, developed under the aegis of the Exascale Computing Project. libCEED adds wrapper functions to architecture-agnostic codes for physical models such that the models can be run in a finite-element framework. libCEED minimizes global traffic through the GPU memory hierarchy, effectively boosting performance.\n\nEnabling the online ML component required that the developers ensure the framework for carrying out large-scale, high-fidelity LES alongside distributed ML training (as well as the periodic transfer of terabyte-scale training data) scale to thousands of nodes with minimal overhead. To this end, the development team leveraged the open-source library SmartSim, allowing them to decouple the data transfer between simulation and training by staging the data in a database deployed on the compute nodes. In this way, the rate of data production from the simulation is completely independent of the data consumption rate of the ML training, with neither component blocking the progress of the other. Additionally, the team spearheaded a co-located framework with SmartSim which places all components (simulation, training, and database) on the same set of nodes, sharing the available CPU and GPU resources. With this approach, all training data transfer between simulation and ML training is confined within a single node: even though both the simulation and distributed training span thousands of nodes, the framework is, in effect, made scalable.\n\nPerformance Results\nThe development team has been able to run the PHASTA application on Aurora using as many as 4096 nodes; depending on the problem size, the team has achieved a desirable timestep on the order of 0.6 seconds.\n\nScaling on up to 1536 nodes, the total overhead generated by the CFD simulation and ML trainer has remained small—approximately 5 percent of runtime—and, importantly, effectively constant. These results have demonstrated that the framework is both performant and scalable.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nBringing PHASTA to Aurora has provided a scalable and performant solution to performing in-situ ML at exascale, thereby enabling researchers to access and utilize much richer datasets for training ML models using simulation-generated data. Additionally, accuracy and uncertainty quantification metrics obtained from the ongoing training inform the data-generation process within the simulation, which can help permit intelligent data sub-sampling techniques that train models on fewer data samples with reduced energy costs.\n\nPerforming high-resolution simulations of turbulent flows at high Reynolds numbers will help refine the turbulence models currently available, while improving the robustness and accuracy of turbine models. These models can in turn be used for improved design of aircraft and other mechanical systems.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/phasta"
      }
      ,
    
      "features-aurora-performance-highlights-uintah": {
        "title": "Reverse Monte-Carlo Radiative Ray Tracing Calculations at Scale: Uintah",
        "content": "The Uintah Computational Framework, an open-source asynchronous many-task (AMT) runtime system, has been modified to be performance and large-scale portable across exascale DOE supercomputing systems, including Aurora. This work is the culmination of Uintah’s decade-long, University of Utah-led preparation for DOE exascale systems through highly collaborative multidisciplinary efforts pursued in conjunction with Argonne, Oak Ridge National Laboratory, and Intel as a part of the Predictive Science Academic Alliance Program II and the Aurora Early Science Program. Notable updates to Uintah’s support for Kokkos were required to make this extension possible through Uintah’s adoption of a portable MPI+X hybrid parallelism approach using the Kokkos performance portability library (that is, MPI and Kokkos).\n\nChallenge\n\nTo scale application codes to hundreds and thousands of Aurora processes via the AMT framework, the development team focused on a Reverse Monte-Carlo Ray Tracing radiation benchmark calculation, central to the University of Utah’s predictive boiler simulations. This benchmark involves potentially global all-to-all communication and uses adaptive mesh refinement and ray tracing to achieve scalability. This benchmark has been used as part of previous scalability studies on a number of pre-exascale systems and the DOE-operated exascale Frontier system while harnessing 98 percent of the machine.\n\nPerformance Results\nWorking on Aurora, the team was able prepare for a run that would scale from 1280 nodes up to 10,240 nodes successfully for the Burns and Christon benchmark problem on a 2-level structured adaptive mesh refinement grid with more than 129 billion cells, demonstrating that good strong-scaling efficiency is achievable.\n\nThese large-scale studies explored the impact of a larger refinement ratio to determine if aggressive mesh refinement will make full-system runs possible. The Uintah runtime systems have proved exceptionally capable in enabling the application to run scalably on Aurora.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\n\nImpact\nThe scalability results obtained are a promising start and help show the capabilities of Uintah as an exascale AMT runtime system and pave the way for scientific simulations, such as fluid-structure interaction problems or simulations of turbulent reacting flows at unprecedented sizes on exascale machines. Additional work is underway to improve performance, and to better understand how to balance the tradeoffs between computation loads and communications costs when running at the largest scales.",
        "url": "http://localhost:4000//features/aurora-performance-highlights/uintah"
      }
      
    
  };
  const pages_json = {
    
      "kitchen-sink": {
          "title": "Kitchen Sink",
          "content": "H1 Subhead\n\nH2 Subhead\n\nH3 Subhead\n\nH4 Subhead\n\nParagraph text, the quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. Includes italic text, bold text, highlight, strikethrough, and \tlinks.\n\n\n  Unordered list item 1\n  Unordered list item 2\n  Unordered list item 3\n\n\n\n  Ordered list item 1\n  Ordered list item 2\n  Ordered list item 3\n\n\nCode block\nvar example = example;\n\n\n\n  \n    \n      Example\n      Table\n      Table subhead\n      Table subhead\n    \n  \n  \n    \n      Value 1a\n      Value 1b\n      Value 1c\n      Value 1d\n    \n    \n      Value 2a\n      Value 2b\n      Value 2c\n      Value 2d\n    \n    \n      Value 3a\n      Value 3b\n      Value 3c\n      Value 3d\n    \n  \n\n\n\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\n\t\t\t\tImage: Name A. Name\n\t\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\n\n\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\t\n\t\t\t\t\tImage: Name A. Name\n\t\t\t\t\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\t\n\t\t\n\n\t\t\n\t\t\t\n\t\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\t\n\t\t\t\t\tImage: Name A. Name\n\t\t\t\t\n\t\t\t\n\t\t\n\t\n\n\n\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\n\t\t\tCredit: Name A. Name\n\t\t\n\t\n\n\n\n\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\n\t\t\t\tCredit: Name A. Name\n\t\t\t\n\t\t\n\t\n\t\n\t\t\n\t\t\t\n\t\t\n\t\t\n\t\t\tThe quick brown fox jumps over the lazy dog.\n\t\t\t\n\t\t\t\tCredit: Name A. Name\n\t\t\t\n\t\t\n\t\n\n\n\n\nExample Section with Meta Info\n\n\n\n\n\n\t\n\t\tNAME 1: \n\t\tValue 1\n\t\n\t\n\t\n\t\t\n\t\t\tNAME 2: \n\t\t\tValue 2\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tNAME 3: \n\t\t\tValue 3\n\t\t\n\t\n\n\t\n\n\t\n\n\t\n\n\n\nThe quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog. The quick brown fox jumps over the lazy dog.\n\n\n\n\n\n\n\t\n\t\tNumber of things\n\t\t3k\n\t\t\n\t\t\tthings from 2022 count of stuff\n\t\t\t\n\t\n\t\n\t\t\n\t\t\tPercentage of things\n\t\t\t75%\n\t\t\t\n\t\t\t\tthings from 2022 count of stuff",
          "url": "http://localhost:4000//kitchen-sink/"
        },
  
    
      "404-html": {
          "title": "",
          "content": "404\n\n  Page not found :(\n  The requested page could not be found.",
          "url": "http://localhost:4000//404.html"
        },
  
    
      "year-in-review-about-alcf": {
          "title": "About ALCF",
          "content": "The Argonne Leadership Computing Facility (ALCF), a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory, enables breakthroughs in science and engineering by providing supercomputing and AI resources to the research community.\n\nALCF computing resources—available to researchers from academia, industry, and government agencies—support large-scale computing projects aimed at solving some of the world’s most complex and challenging scientific problems. Through awards of computing time and support services, the ALCF enables researchers to accelerate the pace of discovery and innovation across a broad range of disciplines.\n\nAs a key player in the nation’s efforts to provide the most advanced computing resources for science, the ALCF is helping to chart new directions in scientific computing through a convergence of simulation, data science, and AI methods and capabilities.\n\nSupported by the DOE’s Advanced Scientific Computing Research (ASCR) program, the ALCF and its partner organization, the Oak Ridge Leadership Computing Facility, operate leadership-class supercomputing resources that are orders of magnitude more powerful than the systems typically used for open scientific research.",
          "url": "http://localhost:4000//year-in-review/about-alcf"
        },
  
    
      "about": {
          "title": "About",
          "content": "Argonne Leadership Computing Facility\nArgonne’s Leadership Computing Facility Division operates the Argonne Leadership Computing Facility (ALCF) as part of the U.S. Department of Energy’s effort to provide leadership-class computing resources to the scientific community. The ALCF is supported by the DOE Office of Science, Advanced Scientific Computing Research (ASCR) program.\n\nArgonne National Laboratory\nArgonne is a U.S. Department of Energy Laboratory managed by UChicago Argonne, LLC, under contract DE-AC02-06CH11357. The Laboratory’s main facility is outside of Chicago, at 9700 South Cass Avenue, Lemont, Illinois 60439. For information about Argonne and its pioneering science and technology programs, visit www.anl.gov.",
          "url": "http://localhost:4000//about"
        },
  
    
      "features-ai-training": {
          "title": "Preparing a New Generation of AI-Ready Researchers",
          "content": "Since its launch in 2021, the ALCF’s annual “Intro to AI-driven Science on Supercomputers” training series has been helping students develop the expertise to harness the power of AI and HPC to accelerate scientific discoveries and innovation.\n\nAimed at undergraduate and graduate students from U.S. universities and community colleges, the program offers a combination of hands-on sessions and expert guidance to equip attendees with the knowledge and skills to effectively use AI in their research. Over the past four years, the series has welcomed over 700 total participants.\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tALCF's Marieme Ngom leads a session on the basics of using neural networks for scientific research.\n\t\t\n\t\n\n\nComprehensive AI for Science Curriculum\n\nThe primary goal of the series is to introduce students to using AI and HPC for science, equipping them with the skills needed to apply these technologies in their own research. The training covers fundamental AI concepts, while also giving participants the opportunity to work directly with ALCF’s powerful systems, such as Polaris and the ALCF AI Testbed. By providing this foundational knowledge, the series helps participants bridge the gap between theoretical AI concepts and practical application using world-class computational resources.\n\nWith weekly sessions led by ALCF AI experts, the program provides detailed instruction on topics like large language models, neural networks, model training, and AI accelerators. In addition, attendees learn about the infrastructure necessary to run large-scale AI models efficiently, from computational strategies to optimizing workloads on advanced hardware systems like GPUs and novel AI accelerators.\n\nEach session concludes with a science talk, delivered by different domain scientists, offering insights into how various Argonne teams are using AI to drive advancements across fields such as biology, cosmology, and materials science. These talks not only provide context for how AI is applied but also highlight the potential of supercomputing resources in accelerating science and engineering.\n\nA key feature of the series is its hands-on learning approach, where participants apply their new skills using ALCF computing resources. Each session includes exercises that reinforce key concepts, while also providing a platform for students to explore AI applications in science. At the end of the program, participants who complete all exercises receive a certificate of completion and a digital badge, marking their achievements.\n\nExpanding Reach and Impact\n\nThe ALCF training series continues to expand its reach, with over 250 students registered in 2024 alone. By offering free access to advanced tools and resources, the program plays a vital role in developing the next generation of AI researchers and practitioners.\n\nIn addition to the live sessions, ALCF has made all training materials and session recordings publicly available on YouTube and GitHub. This ensures that even students who cannot attend live sessions have access to the curriculum and can continue their education at their own pace. By broadening access to these valuable resources, the training series empowers students nationwide to gain practical, real-world experience with AI and HPC.\n\nThrough its commitment to education and the development of an AI-ready workforce, the ALCF’s training series is playing a vital role in shaping the future of AI-driven science. By preparing students to leverage the power of AI and supercomputers, the program is empowering a new generation of researchers who will drive scientific breakthroughs and innovations in the years to come.\n\nAttendees who complete all in-class and post-class exercises by the end of the series receive a certificate of completion and a digital badge.",
          "url": "http://localhost:4000//features/ai-training"
        },
  
    
      "features-alcf-ai-testbed": {
          "title": "ALCF Continues to Expand AI Testbed Systems Deployed for Open Science",
          "content": "In 2024, the ALCF AI Testbed initiated further upgrades with the deployment of SambaNova Suite and established benchmarks for its optimized Cerebras Wafer-Scale Cluster WSE-2 that promise to make extreme-scale AI computing substantially more manageable and effective.\n\nThe testbed is a growing collection of some of the world’s most advanced AI accelerators available for open science. Designed to enable researchers to explore next-generation machine learning applications and workloads to advance AI for science, the systems are also helping the facility to gain a better understanding of how novel AI technologies can be integrated with traditional supercomputing systems powered by CPUs and GPUs. With the AI Testbed, the ALCF user community can leverage novel AI technologies for innovative research projects involving large language models (LLMs), large-scale data analysis, and the development of trustworthy AI.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tThe ALCF AI Testbed includes systems from Cerebras, Graphcore, Groq, and SambaNova.\n\t\t\t\n\t\t\n\t\n\n\nThe testbed’s AI accelerators are equipped with unique hardware and software features to efficiently handle a variety of AI tasks, including:\n\n\n  AI Model Training: Using large datasets to “teach” an AI model to detect patterns and make accurate, trustworthy predictions.\n  Inference: Employing a trained AI model to make predictions on new data.\n  Large Language Models (LLMs): AI models that are trained on large amounts of text data to understand, generate, and predict text-based content.\n  Computer Vision Models: AI models that are trained to understand and analyze visual data for tasks such as image classification and object recognition.\n  Foundation Models: Similar to LLMs, these AI models are trained on diverse datasets to perform a broad set of processing tasks. Foundation models, however, can serve as a starting point for developing more specialized AI models for specific domains or applications.\n\n\nThese methods are powerful tools for speeding up scientific progress. Computer vision models can help scientists automate the analysis of images generated by microscopes, x-ray light sources, and other imaging techniques. LLMs, on the other hand, are helping researchers to sift through massive amounts of published scientific data quickly to identify promising materials for medicines, batteries, and other uses.\n\nExperimental data analysis also benefits from the lab’s AI Testbed. Researchers from Argonne’s Advanced Photon Source (APS) are exploring how different accelerators can enable fast, scalable AI model training and inference to accelerate the analysis of x-ray imaging data. Rapid data analysis methods are becoming increasingly important for the APS and other experimental facilities as data generation rates continue to grow.\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tArgonne's Rick Stevens discusses how modern science requires access to powerful AI systems for inference to drive scientific discovery.\n\t\t\n\t\n\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tALCF's Venkat Vishwanath discusses how AI inference is becoming a large and critical workload for advancing science.\n\t\t\n\t\n\n\nSambaNova\nThe ALCF AI Testbed began further expansion of its SambaNova platform with the addition of SambaNova Suite. Powered by SambaNova DataScale SN40L systems under installation, SambaNova Suite is a fully integrated hardware-software platform that enables users to train, fine tune, and deploy AI workloads. Optimized for low-latency, high-throughput inference, the platform provides scientists with a new AI resource to accelerate scientific research.\n\nThe deployment of the DataScale SN40L system will extend advanced AI inference capabilities beyond the ALCF’s traditional user base. By making trained AI models more accessible, the platform aims to enable a wider community of researchers to explore new directions in generative and agentic AI workloads for science and engineering.\n\nBeing able to rapidly evaluate AI models and adjust parameters for improved performance is crucial for driving progress in AI-driven science across many research areas, including drug discovery, materials identification, and brain mapping.\n\nThe ALCF’s platform contains sixteen of SambaNova’s Reconfigurable DataFlow Units (RDU). The system’s capabilities support the development of large foundation models like Argonne’s AuroraGPT, which is being built to enable autonomous scientific exploration across disciplines, including biology, chemistry, and materials science. AuroraGPT is being trained on Argonne’s Aurora exascale system.\n\nThe ability to switch between different AI models instantly and fine-tune them using domain-specific datasets can help streamline the process of testing and validating their performance.\n\nThe system also gives the lab a new platform to continue its explorations into energy-efficient technologies for next-generation supercomputers and data centers, as one of the aims of the ALCF AI Testbed is to determine how novel AI accelerators like the SN40L can be integrated with future supercomputers to enhance energy efficiency.\n\nCerebras\nThe Cerebras system, previously upgraded to a Wafer-Scale Cluster WSE-2, optimized the ALCF’s existing Cerebras CS-2 system to include two CS-2 engines, enabling near-perfect linear scaling of large language models (LLMs). This capability helps make extreme-scale AI substantially more manageable.\n\nAn Argonne-led research team examined the feasibility of performing continuous energy Monte Carlo (MC) particle transport on the Cerebras WSE-2—simulations with the potential to fill in crucial gaps in experimental and operational nuclear reactor data.\n\nThe researchers ported a key kernel from the MC transport algorithm to the Cerebras Software Language programming model and evaluated the performance of the kernel on the Cerebras WSE-2. The team developed an architecture-specific optimization to leverage the capabilities of the WSE-2 and a highly optimized CUDA kernel for testing on a conventional graphics processing unit (GPU), which served as a baseline to contextualize the WSE-2’s performance.\n\nA single WSE-2 was found to run 130 times faster than the highly optimized CUDA version of the kernel deployed on the conventional GPU comparison—significantly outpacing expected performance increase, given the difference in transistor counts between the architectures. A follow-up study saw the WSE-2 achieve a 182x speedup over the GPU.\n\nThe team’s analysis suggests the potential for a wide variety of complex and irregular simulation methods to be mapped efficiently onto AI accelerators like the Cerebras WSE-2, providing users with an invaluable tool for effective scientific discovery.",
          "url": "http://localhost:4000//features/alcf-ai-testbed"
        },
  
    
      "science-allocation-programs": {
          "title": "Accessing ALCF Resources for Science",
          "content": "ALCF’s Katherine Riley explains how researchers can apply for time on ALCF systems through allocation programs supported by DOE and Argonne.\n\t\t\n\t\n\n\nResearchers gain access to ALCF systems for computational science and engineering projects through competitive, peer-reviewed allocation programs supported by the DOE and Argonne.\n\nThe ALCF also hosts competitive, peer-reviewed application programs designed to prepare key scientific applications and innovative computational methods for the architecture and scale of DOE supercomputers.\n\nAllocation Programs\n\nINCITE\nThe Innovative Novel Computational Impact on Theory and Experiment (INCITE) program aims to accelerate scientific discoveries and technological innovations by awarding ALCF computing time and resources to large-scale, computationally intensive projects that address grand challenges in science and engineering.\n\nALCC\nThe ASCR Leadership Computing Challenge (ALCC) program allocates ALCF computing resources to projects that advance the DOE mission; help to broaden the community of researchers capable of using leadership computing resources; and serve the national interests for scientific discovery, technological innovation, and economic competitiveness.\n\nDirector’s Discretionary\nDirector’s Discretionary projects are dedicated to leadership computing preparation, INCITE and ALCC scaling, and efforts to maximize scientific application efficiency and productivity on leadership computing platforms.\n\nESP\nAs part of the process of bringing a new supercomputer into production, the ALCF conducts its Early Science Program (ESP) to prepare applications for the architecture and scale of a new system. ESP projects represent a typical system workload at the ALCF and cover key scientific areas and numerical methods.",
          "url": "http://localhost:4000//science/allocation-programs"
        },
  
    
      "features-aurora": {
          "title": "Aurora Reaches Exascale, Leads in AI Performance",
          "content": "Over the past year, the ALCF made significant progress in preparing its Aurora exascale supercomputer for deployment in January 2025. In close collaboration with Intel and Hewlett Packard Enterprise, the team completed extensive system validation, verification, and scale-up efforts to pave the way for system acceptance.\n\nDedicated to open science, Aurora will provide the research community with powerful simulation, AI, and data analysis capabilities to drive breakthroughs in physics, engineering, materials science, and other domains.\n\nBreaking the Exascale Barrier\n\nAurora demonstrated its capabilities in various performance benchmarks revealed at the ISC and SC conferences, further cementing its place among the world’s most powerful supercomputers. The ALCF system officially broke the exascale barrier in June, achieving 1.012 exaflops on the High Performance LINPACK (HPL) benchmark. Aurora also set a new record for AI performance, registering 11.6 exaflops on the HPL-MxP mixed-precision benchmark. Its strengths in data-intensive applications were further highlighted with leading results on the Graph500 and HPCG benchmarks, while its storage system, DAOS, retained the top ranking on the IO500 production list.\n\nTogether with Oak Ridge National Laboratory’s Frontier and Lawrence Livermore National Laboratory’s El Capitan, DOE is now home to the world’s first three exascale systems. These machines not only mark the first to reach exascale but are also the three fastest supercomputers on the TOP500 List.\n\nBuilt in partnership with Intel and HPE, Aurora’s architecture represents a first-of-its-kind deployment, integrating cutting-edge technologies at an unprecedented scale. Equipped with 63,744 GPUs and 84,992 network endpoints, the system is designed to tackle complex computational challenges in ways that were previously unimaginable.\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tArgonne's Rick Stevens discusses how researchers will use Aurora to revolutionize the use of AI for science.\n\t\t\n\t\n\n\nWorld-Class Simulation, AI, and Data Capabilities\n\nAurora’s computing power and advanced capabilities are expected to transform research across a wide range of scientific domains. Ahead of the system’s deployment, teams participating in DOE’s Exascale Computing Project (ECP) and the ALCF’s Aurora Early Science Program (ESP) have demonstrated its potential in training large-scale AI models and carrying out extreme-scale modeling and simulation campaigns.\n\nOne key target involves the development of AI-driven scientific models that can accelerate discovery across multiple disciplines, including materials design, drug development, and energy research. The system is also being prepared to support high-fidelity simulations of complex systems, such as the human circulatory system, nuclear reactors, and supernovae, to gain new insights into their behavior. Additionally, its capacity to process massive datasets will be critical for analyzing the growing data streams from large-scale research facilities such as Argonne’s Advanced Photon Source and CERN’s Large Hadron Collider.\n\nPreparing for Science on Day One\n\nBringing a system of this scale online has required close collaboration among the ALCF, Intel, HPE, and researchers from the DOE’s Exascale Computing Project and Aurora Early Science Program. Throughout 2024, these teams worked to optimize codes and stress-test the system, ensuring it would be ready for science from day one of production.\n\nA co-design approach was essential in this effort, with hardware and software developed in tandem to maximize performance and usability. As part of this process, researchers ran early science applications to fine-tune their software for Aurora’s architecture, resulting in a suite of computational tools that will be ready to accelerate discoveries as soon as the system becomes fully operational.\n\nWith acceptance testing and final preparations completed in December, Aurora is poised to drive significant advancements in scientific computing. Its advanced capabilities are empowering researchers to tackle some of the most challenging problems in science and engineering at an unprecedented scale and speed, unlocking discoveries and insights more quickly than ever before.",
          "url": "http://localhost:4000//features/aurora"
        },
  
    
      "credits": {
          "title": "Credits",
          "content": "ALCF Leadership: Michael E. Papka (Division Director), Bill Allcock (Director of Operations), Susan Coghlan (ALCF-X Project Director, Kalyan Kumaran (Director of Technology), Jini Ramprakash (Deputy Division Director), and Katherine Riley (Director of Science)\n\nEditorial Team: Beth Cerny, Jim Collins, Nils Heinonen, Logan Ludwig, and Laura Wolf\n\nDesign and Production: Sandbox Studio, Chicago",
          "url": "http://localhost:4000//credits"
        },
  
    
      "year-in-review-directors-letter": {
          "title": "Director’s Letter",
          "content": "The process of planning for and installing a supercomputer takes years. It includes a critical period of stabilizing the system through validation, verification, and scale-up activities, which can vary for each machine. However, unlike ALCF’s previous or current production machines, Aurora’s long ramp-up journey has also included several configuration changes and COVID-related supply chain issues.\n\nAurora is a highly advanced system designed for various AI and scientific computing applications. It will also be used to train a one-trillion-parameter large language model for scientific research. Aurora’s architecture boasts more endpoints in the interconnect technology than any other system, and it has over 60,000 GPUs, making it the system with the largest number of GPUs in the world.\n\nIn 2023, ALCF made significant progress toward realizing Aurora’s full capabilities. In June, Aurora completed the installation of its 10,624th and final blade. Shortly after, Argonne shared the results of benchmarking runs for about half of Aurora to the TOP500. These results were used in the November announcement of the world’s fastest supercomputers, where Aurora secured the second position. Once the full system goes online, its theoretical peak performance is expected to be approximately two exaflops.\n\nSome application teams participating in the DOE’s Exascale Computing Project and the ALCF’s Aurora Early Science Program have begun using Aurora to scale and optimize their applications for the system’s initial science campaigns. Soon to follow will be all the early science teams and an additional 24 INCITE research teams in 2024.\n\nThis new exascale machine brings with it some more big changes. Theta, one of ALCF’s production systems, was retired on December 31, 2023. ThetaGPU will be decoupled and reconfigured to become a new system named Sophia, which will be used for AI development and as a production resource for visualization and analysis. Meanwhile, the ALCF AI Testbed will continue to make more production systems available to the research community.\n\nFor more than three decades, researchers at Argonne have been developing tools and methods that connect powerful computing resources with large-scale experiments, such as the Advanced Photon Source and the DIII-D National Fusion Facility. Their work is shaping the future of inter-facility workflows by automating them and identifying ways to make these workflows reusable and adaptable for different experiments. Argonne’s Nexus effort, in which ALCF plays a key role, offers the framework for a unified platform to manage high-throughput workflows across the HPC landscape.\n\nIn the following pages, you will learn more about how Nexus supports the DOE’s goal of building a broadscale Integrated Research Infrastructure (IRI) that leverages supercomputing facilities for experiment-time data analysis. The IRI will accelerate the next generation of data-intensive research by combining scientific facilities, supercomputing resources, and new data technologies like AI, machine learning, and edge computing.\n\nIn 2023, we continued our commitment to education and workforce development by organizing a number of informative learning experiences and training events. As part of this effort, ALCF staff members led a pilot program called “Introduction to High-Performance Computing Bootcamp” in collaboration with other DOE labs. This was an immersive program designed for students in STEM to work on energy justice projects using computational and data science tools learned throughout the week. In a separate effort, the ALCF worked on developing the curriculum for its “Intro to AI-Driven Science on Supercomputers” training course, with the aim of adapting the content to introduce undergraduates and graduates to the basics of large language models for future course offerings.\n\nTo conclude, I express my sincere gratitude to the exceptional staff, vendor partners, and program office, who have all contributed to making ALCF one of the leading scientific supercomputing facilities in the world. Each year, we take the time to share our numerous achievements with you in our Annual Report, and while there are many more exciting changes on the horizon, I truly appreciate this opportunity.",
          "url": "http://localhost:4000//year-in-review/directors-letter"
        },
  
    
      "disclaimer": {
          "title": "Disclaimer",
          "content": "This report was prepared as an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor UChicago Argonne, LLC, nor any of their employees or officers, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of document authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof, Argonne National Laboratory, or UChicago Argonne, LLC.",
          "url": "http://localhost:4000//disclaimer"
        },
  
    
      "community-and-outreach-educational-outreach-activities": {
          "title": "Inspiring Students",
          "content": "ALCF Student Summer Program\nEvery summer, the ALCF opens its doors to a new class of student researchers who work alongside staff mentors to tackle research projects that address issues at the forefront of scientific computing. In 2024, facility hosted more than 40 students ranging from high school seniors to Ph.D. candidates. Students contributed to ALCF projects spanning from scaling up deep learning benchmark applications for exascale computing to compressing data for AI models that can give us insights into nuclear fusion, and had hands-on experience with some of the most advanced computing technologies in the world.  For a recap of the 2024 program, read the article on our website.\n\nBig Data Camp\nArgonne’s annual Big Data Camp introduced high school juniors and seniors to the advanced tools used by professional data scientists. Campers were required to have coding experience and learned techniques for probing and analyzing massive scientific datasets, such as the dataset from the Array of Things (AoT) urban sensor project. Participants were also introduced to the foundational concepts underlying artificial intelligence and machine learning and why supercomputers are so important to use these tools effectively. This camp was organized by Argonne’s Educational Programs and Outreach staff and taught by ALCF scientists and visualization experts, hosting 12 participants in 2024.  For more details, visit the camp website.\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tArgonne's John Domyancich leads hands-on activities demonstrating how neural networks and classification models are used for data analysis at the lab's annual Big Data Camp.\n\t\t\t\n\t\t\n\t\n\n\nBreakthrough Tech Sprinternship Program\nALCF participated for the first time in the Breakthrough Tech Sprinternship program, run in partnership with the University of Illinois Chicago. The program offers quick and immersive micro-internships designed to support computer science undergraduates who identify as women and non-binary. ALCF hosted a team of 5 students for a 3-week program in which the developed a program to more effectively track allocations awards and their impact on scientific publications. All students were offered an extension to their sprinternship through the end of Summer 2024, and 4 of the 5 students accepted the extension.\n\nCodeGirls@Argonne Camp\nThe annual CodeGirls@Argonne Camp hosts sixth- and seventh-grade girls each summer for a five-day event dedicated to teaching them the fundamentals of coding. The camp highlights the essential role that women have played throughout history in technology development and invites lab researchers to talk to the students and share how they turned their coding interests into careers. The girls in the camp build friendships as they work together to solve coding-related challenges. For a recap on the 2024 event, read Argonne’s article on CodeGirls@Argonne Camp.\n\nCoding for Science Camp\nAimed at high school students, Argonne’s Coding for Science Camp features programming activities that link computational science with current scientific challenges. Over the course of the week-long camp, the students enhance their problem-solving and teamwork skills through hands-on activities, while discovering how computing is useful and often essential to solving problems in science. They also get an opportunity to interact with Argonne staff members to explore the diverse career pathways that need coding skills. For a recap on the 2024 event, read Argonne’s article on the 2024 Coding for Science Camp.\n\nCSEdWeek/Hour of Code\nAs part of the national Computer Science Education Week (CSEdWeek) and the Hour of Code in December, ALCF staff members provide in-person and virtual talks and demos to Chicago area schools to spark interest in computer science. Working with students in classes from elementary to high school, the volunteers led a variety of activities designed to teach the basics of coding. CSEdWeek was established by Congress in 2009 to raise awareness about the need to elevate computer science education at all levels.\n\nIntroduce a Girl to Engineering Day\nALCF staff members regularly serve as mentors and volunteers for Argonne’s Introduce a Girl to Engineering Day (IGED) program. The annual event gives eighth-grade students a unique opportunity to discover engineering careers alongside Argonne’s world-class scientists and engineers. Participants hear motivational presentations by Argonne engineers, tour the lab’s cutting-edge research facilities, connect with mentors, engage in hands-on engineering experiments, and compete in a team challenge.\n\nIntroduction to AI-driven Science on Supercomputers: A Student Training Series\nThe AI-driven Science on Supercomputers 7-8-week webinar series is aimed at undergraduate and graduate students enrolled at U.S. universities and community colleges and designed to attract a new generation of AI users by having a low entry barrier; that is, attendees need to have only a basic experience with the Python programming language as the pre-requisite. This year’s focus was on introducing participants to foundational concepts in AI and ML and then diving into the fundamentals of large language models. ALCF computer scientists led the weekly sessions and hands-on exercises along with talks by Argonne scientists who use AI in their research. The programs run in 2024 trained just over 500 attendees from over 100 universities from the U.S. and beyond, including undergraduates, graduate students, postdocs, and faculty from fields spanning computer science to materials science to biology. For a recap of the 2024 event, read the article on our website.\n\nScience Careers in Search of Women\nALCF staff members continued to contribute to Argonne’s annual Science Careers in Search of Women (SCSW) conference. The event hosts female high school students for a day of inspiring lectures, facility tours, career booth exhibits, and mentoring. SCSW provides participants with the unique experience to explore their desired profession or area of interest through interaction with Argonne’s women scientists and engineers. For a recap of the 2024, read the article on Argonne’s website.",
          "url": "http://localhost:4000//community-and-outreach/educational-outreach-activities"
        },
  
    
      "science-highlights": {
          "title": "2024 Science Highlights",
          "content": "Biological Sciences\n  \t\tMProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization\n      PI: Arvind Ramanathan, Argonne National Laboratory \n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Chemistry\n  \t\tMicroscopic Insight into Transport Properties of Li-Battery Electrolytes\n      PI: Wei Jiang, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Computer Science\n  \t\tDemonstrating Cross-Facility Data Processing at Scale with Laue Microdiffraction\n      PI: Michael Prince, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Computer Science\n  \t\tEfficient Algorithms for Monte Carlo Particle Transport on AI Accelerator Hardware\n      PI: John Tramm, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Engineering\n  \t\tFrom Quantum Mechanics to Hypersonic Aerothermodynamics\n      PI: Maninder Grover, University of Dayton Research Institute\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Engineering\n  \t\tRobust Gas Turbine Film Cooling Under Manufacturing Uncertainty for Improved Jet Engine Lifecycle Energy Efficiency\n      PI: Michael Joly, RTX Technology Research Center; Pinaki Pal and Muhsin Ameen, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Engineering\n  \t\tThree-Dimensional Shock Boundary Layer Interactions over Flexible Walls\n      PI: Johan Larsson, University of Maryland; Ivan Bermejo-Moreno, University of Southern California\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Physics\n  \t\tExascale Gyrokinetic Study of ITER Challenge on Power-Exhaust and ELM-Free Edge\n      PI: Choongseok Chang, Princeton Plasma Physics Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Physics\n  \t\tSimulating the Cosmos for the Roman and Rubin Telescopes\n      PI: Katrin Heitmann, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      Physics\n  \t\tState-of-the-Art High-Resolution 3D Simulations of Core-Collapse Supernovae\n      PI: Adam Burrows, Princeton University",
          "url": "http://localhost:4000//science/highlights"
        },
  
    
      "community-and-outreach-hpc-community-activities": {
          "title": "Shaping the Future of Supercomputing",
          "content": "Computing Conferences and Events\n\nALCF researchers regularly contribute to some of the world’s leading computing conferences and events to share their latest advances in areas ranging from computational science and AI to HPC software and exascale technologies. In 2024, Argonne staff participated in a wide range of events including:\n\n  SC24\n  ISC High Performance\n  Grace Hopper Celebration,\n  SIAM Conference on Computational Science and Engineering,\n  Richard Tapia Celebration of Diversity in Computing Conference\n  IEEE International Parallel &amp; Distributed Processing Symposium\n  International Conference on Parallel Processing\n  International Symposium on Cluster\n  Cloud and Grid Computing\n  International Workshop on OpenCL and SYCL\n  Platform for Advanced Scientific Computing Conference\n  HPC User Forum\n  Energy High-Performance Computing Conference\n  Lustre User Group Conference\n  Intel eXtreme Performance Users Group Conference\n  Conference on Machine Learning and Systems\n  American Physical Society\n  Energy HPC\n  High-Performance Computing Security\n  International Conference On Preconditioning Techniques For Scientific and Industrial Applications\n  IEEE Conference on Artificial Intelligence\n  IRI/HPDF Meeting\n  PEARC 2024\n  NIST Artificial Intelligence for Materials Science (AIMS) Workshop\n  ADAC Workshop\n  ISO C++ Conference\n  and more\n\n\nHPC Standards and Community Groups\n\nALCF staff members remain actively involved in several HPC standards and community groups that help drive improvements in the usability and efficiency of scientific computing tools, technologies, and applications. Staff activities include contributions to the following:\n\n  C++ Standards Committee\n  Cray User Group\n  DAOS Foundation\n  HPC User Forum\n  HPSF High Performance Software Foundation\n  Intel eXtreme Performance Users Group (IXPUG)\n  Khronos OpenCL and SYCL Working Groups\n  LDMS User Group\n  UXL Silver Member/Steering Member\n  MLCommons\n  NITRD Middleware and Grid Infrastructure Team\n  Open Fabrics Alliance\n  Open Scalable File Systems (OpenSFS) Board\n  OpenMP Architecture Review Board\n  OSTI ORCiD Consortium Membership\n  SPEC (Standard Performance Evaluation Corporation)\n  HPG (High Performance Group)\n  Better Scientific Software\n  Energy Efficient High Performance Computing\n  Unified Communications Framework\n  OCHAMI\n  and more\n\n\n\n\n\n\t\n\t\t\n\t\n\n\t\n\t\t\n\t\t\tALCF's Sam Foreman presents over AuroraGPT at the 2024 Hands-on HPC Workshop.\n\t\t\t\n\t\t\n\t\n\n\nPerformance Portability\n\nThe ALCF continued its collaboration with NERSC and OLCF to operate and maintain a website dedicated to enabling performance portability across the DOE Office of Science HPC facilities. The website serves as a documentation hub and guide for applications teams targeting systems at multiple computing facilities. The DOE computing facilities staff also collaborate on various projects and training events to maximize the portability of scientific applications on diverse supercomputer architectures.\n\nVendor Collaborations\n\nThe ALCF works closely with many companies in the HPC and AI industries to develop and deploy cutting-edge hardware and software for the research community. This includes collaborating with Intel and HPE to deliver the Aurora exascale system, working with HPE to deploy the Polaris testbed supercomputer, and partnering with NVIDIA on system enhancements and training related to ThetaGPU. Such partnerships are critical to ensuring the facility’s supercomputing resources meet the requirements of the scientific computing community. In addition, the ALCF is working with several AI start-up companies, including Cerebras, Graphcore, Groq, and SambaNova, to deploy a diverse set of AI accelerators as part of the ALCF AI Testbed. The testbed is playing a key role in determining how AI accelerators can be applied to scientific research, while also allowing vendors to prepare their software and hardware for scientific AI workloads.",
          "url": "http://localhost:4000//community-and-outreach/hpc-community-activities"
        },
  
    
      "": {
          "title": "Argonne Leadership Computing Facility",
          "content": "&#9650; Researchers used the ALCF’s Polaris supercomputer to perform GPU-enabled weather simulations at cloud-resolving (1 km) spatial resolution for the month of September 2017. Covering the entire North American continent and approximately 3 billion grid cells, this visualization shows water vapor (red-blue) and cloud water (white-grayscale) fields, with wind vectors highlighting the jet stream pattern.\n\t\tImage: ALCF Visualization and Data Analytics Team; Rao Kotamarthi and Gökhan Sever, Argonne National Laboratory\n\t\n\n\n\n \n\n\n\t\n\t\tFeatures\n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tAurora Reaches Exascale, Leads in AI Performance\n\t\t\t    \t\tIn 2024, the system surpassed the exascale barrier, demonstrated its world-class AI capabilities, and completed critical preparations for its release to the research community.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tAurora Performance Highlights\n\t\t\t    \t\tWith Aurora fully assembled, early science team members began running and optimizing their applications to prepare the system for open access. Here are some early performance results on Aurora.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tLinking Experimental Facilities and Leadership Computing\n\t\t\t    \t\tArgonne researchers are working to advance the DOE’s vision by integrating experimental facilities with ALCF computing resources.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tALCF Continues to Expand AI Testbed Systems Deployed for Open Science\n\t\t\t    \t\tBy updating and expanding its testbed for AI accelerators, the ALCF enables users to harness leading-edge AI technologies for efficient and impactful scientific discovery.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\n\t\t\t\t\t\t  \n\t\t\t  \n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tPreparing a New Generation of AI-Ready Researchers\n\t\t\t    \t\tThe ALCF’s “Intro to AI” student training series has hosted over 700 participants from across the nation to date, helping to cultivate a new generation of AI researchers.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\t \n\t\t\t\t\n\t\t\t\n\t\t\t    \n\t\t\t    \t\n\t\t\t    \t\t\n\t\t\t    \t\t\n\t\t\t    \t\n\t\t\t    \t\n\t\t\t    \t\tScience Highlights\n\t\t\t    \t\tIn 2024, researchers from around the globe leveraged ALCF's supercomputing and AI resources to drive breakthroughs across a diverse array of scientific fields, ranging from AI-driven protein design to hypersonic aircraft research to fusion energy science.\n\t\t\t    \t\n\t\t\t    \n\t\t\t  \t\n\t\n\n\n\n\n\n\t\n\t\tInsights\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tTimothy Williams discusses how Aurora will transform scientific computing.\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tSalman Habib explains Aurora's role in advancing computational cosmology.\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\t\t\n\t\t\t\n\t\t\t\n\t\t\t\tAnouar Benali shares how Aurora will revolutionize materials discovery.",
          "url": "http://localhost:4000//"
        },
  
    
      "community-and-outreach-industry": {
          "title": "Powering Breakthroughs for U.S. Industry",
          "content": "The ALCF’s Industry Partnerships Program enables companies of all sizes—from startups to Fortune 500 corporations—to leverage leadership computing resources and expertise. These collaborations help industry partners to tackle R&amp;D challenges that exceed the capabilities of traditional computing systems.\n\nWith state-of-the-art capabilities for simulation, AI, and data analysis, ALCF computing resources help researchers create higher-fidelity models, achieve more accurate predictions, and quickly process massive datasets. As a result, companies can accelerate breakthroughs, reduce uncertainties, and minimize the need for costly prototypes.\n\nThe ALCF continues to expand its industry outreach efforts by partnering with other Argonne divisions and user facilities, including the Science and Technology Partnership Outreach (STPO) division. This integrated approach provides a more comprehensive understanding of the laboratory’s capabilities and promotes broader, more impactful partnerships.\n\nCollaboration Opportunities\nThe ALCF offers several avenues for industry partners to access its computing resources. These include the INCITE, ALCC, and Director’s Discretionary allocation programs, which provide researchers with opportunities to apply for computing time and collaborate on high-impact projects.\n\nFor companies focused on addressing energy-related challenges in manufacturing or materials development, DOE’s High Performance Computing for Energy Innovation (HPC4EI) program is another available option. HPC4EI provides access to DOE’s world-class supercomputing resources through two subprograms: HPC4Mfg, which focuses on optimizing manufacturing processes, and HPC4Mtls, which supports the development of advanced materials that perform well in harsh or complex environments.\n\nThrough these programs, the ALCF helps U.S. industries stay competitive in an evolving global market. By providing access to HPC and AI computing resources and expertise, these partnerships enable companies to tackle complex challenges, accelerate the R&amp;D process, and drive technological innovation.\n\n\n\nTo explore potential collaboration opportunities with the ALCF, contact us at industry@alcf.anl.gov\n\n\n\nDriving R&amp;D Innovation\nFrom AI-driven workflows to large-scale simulations, the following examples illustrate how companies are using ALCF resources to advance their R&amp;D efforts.\n\nMSBAI\nResearchers at MSBAI are using DOE’s exascale supercomputers as part of an ALCC project aimed at enhancing GURU, an autonomous system designed to reduce modeling and simulation setup time from hours to minutes for a range of engineering applications. The team will use ALCF’s Aurora and OLCF’s Frontier to drive advancements in hybrid interaction, skills agent learning, and geometry search and synthesis – core components of GURU’s AI-driven workflow automation. By addressing key barriers in simulation and CAD data handling, the project aims to accelerate product development across industries such as energy, automotive, aerospace, and medical devices, while democratizing access to HPC resources.\n\nRTX Technology Research Center\nWith support from DOE’s HPC4EI program, researchers from RTX Technology Research Center are working with Argonne to develop reduced-order deep learning surrogate models to capture the impact of manufacturing uncertainties on the performance of film cooling schemes used for thermal management of aviation gas turbines. Reliable film cooling drives durability and thermal efficiency in gas turbine engines but is greatly sensitive to variations in the shape of cooling holes due to surface roughness induced by the manufacturing process. To this end, the team leveraged ALCF supercomputers and Argonne’s highly scalable nekRS solver to perform morphology-resolved computational fluid dynamics simulations of gas turbine film cooling schemes incorporating surface roughness of the cooling hole. Detailed analysis of the numerical results was carried out to gain insights into the impact of various surface roughness parameters on the downstream film cooling effectiveness. High-fidelity datasets from these simulations will be combined with data from coarse-grained simulations to develop multi-fidelity deep learning surrogate models to predict the impact of surface roughness on film cooling effectiveness. The team’s framework aims to help the company improve the fuel efficiency and durability of aircraft engines while reducing design times and costs.\n\nTAE Technologies\nTAE Technologies is leveraging ALCF supercomputers to advance fusion energy research and accelerate the development of a commercially viable fusion-based electricity generator. By running high-fidelity simulations, researchers are gaining critical insights into plasma stability and the conditions needed to sustain fusion reactions. This work supports the development of field-reversed configuration plasma and neutral beam injection techniques, key to achieving the extreme temperatures required for fusion with proton-boron-11 fuel—a safer, low-waste alternative to conventional fusion methods. These advancements are accelerating the design of compact, efficient fusion reactors, moving the world closer to a transformative, near-limitless energy source.",
          "url": "http://localhost:4000//community-and-outreach/industry"
        },
  
    
  
    
      "features-nexus-iri": {
          "title": "Linking Experimental Facilities and Leadership Computing",
          "content": "As the volume of data generated by large-scale experiments continues to grow, the need for rapid data analysis capabilities is becoming increasingly critical to new discoveries.\n\nArgonne’s Advanced Photon Source Upgrade project (APS-U) is increasing the brightness of APS x-rays by as much as 500 times. With a corresponding increase in the amounts of experimental data generated, high-performance computing (HPC) resources are required to quickly process and analyze the results.\n\nNumerous ALCF activities and achievements have helped realize the DOE effort to build an Integrated Research Structure (IRI) that seamlessly connects experimental facilities with its world-class supercomputing resources, including:\n\n\n  Developing and testing methods to closely integrate supercomputers and experiments for near-real-time data analysis.\n  Partnering with Pathfinder projects to advance plasma physics and fusion energy research.\n  Participating in leadership groups and technical subcommittees dedicated to the design and implementation of computing facility functionality useful for experimentalists.\n\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tAt SC24, Sterling Smith of the DIII-D National Fusion Facility led a technical demo showing how they are using DOE supercomputers for experiment-time data analysis.\n\t\t\n\t\n\n\nNexus\nUnder Argonne’s Nexus effort, Argonne researchers are working to advance the DOE’s vision by integrating experimental facilities with ALCF computing resources.\n\nAs IRI aims to deliver DOE-enterprise-wide infrastructure for computing, ALCF has continued its commitment to linking experimental facilities with ALCF computing. Work with the APS over recent years has been a primary driver for defining new functionality and services ALCF has deployed to satisfy experiment-time computing needs at APS beamlines. Service accounts enable APS users to leverage automated analysis of their data at ALCF in a shared environment throughout their multi-day beamline campaigns, with jobs running immediately at experiment time in the on-demand queue on Polaris. Analysis results are available to scientists at the beamline via Globus Sharing enabled on the Eagle filesystem, at the time of experiment and post-hoc. Building on these ALCF-deployed features, Globus Compute and Globus Flows manage application execution and data transfer in a frictionless manner, for projects across the DOE-SC program offices.\n\n\n\n\n\t\n\t\n\t\t\n\t\n\t\n\n\t\n\t\tArgonne's Nicholas Schwarz discusses how integrating the APS with ALCF computing resources will allow researchers to collect, analyze, and act on data on a scale that has never been possible before.\n\t\t\n\t\n\n\nFacility Integration\n\nFor over a decade, the ALCF and the APS have been collaborating to build the infrastructure for integrated ALCF-APS research, including the development of workflow management tools and enabling secure access to on-demand computing.\n\nWith the upgraded APS providing x-rays up to 500 times brighter than before, the APS-ALCF collaboration is providing increased computational power at experiment time. More than 20 beamlines housed at the APS identified significant computing needs and have engaged the full power of ALCF’s Nexus services and functionality, using service accounts for transparent access to the ALCF, and the demand queue for time-sensitive analysis of beamline data through integration with the APS Data Management System. With more beamlines coming online with ever greater computational needs, the APS demand for ALCF supercomputing resources and newly upgraded inter-facility network connectivity will continue to grow.\n\nExpanding and Demonstrating Capabilities\n\nIn a recent achievement of facility integration for near-real-time data analysis, Argonne deployed a fully automated pipeline that uses ALCF resources to rapidly process data obtained from the x-ray experiments at the APS.\n\nTo demonstrate the capabilities of the pipeline, Argonne researchers carried out a study focused on a technique called Laue microdiffraction, which is employed at the APS and other light sources to analyze materials with crystalline structures. The team used the ALCF’s Polaris supercomputer to reconstruct data obtained from an APS experiment, returning reconstructed scans to the APS within 15 minutes of them being sent to the ALCF. The beamline technique introduced in the study allows users to collect data about 10 times faster than was previously possible.\n\nThese results carry implications for future software development, engineering, and beamline science.\n\nArgonne researchers showcased the use of the Polaris system for processing data from APS experiments in near-real time during a demonstration at the SC24 conference.\n\nAdditional experiments and papers presented at the SC24 XLOOP workshop explored multiple IRI-related issues, including the scaling capabilities of file-based reconstruction of ptychography data—which requires particularly short data-processing turnaround times. New scans on the APS beamline storage system were automatically transferred to ALCF’s Eagle file system through Globus using a file-based workflow, which automatically launched reconstruction jobs on Polaris compute nodes using the on-demand queue. Once the reconstruction results were available on Eagle, they were transferred back to the APS through the same Globus transfer workflow.\n\nIn a related example, working with a team at the Lawrence Berkeley National Laboratory Advanced Light Source, ALCF staff have helped to automate analysis of data from a tomography beamline on Polaris. Using a service account to submit jobs to Polaris through Globus Compute and the demand queue to analyze data at experiment time, the team has moved beyond an initial prototype and is now able to run analysis in a dedicated discretionary allocation. This production-ready capability is planned to be used in upcoming beamline experiments.\n\nPartnering to Advance Energy Technologies\n\nThe Plasma Physics and Fusion Energy Pathfinder aims to incorporate remote use of high-performance computing into experiments running at DOE’s\nDIII-D National Fusion Facility in San Diego, California.\n\nEach DIII-D experiment runs on a 20-minute cycle that requires time-sensitive analysis of the data generated to inform and prepare the next experiment. Working closely with the DIII-D team and NERSC researchers, ALCF staff have collaborated with teams from DIII-D and the National Energy Research Scientific Computing Center (NERSC) to improve and automate the Consistent Automatic Kinetic Equilibrium (CAKE) workflow, developed and implemented at DIII-D to produce low-error, kinetically constrained magnetic equilibrium reconstructions without human intervention. The automation has yielded dramatic spikes in reconstruction productivity.\n\nALCF staff also worked to automate the Ion Orbiter workflow, which simulates particle trajectories and determines their hit locations on tokamak walls, culminating in production-ready analysis during experiments at DIII-D using Globus Flows to analyze data automatically between experiments Ultimately the IonOrbiter workflow will enable control-room personnel to quickly determine future wall-heating to enable plasma adjustments as needed.\n\nBoth the CAKE and Ion Orbiter workflows were demonstrated in the DOE booth at the Supercomputing 24 (SC24) conference.\n\nLeading the Future of Inter-Facility Science\n\nALCF staff participate in and co-chair weekly Leadership Group meetings to direct overall IRI efforts and specific tasks for technical subcommittees, form new subcommittees, and work with the Pathfinder projects. In 2024, ALCF staff served on the organizing committee for the IRI/HPDF kickoff meeting in Gaithersburg, Maryland, and produced related materials describing outcomes from the meeting. ALCF staff also presented during the Leadership Group’s participation in the DOE ASCAC meeting in May 2024.\n\nALCF staff have participated in all existing IRI technical subcommittees from day one, including Outreach and Engagement, Interfaces, and TRUSTID. These groups are dedicated to designing and building functionality at computing facilities to facilitate their use by experimentalists.",
          "url": "http://localhost:4000//features/nexus-iri"
        },
  
    
      "features-aurora-performance-highlights": {
          "title": "Aurora Performance Highlights",
          "content": "A Large-Scale Foundation Model for Advancing Science: AuroraGPT\n      PI: Rick Stevens, Argonne National Laboraotory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tCooperative Caching Layer for Scalable Parallel Data Movement in Exascale Supercomputing: Copper\n      PI: Venkat Vishwanath, Argonne National Laboratory\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tExtreme-Scale Visualization and Analysis of Fluid-Structure Interactions: HARVEY\n      PI: Amanda Randles, Duke University\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tMachine Learning-Guided Computational Fluid Dynamics at Extreme Scales\n      PI: Kenneth Jansen, University of Colorado\n  \t\n  \n\n\n\n\n\n  \n  \t\n  \t\t\n  \t\t\n  \t\n  \t\n      \n  \t\tReverse Monte-Carlo Radiative Ray Tracing Calculations at Scale: Uintah\n      PI: Martin Berzins, The University of Utah",
          "url": "http://localhost:4000//features/aurora-performance-highlights"
        },
  
    
      "science-projects": {
          "title": "2024 Award List",
          "content": "INCITE 2024\n\nBIOLOGICAL SCIENCES\n\nCOMbining Deep-Learning with Physics-Based affinIty estimatiOn 3 (COMPBIO3)\nPI: Peter Coveney, University College London\nHOURS: ALCF: 1,625,000 Node-Hours\n\nEstablishing Digital Twins for High-Throughput Cellular Analysis in Whole Blood\nPI: Amanda Randles, Duke University\nHOURS: ALCF: 60,000 Node-Hours, OLCF: 240,000 Node-Hours\n\nExaCortex: Exascale Reconstruction of Human Cerebral Cortex\nPI: Nicola Ferrier, Argonne National Laboratory\nHOURS: ALCF: 250,000 Node-Hours\n\nFoundation Models for Predictive Molecular Epidemiology\nPI: Arvind Ramanathan, Argonne National Laboratory\nHOURS: ALCF: 320,000 Node-Hours\n\nOpenFold-Powered Machine Learning of Protein-Protein Interactions and Complexes\nPI: Mohammed AlQuraishi, Columbia University\nHOURS: ALCF: 200,000 Node-Hours\n\nCOMPUTER SCIENCE\n\nDemocratizing AI by Training Deployable Open-Source Language Models\nPI: Abhinav Bhatele, University of Maryland\nHOURS: ALCF: 150,000 Node-Hours, OLCF: 600,000 Node-Hours\n\nCHEMISTRY\n\nExascale Catalytic Chemistry\nPI: David Bross, Argonne National Laboratory\nHOURS: ALCF: 425,000 Node-Hours\n\nHeterogeneous Catalysis as a Collective Phenomenon within a Dynamic Ensemble of Sites\nPI: Anastassia Alexandrova, University of California, Los Angeles\nHOURS: ALCF: 250,000 Node-Hours\n\nHeteropolymer Design Harnessing New and Emerging Computing Technologies\nPI: Vikram Mulligan, Flatiron Institute\nHOURS: ALCF: 375,000 Node-Hours\n\nEARTH SCIENCE\n\nEnergy Exascale Earth System Model\nPI: Peter Caldwell, Lawrence Livermore National Laboratory\nHOURS: ALCF: 500,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nENGINEERING\n\nFlight-Scale Simulations of a Transport Aircraft in High-Lift Conditions\nPI: Eric Nielsen, NASA Langley Research Center\nHOURS: ALCF: 350,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nHigh Reynolds Number Hypersonic Transition Control via Porous Walls\nPI: Carlo Scalo, Purdue University\nHOURS: ALCF: 300,000 Node-Hours\n\nInterface-resolved Simulations of Scalar Transport in Turbulent Bubbly Flows]\nPI: Parisa Mirbod, University of Illinois at Chicago\nHOURS: ALCF: 50,000 Node-Hours\n\n[Online Machine Learning for Large-Scale Turbulent Simulations]https://www.alcf.anl.gov/science/projects/online-machine-learning-large-scale-turbulent-simulations-0)]\nPI: Kenneth Jansen, University of Colorado Boulder\nHOURS: ALCF: 375,000 Node-Hours\n\nMATERIALS SCIENCE\n\nCarbon at Extremes: Discovery Science with Exascale Computers\nPI: Ivan Oleynik, University of South Florida\nHOURS: ALCF: 300,000 Node-Hours, OLCF: 300,000 Node-Hours\n\nExascale Simulations of Quantum Materials\nPI: Paul Kent, Oak Ridge National Laboratory\nHOURS: ALCF: 590,000 Node-Hours, OLCF: 600,000 Node-Hours\n\nHeterogeneous Reaction Dynamics for Energy Storage and Hydrogen Production\nPI: Boris Kozinsky, Harvard University\nHOURS: ALCF: 150,000 Node-Hours, OLCF: 300,000 Node-Hours\n\nPHYSICS\n\n3D Imaging of Strong Interaction Nambu-Goldstone Bosons\nPI: Yong Zhao, Argonne National Laboratory\nHOURS: ALCF: 150,000 Node-Hours\n\nAb-initio Nuclear Structure and Nuclear Reactions\nPI: Gaute Hagen, Oak Ridge National Laboratory\nHOURS: ALCF: 500,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nAdvances in Quark and Lepton Flavor Physics with Lattice QCD\nPI: Andreas Kronfeld, Fermilab\nHOURS: ALCF: 1,200,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nExascale Models of Astrophysical Thermonuclear Explosions\nPI: Katrin Heitmann, Argonne National Laboratory\nHOURS: ALCF: 375,000 Node-Hours, OLCF: 800,000 Node-Hours\n\nExascale Gyrokinetic Study of ITER Challenge on Power-Exhaust and ELM-Free Edge\nPI: Choongseok Chang, Princeton Plasma Physics Laboratory\nHOURS: ALCF: 550,000 Node-Hours, OLCF: 1,000,000 Node-Hours\n\nExascale Models of Astrophysical Thermonuclear Explosions\nPI: Michael Zingale, Stony Brook University\nHOURS: ALCF: 75,000 Node-Hours, OLCF: 400,000 Node-Hours\n\nNovel Calculation of Nucleon Generalized Form Factors in Lattice QCD at the Physical Point\nPI: Martha Constantinou, Temple University\nHOURS: ALCF: 315,000 Node-Hours\n\nRadiation-Dominated Black Hole Accretion\nPI: James Stone, Institute for Advanced Studies\nHOURS: ALCF: 425,000 Node-Hours, OLCF: 800,000 Node-Hours\n\nResolving Cosmic Ray Transport by Pushing the Frontier of MHD Turbulence\nPI: Drummond Fielding, Cornell University\nHOURS: ALCF: 200,000 Node-Hours, OLCF: 700,000 Node-Hours\n\nState-of-the-Art High-Resolution 3D Simulations of Core-Collapse Supernovae\nPI: Adam Burrows, Princeton\nHOURS: ALCF: 1,000,000 Node-Hours\n\nQCD under Extreme Conditions\nPI: Zoltan Fodor, Pennsylvania State University\nHOURS: ALCF: 400,000 Node-Hours\n\nALCC 2024–2025\n\nBIOLOGICAL SCIENCES\n\nBuilding Digital Twin of a Model Host-Pathogen System for Enhancing Biopreparedness\nPI: Margaret S. Cheung, Pacific Northwest National Laboratory\nHOURS: ALCF: 200,000 Node-Hours, OLCF, 50,000 Node-Hours, NERSC: 300,000 Node-Hours\n\nFoundation Neuroscience AI Model-NeuroX\nPI: Shinjae Yoo, Brookhaven National Laboratory\nHOURS: ALCF: 290,000 Node-Hours, OLCF: 152,000 Node-Hours, NERSC: 120,000 Node-Hours\n\nCOMPUTER SCIENCE\n\nScalable and Resilient Modeling for Federated Learning Systems and Applications\nPI: Xiaoyi Lu, University of California\nHOURS: ALCF: 44,800 Node-Hours, OLCF: 15,360 Node-Hours, NERSC: 207,520 Node-Hours\n\nCHEMISTRY\n\nExploring Exascale Quantum Chemical Methods for Transition Metal Chemistry\nPI: Daniel Mejia Rodriguez, Pacific Northwest National Laboratory\nHOURS: ALCF: 313,442 Node-Hours, OLCF: 145,628 Node-Hours\n\nHighly Scalable Ab Initio Simulations of N-Doped Porous Materials for Carbon Capture\nPI: Mark Gordon, Ames National Laboratory\nHOURS: ALCF: 2,000,000 Node-Hours\n\nENERGY TECHNOLOGIES\n\nDNS of Buoyancy Driven Flows for Developing NN-informed High-fidelity Turbulence Closures\nPI: Som Dutta, Utah State University\nHOURS: ALCF: 100,000 Node-Hours, OLCF: 300,000 Node-Hours, NERSC: 200,000 Node-Hours\n\nExascale Computing for Energy Applications\nPI: Misun Min, Argonne National Laboratory\nHOURS: ALCF: 250,000 Node-Hours, OLCF: 200,000 Node-Hours, NERSC: 50,000 Node-Hours\n\nHigh-Fidelity CFD Enabling Advanced Nuclear Power\nPI: Dillon Shaver, Argonne National Laboratory\nHOURS: ALCF: 150,000 Node-Hours, OLCF: 200,000 Node-Hours\n\nHigh Fidelity Numerical Analysis on Flow and Heat Transfer behavior in Involute Plate Research Reactor to Support the Conversion Program\nPI: Yiqi Yu, Argonne National Laboratory\nHOURS: ALCF: 200,000 Node-Hours, OLCF: 500,000 Node-Hours\n\nHigh-Fidelity Simulations of Helium-Air Mixing in High-Temperature Gas Reactor Cavities\nPI: Saumil Patel, Argonne National Laboratory\nHOURS: ALCF: 115,000 Node-Hours\n\nENGINEERING\n\nAutonomy for DOE Simulations\nPI: Allan Grosvenor, MSBAI\nHOURS: ALCF: 30,000 Node-Hours, OLCF: 100,000 Node-Hours\n\nMATERIALS SCIENCE\n\nHigh Energy Density Physics of Novel Inertial Fusion Energy Ablator Materials\nPI: Ivan Oleynik, University of South Florida\nHOURS: ALCF: 600,000 Node-Hours, OLCF: 900,000 Node-Hours\n\nMachine Learning Enabled Atomistic Simulation of Iron at Extreme Pressure\nPI: Robert Rudd, Lawrence Livermore National Laboratory\nHOURS: ALCF: 150,000 Node-Hours\n\nPredicting Heterogeneous Photocatalysts Using Large-scale Ab Initio Calculations\nPI: Felipe Jornada, Stanford University\nHOURS: ALCF: 100,000 Node-Hours, OLCF: 140,000 Node-Hours\n\nPHYSICS\n\nHadronic Contributions to the Muon g-2 from Lattice QCD\nPI: Thomas Blum, University of Connecticut\nHOURS: ALCF: 1,000,000 Node-Hours, OLCF: 3,846,000 Node-Hours\n\nSimulating Large-scale Long-lived Neutron Star Remnants from Binary Neutron Star Mergers\nPI: Ore Gottlieb, Flatiron Institute\nHOURS: ALCF: 400,000 Node-Hours\n\nALCC 2023–2024\n\nBIOLOGICAL SCIENCES\n\nProbabilistic Comparative Modeling of Colorectal Cancer Screening Strategies\nPI: Jonathan Ozik, Argonne National Laboratory\nHOURS: ALCF: 160,000 Node-Hours, NERSC: 100,000 Node-Hours\n\nScaling Genomic Variant Callers to Leadership-Class Systems: A Collaboration Between VA-MVP and DOE\nPI: Ravi Madduri, Argonne National Laboratory\nHOURS: ALCF: 210,000 Node-Hours\n\nCHEMISTRY\n\nMicroscopic Insight Into Transport Properties of Li-Battery Electrolytes\nPI: Wei Jiang, Argonne National Laboratory\nHOURS: ALCF: 710,000 Node-Hours\n\nRelativistic Quantum Dynamics in the Non-Equilibrium Regime\nPI: Eugene DePrince, Florida State University\nHOURS: ALCF: 700,000 Node-Hours\n\nENERGY TECHNOLOGIES\n\nAccelerating Deployment of Next-Generation Nuclear Power Using High-Fidelity CFD\nPI: Dillon Shaver, Argonne National Laboratory\nHOURS: ALCF: 500,000 Node-Hours\n\nHigh Energy Density Physics of Inertial Confinement Fusion Ablator Materials\nPI: Ivan Oleynik , University of South Florida\nHOURS: ALCF: 500,000 Node-Hours, OLCF: 1,500,000 Node-Hours\n\nLarge Eddy Simulation on Flow and Heat Transfer Behavior in Involute Plate Research Reactor Supporting the Needs of the Materials Management and Minimization (M3) Reactor Conversion Program\nPI: Yiqi Yu, Argonne National Laboratory\nHOURS: ALCF: 510,000 Node-Hours\n\nENGINEERING\n\nTwo-Phase Flow Interface Capturing Simulations\nPI: Igor Bolotnov, North Carolina State University\nHOURS: ALCF: 200,000 Node-Hours, NERSC: 300,000 Node-Hours\n\nMATERIALS SCIENCE\n\nComputational Design of Novel Semiconductors for Power and Energy Applications\nPI: Feliciano Giustino , The University of Texas at Austin\nHOURS: ALCF: 100,000 Node-Hours\n\nLarge Scale Simulations of Materials for Quantum Information Science\nPI: Giulia Galli , University of Chicago\nHOURS: ALCF: 600,000 Node-Hours, NERSC: 400,000 Node-Hours\n\nQuantum Accurate Large-Scale Atomistic Simulations of Advanced Fusion Reactor Materials\nPI: Aidan Thompson, Sandia National Laboratories\nHOURS: ALCF: 850,000 Node-Hours, OLCF: 500,000 Node-Hours, NERSC: 250,000 Node-Hours\n\nPHYSICS\n\nContinuum Limit Latice Calculation of Direct CP-violation in Kaon Decays\nPI: Christopher Kelly, Brookhaven National Laboratory\nHOURS: ALCF: 135,000 Node-Hours\n\nEnergy Partition and Particle Acceleration in Laser-Driven Laboratory Magnetized Shocks\nPI: Frederico Fiuza, SLAC National Accelerator Laboratory\nHOURS: ALCF: 300,000 Node-Hours, NERSC: 150,000 Node-Hours\n\nHadronic Contributions to the Muon G-2 from Lattice QCD\nPI: Thomas Blum, University of Connecticut\nHOURS: ALCF: 5,000 Node-Hours, OLCF: 3,283,000 Node-Hours\n\nUsing GPU to Reconstruct LHC Collisions Recorded with the CMS Detector\nPI: Dirk Hufnagel , Fermi National Accelerator Laboratory\nHOURS: ALCF: 70,000 Node-Hours\n\nALCF Data Science Program\n\nAdvanced Materials Characterization with AI-Informed Computation\nPI: Marco Govoni, Argonne National Laboratory\n\nAutonomous Molecular Design for Redox Flow Batteries\nPI: Logan Ward, Argonne National Laboratory\n\nDeep Learning at Scale for Multimessenger Astrophysics Through the NCSA-Argonne Collaboration\nPI: Eliu Huerta, University of Illinois at Urbana-Champaign\n\nDeveloping High-Performance-Computing Applications for Liquid Argon Neutrino Detectors\nPI: Andrzej Szelc, University of Manchester\n\nDynamic Compressed Sensing for Real-Time Tomographic Reconstruction\nPI: Robert Hovden, University of Michigan\n\nLearning Optimal Image Representations for Current and Future Sky Surveys\nPI: George Stein, Lawrence Berkeley National Laboratory\n\nMachine Learning for Data Reconstruction to Accelerate Physics Discoveries in Accelerator-Based Neutrino Oscillation Experiments\nPI: Marco Del Tutto, Fermi National Accelerator Laboratory\n\nMachine Learning Magnetic Properties of Van Der Waals Heterostructures\nPI: Trevor Rhone, Rensselaer Polytechnic Institute\n\nX-ray Microscopy of Extended 3D Objects: Scaling Towards the Future\nPI: Chris Jacobsen, Argonne National Laboratory and Northwestern University\n\nAURORA EARLY SCIENCE PROGRAM\n\nAccelerated Deep Learning Discovery in Fusion Energy Science\nPI: William Tang, Princeton Plasma Physics Laboratory\n\nDark Sky Mining\nPI: Salman Habib, Argonne National Laboratory\n\nData Analytics and Machine Learning for Exascale Computational Fluid Dynamics\nPI: Kenneth Jansen, University of Colorado Boulder\n\nEnabling Connectomics at Exascale to Facilitate Discoveries in Neuroscience\nPI: Nicola Ferrier, Argonne National Laboratory\n\nExascale Computational Catalysis\nPI: David Bross, Argonne National Laboratory\n\nExtending Moore’s Law Computing with Quantum Monte Carlo\nPI: Anouar Benali, Argonne National Laboratory\n\nExtreme-Scale Cosmological Hydrodynamics\nPI: Katrin Heitmann, Argonne National Laboratory\n\nExtreme-Scale In-Situ Visualization and Analysis of Fluid-Structure-Interaction Simulations\nPI: Amanda Randles, Duke University\n\nExtreme-Scale Unstructured Adaptive CFD\nPI: Kenneth Jansen, University of Colorado Boulder\n\nHigh-Fidelity Simulation of Fusion Reactor Boundary Plasmas\nPI: C.S. Chang, Princeton Plasma Physics Laboratory\n\nMachine Learning for Lattice Quantum Chromodynamics\nPI: William Detmold, Massachusetts Institute of Technology\n\nMany-Body Perturbation Theory Meets Machine Learning to Discover Singlet Fission Materials\nPI: Noa Marom, Carnegie Mellon University\n\nNWChemEx: Tackling Chemical, Materials, and Biochemical Challenges in the Exascale Era\nPI: Theresa Windus, Iowa State University and Ames Laboratory\n\nSimulating and Learning in the ATLAS Detector at the Exascale\nPI: Walter Hopkins, Argonne National Laboratory\n\nVirtual Drug Response Prediction\nPI: Rick Stevens, Argonne National Laboratory\n\nDIRECTOR’S DISCRETIONARY\n\nThe following list provides a sampling of the many Director’s Discretionary projects at the ALCF.\n\nBIOLOGICAL SCIENCES\n\nLarge Ensemble Model of Single-Cell 3D Genome Structures\nPI: Jie Liang, University of Illinois at Chicago\n\nTargeting Intrinsically Disordered Proteins Using Artificial Intelligence Driven Molecular Simulations\nPI: Arvind Ramanathan, Argonne National Laboratory\n\nCHEMISTRY\n\nMultimodal Imaging with Intense X-ray Pulses\nPI: Phay Ho, Argonne National Laboratory\n\nCOMPUTER SCIENCE\n\nAPS Beamline Data Processing and Analysis\nPI: Rafael Vescovi, Argonne National Laboratory\n\nAdvanced Photon Source (APS) Data Processing\nPI: Nicholas Schwarz, Argonne National Laboratory\n\nMaterials Science, Data¬Driven Molecular Engineering of Solar-powered Windows\nPI: Jacqueline Cole, University of Cambridge\n\nOptimizing Bayesian Neural Networks for Scientific Machine Learning Applications\nPI: Murali Emani, Argonne National Laboratory\n\nENERGY TECHNOLOGIES\n\nOptimizing Bayesian Neural Networks for Scientific Machine Learning Applications\nPI: Joshua New, Oak Ridge National Laboratory\n\nDevelopment of High-Fidelity Simulations of Gas Turbine Combustors for Sustainable Aviation Applications\nPI: Sicong Wu, Northwestern University\n\nInvestigation of Flow and Heat Transfer Behavior in Involute Plate Research Reactor with Large Eddy Simulation to Support the Conversion of Research Reactors to Low Enriched Uranium Fuel\nPI: Yiqi Yu, Argonne National Laboratory\n\nMultiphase Simulations of Nuclear Reactor Thermal Hydraulics\nPI: Igor A. Bolotnov, North Carolina State University\n\nENGINEERING\n\nTurbulent Rayleigh-Benard Convection in Suspensions of Bubbles\nPI: Parisa Mirbod, University of Illinois at Chicago\n\nMATERIALS SCIENCE\n\nAdsorptive CO2 Removal from Dilute Sources\nPI: John J. Low, Argonne National Laboratory\n\nMaterials Informatics Study of Two-Dimensional Magnetic Materials and Their Heterostructures\nPI: Trevor David Rhone, Rensselaer Polytechnic Institute\n\nPHYSICS\n\nAnalytic Continuation of Interacting Fermion Spectra\nPI: Adrian Giuseppe Del Maestro, University of Tennessee\n\nNeural Network Quantum States for Atomic Nuclei\nPI: Alessandro Lovato, Argonne National Laboratory\n\nReproducible, Interpretable, and Physics-Inspired AI Models in Astrophysics\nPI: Eliu Huerta, Argonne National Laboratory",
          "url": "http://localhost:4000//science/projects"
        },
  
    
      "science-publications": {
          "title": "Publications",
          "content": "JANUARY\n\nAkinsanola, A. A., C. Jung, J. Wang, and V. R. Kotamarthi. “Evaluation of Precipitation Across the Contiguous United States, Alaska, and Puerto Rico in Multi-Decadal Convection-Permitting Simulations,” Scientific Reports (January 2024), Springer Nature. doi: 10.1038/s41598-024-51714-3\n\nBechtel Amara, T., S. P. Smith, Z. A. Xing, S. S. Denk, A. Deshpande, A. O. Nelson, C. Simpson, E. W. DeShazer, T. F. Neiser, O. Antepara, C. M. Clark, J. Lestz, J. Colmenares, N. Tyler, P. Ding, M. Kostuk, E. D. Dart, R. Nazikian, T. Osborne, S. Williams, T. Uram, and D. Schissel. “Accelerating Discoveries at DIII-D with the Integrated Research Infrastructure,” Frontiers in Physics (January 2024), Frontiers Media SA. doi: 10.3389/fphy.2024.1524041\n\nCheng, D., A. N. Alexandrova, and P. Sautet. “H-Induced Restructuring on Cu(111) Triggers CO Electroreduction in an Acidic Electrolyte,” The Journal of Physical Chemistry Letters (January 2024), ACS. doi: 10.1021/acs.jpclett.3c03202\n\nChitty-Venkata, K. T., S. Raskar, B. Kale, F. Ferdaus, A. Tanikanti, K. Raffenetti, V. Taylor, M. Emani, and V. Vishwanath. “LLM-Inference-Bench: Inference Benchmarking of Large Language Models on AI Accelerators,” SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (January 2024), Atlanta, GA, IEEE. doi: 10.1109/SCW63240.2024.00178\n\nDai, Z., C. Lian, J. Lafuente-Bartolome, and F. Giustino. “Excitonic Polarons and Self-Trapped Excitons from First-Principles Exciton-Phonon Couplings,” Physical Review Letters (January 2024), APS. doi: 10.1103/PhysRevLett.132.036902\n\nDai, Z., C. Lian, J. Lafuente-Bartolome, and F. Giustino. “Theory of Excitonic Polarons: From Models to First-Principles Calculations,” Physical Review B (January 2024), APS. doi: 10.1103/PhysRevB.109.045202\n\nFields, J., H. Zhu, D. Radice, J. M. Stone, W. Cook, S. Bernuzzi, and B. Daszuta. “Performance-Portable Binary Neutron Star Mergers with AthenaK,” The Astrophysical Journal Supplement Series (January 2024), IOP Publishing. doi: 10.3847/1538-4365/ad9687\n\nHammonds, J., S. Henke, P. R. Jemian, S. Kandel, H. Parraga L. Rebuffi, X. Shi, S. Veseli, M. Wolfman, M. Wyman, T. Zhou, R. Chard, B. Cote, W. Allcock, L. Assoufid, M. J. Cherukara, S. Kelly, A. Sandy, J. Sullivan, and N. Schwarz. “Advanced Computational Technologies for Experiment Control, Data Acquisition, and Data Analysis at the Advanced Photon Source,” Synchrotron Radiation News (January 2024), Informa UK Limited. doi: 10.1080/08940886.2023.2277136\n\nHirata, C. M., M. Yamamoto, K. Laliotis, E. Macbeth, M. A. Troxel, T. Zhang, K. Cao, A. Choi, J. Givans, K. Heitmann, M. Ishak, M. Jarvis, E. Kovacs, H. Long, R. Mandelbaum, A. Park, A. Porredon, C. W. Walter, and W. M. Wood-Vasey. “Simulating Image Coaddition with the Nancy Grace Roman Space Telescope - I. Simulation Methodology and General Results,” Monthly Notices of the Royal Astronomical Society (January 2024), The Royal Astronomical Society. doi: 10.1093/mnras/stae182\n\nHuang, D., and J. M. Cole. “A Database of Thermally Activated Delayed Fluorescent Molecules Auto-Generated from Scientific Literature with ChemDataExtractor,” Scientific Data (January 2024), Springer Nature. doi: 10.1038/s41597-023-02897-3\n\nHuang, J., J. Liu, S. Di, Y. Zhai, Z. Jian, S. Wu, K. Zhao, Z. Chen, Y. Guo, and F. Cappello. “Exploring Wavelet Transform Usages for Error-Bounded Scientific Data Compression,” 2023 IEEE International Conference on Big Data (BigData) (January 2024), Sorrento, Italy, IEEE. doi: 10.1109/bigdata59044.2023.10386386\n\nLi, Y., F. Zhang, V.-A. Ha, Y.-C. Lin, C. Dong, Q. Gao, Z. Liu, X. Liu, S. H. Ryu, H. Kim, C. Jozwiak, A. Bostwick, K. Watanabe, T. Taniguchi, B. Kousa, X. Li, E. Rotenberg, E. Khalaf, J. A. Robinson, F. Giustino, and C.-K. Shih. “Tuning Commensurability in Twisted van der Waals Bilayers,” Nature (January 2024), Springer Nature. doi: 10.1038/s41586-023-06904-w\n\nLi, Z., P. Chaturvedi, S. He, H. Chen, G. Singh, V. Kindratenko, E. A. Huerta, K. Kim, and R. Madduri. “FedCompass: Efficient Cross-Silo Federated Learning on Heterogeneous Client Devices Using a Computing Power-Aware Scheduler,” ICLR 2024 (January 2024), Vienna, Austria, ICLR. doi: 10.48550/arXiv.2309.14675\n\nLiu, J., M. Liu, J.-P. Liu, Z. Ye, Y. Wang, Y. Alexeev, J. Eisert, and L. Jiang. “Towards Provably Efficient Quantum Algorithms for Large-Scale Machine-Learning Models,” Nature Communications (January 2024), Springer Nature. doi: 10.1038/s41467-023-43957-x\n\nMa, B., V. Nikitin, D. Li, and T. Bicer. “Accelerated Laminographic Image Reconstruction Using GPUs,” Symposium on Electronic Imaging (January 2024), San Fransisco, CA. doi: 10.2352/EI.2024.36.12.HPCI-188\n\nMaeno, T., A. Alekseev, F. H. B. Megino, K. De, W. Guan, E. Karavakis, A. Klimentov, T. Korchuganova, F. Lin, P. Nilsson, T. Wenaus, Z. Yang, and X. Zhao. “PanDA: Production and Distributed Analysis System,” Computing and Software for Big Science (January 2024), Springer Nature. doi: 10.1007/s41781-024-00114-3\n\nNguyen-Cong, K., J. T. Willman, J. M. Gonzalez, A. S. Williams, A. B. Belonoshko, S. G. Moore, A. P. Thompson, M. A. Wood, J. H. Eggert, M. Millot, L. A. Zepeda-Ruiz, and I. I. Oleynik. “Extreme Metastability of Diamond and its Transformation to the BC8 Post-Diamond Phase of Carbon,” The Journal of Physical Chemistry Letters (January 2024), ACS Publications. doi: 10.1021/acs.jpclett.3c03044\n\nNikolakopoulos, A., A. Lovato, and N. Rocco. “Relativistic Effects in Green’s Function Monte Carlo Calculations of Neutrino-Nucleus Scattering,” Physical Review C (January 2024), APS. doi: 10.1103/PhysRevC.109.014623\n\nRen, Z., S. Elhatisari, T. A. Lähde, D. Lee, and U.-G. Meißner. “Ab Initio Study of Nuclear Clustering in Hot Dilute Matter,” Physics Letters B (January 2024), Elsevier. doi: 10.1016/j.physletb.2024.138463\n\nRoa Perdomo, D. A., R. A. Herrera Guaitero, D. Fox, H. Yviquel, S. Raskar, X. Li, and J. M. M. Diaz. “Towards Fault Tolerance and Resilience in the Sequential Codelet Model,” High Performance Computing: CARLA 2023 (January 2024), Cartagena, Colombia, Springer Nature, pp. 77-94. doi: 10.1007/978-3-031-52186-7_6\n\nTramm, J., B. Allen, K. Yoshii, A. Siegel, and L. Wilson. “Efficient Algorithms for Monte Carlo Particle Transport on AI Accelerator Hardware,” Computer Physics Communications (January 2024), Elsevier. doi: 10.1016/j.cpc.2023.109072\n\nFEBRUARY\n\nAdamson, R., P. Bryant, D. Montoya, J. Neel, E. Palmer, R. Powell, R. Prout, and P. Upton. “Creating Continuous Integration Infrastructure for Software Development on U.S. Department of Energy High-Performance Computing Systems,” Computing in Science and Engineering (February 2024), IEEE. doi: 10.1109/MCSE.2024.3362586\n\nBaars, W. J., G. Dacome, and M. Lee. “Reynolds-Number Scaling of Wall-Pressure-Velocity Correlations in Wall-Bounded Turbulence,” Journal of Fluid Mechanics (February 2024), Cambridge University Press. doi: 10.1017/jfm.2024.46\n\nBalaguru, K., S. S.-C. Wang, L. R. Leung, S. Hagos, B. Harrop, C.-C. Chang, S. W. Lubis, O. A. Garuba, and S. Taraphdar. “Influence of Eastern Pacific Hurricanes on the Southwest US Wildfire Environment,” Geophysical Research Letters (February 2024), John Wiley and Sons. doi: 10.1029/2023GL106774\n\nBurrows, A., T. Wang, D. Vartanyan, and M. S. B. Coleman. “A Theory for Neutron Star and Black Hole Kicks and Induced Spins,” The Astrophysical Journal (February 2024), IOP Publishing. doi: 10.3847/1538-4357/ad2353\n\nCheng, S., J.-L. Lin, M. Emani, S. Raskar, S. Foreman, Z. Xie, V. Vishwanath, and M. T. Kandemir. “Thorough Characterization and Analysis of Large Transformer Model Training At-Scale,” Proceedings of the ACM on Measurement and Analysis of Computing Systems (February 2024), ACM. doi: 10.1145/3639034\n\nGrošelj, D., H. Hakobyan, A. M. Beloborodov, L. Sironi, and A. Philippov. “Radiative Particle-in-Cell Simulations of Turbulent Comptonization in Magnetized Black-Hole Coronae,” Physical Review Letters (February 2024), APS. doi: 10.1103/PhysRevLett.132.085202\n\nHuang, J., S. Di, X. Yu, Y. Zhai, J. Liu, Y. Huang, K. J. Raffenetti, H. Zhou, K. Zhao, Z. Chen, F. Cappello, Y. Guo, and R. Thakur. “POSTER: Optimizing Collective Communications with Error-Bounded Lossy Compression for GPU Clusters,” PPoPP ‘24: Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (February 2024), Edinburgh, United Kingdom, ACM. doi: 10.1145/3627535.3638467\n\nKim, S. W., M. M. Elsayed, N. S. Nichols, T. Lakoba, J. Vanegas, C. Wexler, V. N. Kotov, and A. Del Maestro. “Atomically Thin Superfluid and Solid Phases for Atoms on Strained Graphene,” Physical Review B (February 2024), APS. doi: 10.1103/PhysRevB.109.064512\n\nLee, A. S., S. Elliott, H. Harb, L. Ward, I. Foster, L. Curtiss, and R. S. Assary. “Emin: A First-Principles Thermochemical Descriptor for Predicting Molecular Synthesizability,” Journal of Chemical Information and Modeling (February 2024), ACS Publications. doi: 10.1021/acs.jcim.3c01583\n\nMcCoy, A. E., M. A. Caprio, P. Maris, and P. J. Fasano. “Intruder Band Mixing in an Ab Initio Description of 12Be,” Physics Letters B (February 2024), Elsevier. doi: 10.1016/j.physletb.2024.138870\n\nMeißner, U.-G., S. Shen, S. Elhatisari, and D. Lee. “Ab Initio Calculation of the Alpha-Particle Monopole Transition Form Factor,” Physical Review Letters (February 2024), APS. doi: 10.1103/PhysRevLett.132.062501\n\nMerzari, E., V. C. Leite, J. Fang, D. Shaver, M. Min, S. Kerkemeier, P. Fischer, and A. Tomboulides. “Energy Exascale Computational Fluid Dynamics Simulations with the Spectral Element Method,” Journal of Fluids Engineering (February 2024), ASME. doi: 10.1115/1.4064659\n\nOhayon, B., A. Abeln, S. Bara, T. E. Cocolios, O. Eizenberg, A. Fleischmann, L. Gastaldo, C. Godinho, M. Heines, D. Hengstler, G. Hupin, P. Indelicato, K. Kirch, A. Knecht, D. Kreuzberger, J. Machado, P. Navratil, N. Paul, R. Pohl, D. Unger, S. M. Vogiatzi, K. von Schoeler, and F. Wauters. “Towards Precision Muonic X-Ray Measurements of Charge Radii of Light Nuclei,” Physics (February 2024), MDPI. doi: 10.3390/physics6010015\n\nPark, H., X. Yan, R. Zhu, E. A. Huerta, S. Chaudhuri, D. Cooper, I. Foster, and E. Tajkhorshid. “A Generative Artificial Intelligence Framework Based on a Molecular Diffusion Model for the Design of Metal-Organic Frameworks for Carbon Capture,” Communications Chemistry (February 2024), Springer Nature. doi: 10.1038/s42004-023-01090-2\n\nRoy, R. B., and D. Tiwari. “StarShip: Mitigating I/O Bottlenecks in Serverless Computing for Scientific Workflows,” Proceedings of the ACM on Measurement and Analysis of Computing Systems (February 2024), ACM. doi: 10.1145/3639028\n\nShin, H., P. Ganesh, P. R. C. Kent, A. Benali, A. Bhattacharya, H. N. Lee, O. Heinonen, and J. T. Krogel. “DFT+U and Quantum Monte Carlo Study of Electronic and Optical Properties of AgNiO2 and AgNi1−xCoxO2 Delafossite,” Physical Chemistry Chemical Physics (February 2024), Royal Society of Chemistry. doi: 10.1039/D3CP03477A\n\nSobczyk, J. E., B. Acharya, S. Bacca, and G. Hagen. “40Ca Transverse Response Function from Coupled-Cluster Theory,” Physical Review C (February 2024), APS. doi: 10.1103/PhysRevC.109.025502\n\nWang, T., and A. Burrows. “Nucleosynthetic Analysis of Three-Dimensional Core-Collapse Supernova Simulations,” The Astrophysical Journal (February 2024), IOP Publishing. doi: 10.3847/1538-4357/ad12b8\n\nMARCH\n\nAnnaberdiyev, A., P. Ganesh, and J. T. Krogel. “Enhanced Twist-Averaging Technique for Magnetic Metals: Applications Using Quantum Monte Carlo,” Journal of Chemical Theory and Computation (March 2024), ACS Publications. doi: 10.1021/acs.jctc.4c00058\n\nBhattarai, R., P. Minch, Y. Liang, S. Zhang, and T. D. Rhone. “Strain-Induced Topological Phase Transition in Ferromagnetic Janus Monolayer MnSbBiS2Te2,” Physical Chemistry Chemical Physics (March 2024), Royal Society of Chemistry. doi: 10.1039/D3CP05578G\n\nBurrows, A., T. Wang, and D. Vartanyan. “Physical Correlations and Predictions Emerging from Modern Core-Collapse Supernova Theory,” The Astrophysical Journal Letters (March 2024), IOP Publishing. doi: 10.3847/2041-8213/ad319e\n\nCarrillo-Perez, F., M. Pizurica, Y. Zheng, T. N. Nandi, R. Madduri, J. Shen, and O. Gevaert. “Generation of Synthetic Whole-Slide Image Tiles of Tumours from RNA-Sequencing Data via Cascaded Diffusion Models,” Nature Biomedical Engineering (March 2024), Springer Nature. doi: 10.1038/s41551-024-01193-8\n\nDominski, J., C. S. Chang, R. Hager, S. Ku, E. S. Yoon, and V. Pareil. “Neoclassical Transport of Tungsten Ion Bundles in Total-F Neoclassical Gyrokinetic Simulations of a Whole-Volume JET-Like Plasma,” Physics of Plasmas (March 2024), AIP Publishing. doi: 10.1063/5.0144509\n\nGao, X., A. D. Hanlon, S. Mukherjee, S. Petreczky, Q. Shi, S. Syritsyn, and Y. Zhao. “Transversity PDFs of the Proton from Lattice QCD with Physical Quark Masses,” Physical Review D (March 2024), APS. doi: 10.1103/PhysRevD.109.054506\n\nGnedin, N. Y. “Do Minihalos Affect Cosmic Reionization?,” The Astrophysical Journal (March 2024), IOP Publishing. doi: 10.3847/1538-4357/ad298e\n\nIsazawa, T., and J. M. Cole. “How Beneficial Is Pretraining on a Narrow Domain-Specific Corpus for Information Extraction about Photocatalytic Water Splitting?,” Journal of Chemical Information and Modeling (March 2024), ACS Publications. doi: 10.1021/acs.jcim.4c00063\n\nKanwar, G. A. Lovato, N. Rocco, and M. Wagman. “Mitigating Green’s Function Monte Carlo Signal-to-Noise Problems Using Contour Deformations,” Physical Review C (March 2024), APS. doi: 10.1103/PhysRevC.109.034317\n\nLavroff, R.H., J. Munarriz, C. E. Dickerson, F. Munoz, and A.N. Alexandrova. “Chemical Bonding Dictates Drastic Critical Temperature Difference in Two Seemingly Identical Superconductors,” PNAS (March 2024), National Academy of Sciences. doi: 10.1073/pnas.231610112\n\nLi, Z., S. He, P. Chaturvedi, V. Kindratenko, E. A. Huerta, K. Kim, and R. Madduri. “Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources: A Case Study on Federated Fine-Tuning of LLaMA 2,” Computing in Science and Engineering (March 2024), IEEE. doi: 10.1109/MCSE.2024.3382583\n\nMahadevan, V., D. Lenz, I. Grindeanu, and T. Peterka. “Accelerating Optimal Power Flow with GPUs: SIMD Abstraction of Nonlinear Programs and Condensed-Space Interior-Point Methods,” Journal of Computational Science (March 2024), Elsevier. doi: 10.1016/j.jocs.2024.102268\n\nPopov, E. L., N. J. Mecham, and I. A. Bolotnov. “Direct Numerical Simulation of Involute Channel Turbulence,” Journal of Fluids Engineering (March 2024), ASME. doi: 10.1115/1.4064496\n\nPoths, P., S. Vargas, P. Sautet, and A. N. Alexandrova. “Thermodynamic Equilibrium versus Kinetic Trapping: Thermalization of Cluster Catalyst Ensembles Can Extend Beyond Reaction Time Scales,” ACS Catalysis (March 2024), ACS Publications. doi: 10.1021/acscatal.3c06154\n\nQian, Q., and T. Mallick. “Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting,” ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (March 2024), Seoul, Republic of Korea, IEEE. doi: 10.1109/ICASSP48485.2024.10446847\n\nRojas, E., D. Pérez, and E. Meneses. “A Characterization of Soft-Error Sensitivity in Data-Parallel and Model-Parallel Distributed Deep Learning,” Journal of Parallel and Distributed Computing (March 2024), Elsevier. doi: 10.1016/j.jpdc.2024.104879\n\nShah, A. H., Z. Zhang, C. Wan, S. Wang, A. Zhang, L. Wang, A. N. Alexandrova, Y. Huang, and X. Duan. “Platinum Surface Water Orientation Dictates Hydrogen Evolution Reaction Kinetics in Alkaline Media,” Journal of the American Chemical Society (March 2024), ACS Publications. doi: 10.1021/jacs.3c12934\n\nTropiano, A. J., S. K. Bogner, R. J. Furnstahl, M. A. Hisham, A. Lovato, and R. B. Wiringa. “High-Resolution Momentum Distributions from Low-Resolution Wave Functions,” Physical Letters B (March 2024), Elsevier B.V. doi: 10.1016/j.physletb.2024.138591\n\nVan den Puttelaar, R., P. Nascimento de Lima, A. B. Knudsen, C. M. Rutter, K. M. Kuntz, L. de Jonge, F. A. Escudero, D. Lieberman, A. G. Zauber, A. I. Hahn, J. M. Inadomi, and I. Lasdorp-Vogelaar. “Effectiveness and Cost-Effectiveness of Colorectal Cancer Screening with a Blood Test That Meets the Centers for Medicare and Medicaid Services Coverage Decision,” Gastroentereology (March 2024), Elsevier. doi: 10.1053/j.gastro.2024.02.012\n\nVivas, A., C. E. Alvarez, J. M. Monsalve Diaz, E. Hernandez, J. G. Lalinde-Pulido, and H. Castro. “Expanding Horizons: Advancing HPC Education in Colombia through CyberColombia’s Summer Schools,” Journal of Computational Science Education (March 2024), Shodor. doi: 10.22369/issn.2153-4136/15/1/4\n\nWerner, G. R., and D. A. Uzdensky. “Electron and Proton Energization in 3D Reconnecting Current Sheets in Semirelativistic Plasma with Guide Magnetic Field,” The Astrophysical Journal Letters (March 2024), IOP Publishing. doi: 10.3847/2041-8213/ad2fa5\n\nAPRIL\n\nAnaya, J. J., N. S. Manavi, A. A. Tropina, A. Dogariu, R. B. Miles, and M. S. Grover. “Interferometry Analysis and CARS Measurements of Nonequilibrium in Hypersonic Oxygen/Argon and Pure Oxygen Flows,” Experiments in Fluids (April 2024), Springer Nature. doi: 10.1007/s00348-024-03804-1\n\nArndt, D., D. Leburn-Grandie, and C. Trott. “Experiences with Implementing Kokkos’ Backend,” IWOCL ‘24: Proceedings of the 12th International Workshop on OpenCL and SYCL (April 2024), ACM. doi: 10.1145/3648115.3648118\n\nBabcock, N. S., G. Montes-Cabrera, K. E. Oberhofer, M. Chergui, G. L. Celardo, and P. Kurian. “Ultraviolet Superradiance from Mega-Networks of Tryptophan in Biological Architectures,” The Journal of Physical Chemistry B (April 2024), ACS Publications. doi: 10.1021/acs.jpcb.3c07936\n\nBollweg, D., X. Gao, S. Mukherjee, and Y. Zhao. “Nonperturbative Collins-Soper Kernel from Chiral Quarks with Physical Masses,” Physics Letters B (April 2024), Elsevier. doi: 10.1016/j.physletb.2024.138617\n\nChang, C. S., S. Ku, R. Hager, J. Choi, D. Pugmire, S. Klasky, A. Loarte, and R. A. Pitts. “Role of Turbulent Separatrix Tangle in the Improvement of the Integrated Pedestal and Heat Exhaust Issue for Stationary-Operation Tokamak Fusion Reactors,” Nuclear Fusion (April 2024), IOP Publishing. doi: 10.1088/1741-4326/ad3b1e\n\nHolmen, J. K., M. García, A. Bagusetty, A. Sanderson, and M. Berzins. “Making Uintah Performance Portable for Department of Energy Exascale Testbeds,” Euro-Par 2023: Parallel Processing Workshops (April 2024), Springer Nature. doi: 10.1007/978-3-031-48803-0_10\n\nJiang, W. “Studying the Collective Functional Response of a Receptor in Alchemical Ligand Binding Free Energy Simulations with Accelerated Solvation Layer Dynamics,” Journal of Chemical Theory and Computation (April 2024), ACS Publications. doi: 10.1021/acs.jctc.4c00191\n\nKönig, K., J. C. Berengut, A. Borschevsky, A. Brinson, B. A. Brown, A. Dockery, S. Elhatisari, E. Eliav, R. F. Garcia Ruiz, J. D. Holt, B.-S. Hu, J. Karthein, D. Lee, Y.-Z. Ma, U.-G. Meißner, K. Minamisono, A. V. Oleynichenko, S. V. Pineda, S. D. Prosnyak, M. L. Reitsma, L. V. Skripnikov, A. Vernon, and A. Zaitsevskii. “Nuclear Charge Radii of Silicon Isotopes,” Physical Review Letters (April 2024), APS. doi: 10.1103/PhysRevLett.132.162502\n\nLauricella, G., M. M. Naderi, J. Zhou, I. Papautsky, and Z. Peng. “Bifurcation of Equilibrium Positions for Ellipsoidal Particles in Inertial Shear Flows Between Two Walls,” Journal of Fluid Mechanics (April 2024), Cambridge University Press. doi: 10.1017/jfm.2024.152\n\nLo, S. C. Y., J. W. S. McCullough, X. Xue, and P. V. Coveney. “Uncertainty Quantification of the Impact of Peripheral Arterial Disease on Abdominal Aortic Aneurysms in Blood Flow Simulations,” Journal of the Royal Society Interface (April 2024), The Royal Society. doi: 10.1098/rsif.2023.0656\n\nNichols, N. S., J. T. Childers, T. J. Burch, and L. Field. “Improving Performance Portability of the Procedurally Generated High Energy Physics Event Generator MadGraph Using SYCL,” IWOCL ‘24: Proceedings of the 12th International Workshop on OpenCL and SYCL (April 2024), Chicago, IL, ACM. doi: 10.1145/3648115.3648116\n\nPrakash, A., R. Balin, J. A. Evans, and K. E. Jansen. “A Streamline Coordinate Analysis of a Turbulent Boundary Layer Subject to Pressure Gradients and Curvature on the Windward Side of a Bump,” Journal of Fluid Mechanics (April 2024), Cambridge University Press. doi: 10.1017/jfm.2024.199\n\nSchwartz, J., Z. W. Di, Y. Jiang, J. Manassa, J. Pietryga, Y. Qian, M. G. Cho, J. L. Rowell, H. Zheng, R. D. Robinson, J. Gu, A. Kirlin, S. Rozeveld, P. Ercius, J. A. Fessler, T. Xu, M. Scott, and R. Hovden. “Imaging 3D Chemistry at 1 nm Resolution with Fused Multi-Modal Electron Tomography,” Nature Communications (April 2024), Springer Nature. doi: 10.1038/s41467-024-47558-0\n\nVarughese, B., S. Manna, T. D. Loeffler, R. Batra, M. J. Cherukara, and S. K. R. S. Sankaranarayanan. “Active and Transfer Learning of High-Dimensional Neural Network Potentials for Transition Metals,” ACS Applied Materials and Interfaces (April 2024), ACS. doi: 10.1021/acsami.3c15399\n\nYao, Y., A. Kononov, A. Metzlaff, A. Wucher, L. Kalkhoff, L. Breuer, M. Schleberger, and A. Schleife. “Nonequilibrium Dynamics of Electron Emission from Cold and Hot Graphene Under Proton Irradiation,” Nano Letters (April 2024), ACS Publications. doi: 10.1021/acs.nanolett.4c00356\n\nZhang, C., F. Gygi, and G. Galli. “Charge State and Entropic Effects Affecting the Formation and Dynamics of Divacancies in 3C-SiC,” Physical Review Materials (April 2024), APS. doi: 10.1103/PhysRevMaterials.8.046201\n\nMAY\n\nAbbott, R., A. Botev, D. Boyda, E. C. Hackett, G. Kanwar, S. Racanière, D. J. Rezende, F. Romero-López, P. E. Shanahan, and J. M. Urban. “Applications of Flow Models to the Generation of Correlated Lattice QCD Ensembles,” Physical Review D (May 2024), APS. doi: 10.1103/PhysRevD.109.094514\n\nFan, K., S. Petruzza, T. Gilray, and S. Kumar. “Configurable Algorithms for All-to-All Collectives,” ISC High Performance 2024 Research Paper Proceedings (39th International Conference) (May 2024), Hamburg, Germany, IEEE. doi: 10.23919/ISC.2024.10528936\n\nGao, X., W.-Y. Liu, and Y. Zhao. “Parton Distributions from Boosted Fields in the Coulomb Gauge,” Physical Review D (May 2024), APS. doi: 10.1103/PhysRevD.109.094506\n\nGrieder, A. C., K. Kim, L. F. Wan, J. Chapman, B. C. Wood, and N. Adelstein. “Effects of Nonequilibrium Atomic Structure on Ionic Diffusivity in LLZO: A Classical and Machine Learning Molecular Dynamics Study,” The Journal of Physical Chemistry C (May 2024), ACS Publications. doi: 10.1021/acs.jpcc.4c00171\n\nHowarth, T.L., M. A. Picciani, E. S. Richardson, M. S. Day, and A. J Aspden. “Direct Numerical Simulation of a High-Pressure Hydrogen Micromix Combustor: Flame Structure and Stabilisation Mechanism,” Combustion and Flame (May 2024), Elsevier. doi: 10.1016/j.combustflame.2024.113504\n\nKilic, O. O., T. Wang, M. Turilli, M. Titov, A. Merzky, L. Pouchard, and S. Jha. “Workflow Mini-Apps: Portable, Scalable, Tunable, and Faithful Representations of Scientific Workflows,” CCGRID 2024 (May 2024), Philadelphia, PA, IEEE. doi: 10.48550/arXiv.2403.18073\n\nLee, M. “Spectral Analysis of the Correlation Between Temperature and Wall-Normal Velocity in Rayleigh-Bénard Convection at Moderate Rayleigh Numbers,” Journal of Physics: Conference Series (May 2024), vol. 2573, IOP Publishing. doi: 10.1088/1742-6596/2753/1/012006\n\nLi, H., J. Duarte, A. Roy, R. Zhu, E. A. Huerta, D. Diaz, P. Harris, R. Kansal, D. S. Katz, I. H. Kavoori, V. V. Kindratenko, F. Mokhtar, M. S. Neubauer, S. E. M. Quinnan, R. Rusack, and Z. Zhao. “FAIR AI Models in High Energy Physics,” 26th International Conference on Computing in High Energy and Nuclear Physics (CHEP 2023) (May 2024), EDP Sciences. doi: 10.1051/epjconf/202429509017\n\nMa, X., F. Yan, L. Yang, I. Foster, M. E. Papka, Z. Liu, and R. Kettimuthu. “MalleTrain: Deep Neural Networks Training on Unfillable Supercomputer Nodes,” ICPE ‘24: Proceedings of the 15th ACM/SPEC International Conference on Performance Engineering (May 2024), New York, NY, ACM. doi: 10.1145/3629526.3645035\n\nMin, M., M. Brazell, A. Tomboulides, M. Churchfield, P. Fischer, and M. Sprague. “Towards Exascale for Wind Energy Simulations,” The International Journal of High Performance Computing Applications (May 2024), SAGE Publications. doi: 10.1177/10943420241252511\n\nNicholson, G. L., L. Duan, and N. J. Bisek. “Direct Numerical Simulation Database of High-Speed Flow over Parameterized Curved Walls,” AIAA Journal (May 2024), AIAA. doi: 10.2514/1.J063456\n\nNikitin, V., G. Wildenberg, A. Mittone, P. Shevchenko, A. Deriy, and F. De Carlo. “Laminography as a Tool for Imaging Large-Size Samples with High Resolution,” Journal of Synchrotron Radiation (May 2024), Wiley-Blackwell. doi: 10.1107/S1600577524002923\n\nReina, G., and S. Rizzi. “PGV 2024: Frontmatter,” EGPGV24: Eurographics Symposium on Parallel Graphics and Visualization (May 2024), Odense, Denmark, Eurographics. doi: 10.2312/pgv.20242012\n\nSun, Y., O. Sowunmi, R. Egele, S. H. K. Narayanan, L. van Roekel, and P. Balaprakash. “Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A Multiobjective Hyperparameter and Architecture Optimization Approach,” Mathematics (May 2024), MDPI. doi: 10.3390/math12101483\n\nTessier, F., V. Vishwanath, and E. Jeannot. “Adding Topology and Memory Awareness in Data Aggregation Algorithms,” Future Generation Computer Systems (May 2024), Elsevier. doi: 10.1016/j.future.2024.05.016\n\nTiwari, S., E. Kioupakis, J. Menendez, and F. Giustino. “Unified Theory of Optical Absorption and Luminescence Including Both Direct and Phonon-Assisted Processes,” Physical Review B (May 2024), APS. doi: 10.1103/PhysRevB.109.195127\n\nWang, R., S. Snyder, D. Benjamin, Z. Dong, P. Gartung, and K. Herner. “Darshan for HEP Applications,” 26th International Conference on Computing in High Energy and Nuclear Physics (CHEP 2023) (May 2024), Norfolk, VA, EDP Sciences. doi: 10.1051/epjconf/202429505003\n\nWang, X., S. Gao, Y. Luo, X. Liu, R. Tom, K. Zhao, V. Chang, and N. Marom. “Computational Discovery of Intermolecular Singlet Fission Materials Using Many-Body Perturbation Theory,” The Journal of Physical Chemistry C (May 2024), ACS Publications. doi: 10.1021/acs.jpcc.4c01340\n\nZhang, Z., W. Gee, P. Sautet, and A. N. Alexandrova. “H and CO Co-Induced Roughening of Cu Surface in CO2 Electroreduction Conditions,” Journal of the American Chemical Society (May 2024), ACS Publications. doi: 10.1021/jacs.4c03515\n\nJUNE\n\nAbdelfattah, A., N. Beams, R. Carson, P. Ghysels, T. Kolev, T. Stitt, A. Vargas, S. Tomov, and J. Dongarra. “MAGMA: Enabling Exascale Performance with Accelerated BLAS and LAPACK for Diverse GPU Architectures,” The International Journal of High Performance Computing Applications (June 2024), SAGE Publications. doi: 10.1177/10943420241261960\n\nDematties, D., S. Rizzi, and G. K. Thiruvathukal. “Containerization on a Self-Supervised Active Foveated Approach to Computer Vision,” Revista Colombiana de Computación (June 2024), UNAB. doi: 10.29375/25392115.5055\n\nDenny, J. E., S. Lee, P. Valero-Lara, M. Gonzalez-Tallada, K. Teranishi, and J. S. Vetter. “Clacc: OpenACC for C/C++ in Clang,” The International Journal of High Performance Computing Applications (June 2024), SAGE Publications. doi: 10.1177/10943420241261976\n\nHaberlie, A. M., B. Wallace, W. S. Ashley, V. A. Gensini, and A. C. Michaelis. “Mesoscale Convective System Activity in the United States Under Intermediate and Extreme Climate Change Scenarios,” Climatic Change (June 2024), Springer Nature. doi: 10.1007/s10584-024-03752-z\n\nHackett, D. C., D. A. Pefkou, and P. E. Shanahan. “Gravitational Form Factors of the Proton from Lattice QCD,” Physical Review Letters (June 2024), APS. doi: 10.1103/PhysRevLett.132.251904\n\nHammond, K. D., D. Maroudas, and B. D. Wirth. “Helium Aggregation and Surface Morphology Near Grain Boundaries in Plasma-Facing Tungsten,” Journal of Applied Physics (June 2024), AIP Publishing. doi: 10.1063/5.0200464\n\nHidayetoglu, M., S. Garcia de Gonzalo, E. Slaughter, Y. Li, C. Zimmer, T. Bicer, B. Ren, W. Gropp, W. M. Hwu, and A. Aiken. “CommBench: Micro-Benchmarking Hierarchical Networks with Multi-GPU, Multi-NIC Nodes,” ICS ‘24: Proceedings of the 38th ACM International Conference on Supercomputing (June 2024), Kyoto, Japan, ACM. doi: 10.1145/3650200.365659\n\nLee, M. “Direct Numerical Simulation of Magnetohydrodynamic Turbulent Flows,” 13th International Symposium on Turbulence and Shear Flow Phenomena (TSFP13) (June 2024), Montreal, Canada, TSFP Symposia.\n\nLiu, X., D. K. Hoang, Q. A. T. Nguyen, D. D. Phuc, S.-G. Kim, P. C. Nam, A. Kumar, F. Zhang, C. Zhi, and V. Q. Bui. “Advanced Dual-Atom Catalysts on Graphitic Carbon Nitride for Enhanced Hydrogen Evolution via Water Splitting,” Nanoscale (June 2024), Royal Society of Chemistry. doi: 10.1039/D4NR01241K\n\nMarrinan, T., E. Honzik, H. L. N. Brynteson, and M. E. Papka. “Image Synthesis from a Collection of Depth Enhanced Panoramas: Creating Interactive Extended Reality Experiences from Static Images,” IMX ‘24: Proceedings of the 2024 ACM International Conference on Interactive Media Experiences (June 2024), Stockholm, Sweden, ACM. doi: 10.1145/3639701.3656312\n\nNascimento de Lima, P., R. van den Puttelaar, A. B. Knudsen, A. I. Hahn, K. M. Kuntz, J. Ozik, N. Collier, F. Alarid-Escudero, A. G. Zauber, J. M. Inadomi, I. Lansdorp-Vogelaar, and C. M. Rutter. “Characteristics of a Cost-Effective Blood Test for Colorectal Cancer Screening,” Journal of the National Cancer Institute (June 2024), Oxford University Press. doi: 10.1093/jnci/djae124\n\nNúñez, G., H. Romero-Sandí, E. Rojas, and E. Meneses. “A Study of Pipeline Parallelism in Deep Neural Networks,” Revista Colombiana de Computación (June 2024), Universidad Autonoma de Bucaramanga. doi: 10.29375/25392115.5056\n\nPark, H., P. Patel, R. Haas, and E. A. Huerta. “APACE: AlphaFold2 and Advanced Computing as a Service for Accelerated Discovery in Biophysics,” PNAS (June 2024), National Academy of Sciences. doi: 10.1073/pnas.2311888121\n\nPineda-Antunez, C., C. Seguin, L. A. van Duuren, A. B. Knudsen, B. Davidi, P. Nascimento de Lima, C. Rutter, K. M. Kuntz, I. Lansdorp-Vogelaar, N. Colllier, J. Ozik, and F. Alarid-Escudero. “Emulator-Based Bayesian Calibration of the CISNET Colorectal Cancer Models,” Medical Decision Making (June 2024), Society for Medical Decision Making. doi: 10.1177/0272989X241255618\n\nRomero-Sandí, H., G. Núñez, and E. Rojas. “A Snapshot of Parallelism in Distributed Deep Learning Training,” Revista Colombiana de Computación (June 2024), Universidad Autonoma de Bucaramanga. doi: 10.29375/25392115.5054\n\nSalvadore, F., G. Rossi, S. Sathyanarayana, and M. Bernardini. “OpenMP Offload Toward the Exascale Using Intel® GPU Max 1550: Evaluation of STREAmS Compressible Solver,” The Journal of Supercomputing (June 2024), Springer Nature. doi: 10.1007/s11227-024-06254-y\n\nShevel, A. “Artificial Neural Networks in High Energy Physics Data Processing (Succinct Survey) and Probable Future Developments,” Physics of Particles and Nuclei (June 2024), Springer Nature . doi: 10.1134/S1063779624030742\n\nTian, M., E. A. Huerta, H. Zheng, and P. Kumar. “Physics-Inspired Spatiotemporal-Graph AI Ensemble for the Detection of Higher Order Wave Mode Signals of Spinning Binary Black Hole Mergers,” Machine Learning: Science and Technology (June 2024), IOP Publishing. doi: 10.1088/2632-2153/ad4c37\n\nXue, X., J. W. S. McCullough, S. C. Y. Lo, I. Zacharoudiou, B. Joó, and P. V. Coveney. “The Lattice Boltzmann Based Large Eddy Simulations for the Stenosis of the Aorta,” The International Conference on Computer Science 2024 (June 2024), Seattle, WA, Springer Nature. doi: 10.1007/978-3-031-63775-9_30\n\nJULY\n\nAnanthakrishnan, R., Y. Babuji, M. Baughman, J. Bryan, K. Chard, R. Chard, B. Clifford, I. Foster, D. S. Katz, K. H. Kesling, C. Janidlo, R. Mello, and L. Wang. “Enabling Remote Management of FaaS Endpoints with Globus Compute Multi-User Endpoints,” PEARC ‘24: Practice and Experience in Advanced Research Computing 2024: Human Powered Computing (July 2024), Providence, RI, ACM. doi: 10.1145/3626203.3670612\n\nBacchini, F., V. Zhdankin, E. A. Gorbunov, G. R. Werner, L. Arzamasskiy, M. C. Begelman, and D. A. Uzdensky. “Collisionless Magnetorotational Turbulence in Pair Plasmas: Steady-State Dynamics, Particle Acceleration, and Radiative Cooling,” Physical Review Letters (July 2024), APS. doi: 10.1103/PhysRevLett.133.045202\n\nBaker, E., D. Bollweg, P. Boyle, I. Cloët, X. Gao, S. Mukherjee, P. Petreczky, R. Zhang, and Y. Zhao. “Lattice QCD Calculation of the Pion Distribution Amplitude with Domain Wall Fermions at Physical Pion Mass,” Journal of High Energy Physics (July 2024), Springer Nature. doi: 10.1007/JHEP07(2024)211\n\nBarry, D., A. Danalis, and J. Dongarra, “Automated Data Analysis for Defining Performance Metrics from Raw Hardware Events,” 2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (July 2024), San Francisco, CA, IEEE. doi: 10.1109/IPDPSW63119.2024.00134\n\nBernholdt, D. E., G. Bosilca, A. Bouteiller, R. Brightwell, J. Ciesko, M. G. F. Dosanjh, G. Georgakoudis, I. Laguna, S. Levy, T. Naughton, S. L. Olivier, H. P. Pritchard, W. Schonbein, J. Schuchart, and A. Shehata. “Taking the MPI Standard and the Open MPI Library to Exascale,” The International Journal of High Performance Computing Applications (July 2024), SAGE Publications. doi: 10.1177/10943420241265936\n\nBouvier, T., B. Nicolae, A. Costan, T. Bicer, I. Foster, and G. Antoniu. “Efficient Distributed Continual Learning for Steering Experiments in Real-Time,” Future Generation Computer Systems (July 2024), Elsevier. doi: 10.1016/j.future.2024.07.016\n\nEmani, M., S. Foreman, V. Sastry, Z. Xie, S. Raskar, W. Arnold, R. Thakur, V. Vishwanath, M. E. Papka, S. Shanmugavelu, D. Gandhi, H. Zhao, D. Ma, K. Ranganath, R. Weisner, J. Chen, Y. Yang, N. Vassilieva, B. C. Zhang, S. Howland, and A. Tsyplikhin. “Toward a Holistic Performance Evaluation of Large Language Models Across Diverse AI Accelerators,” 2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (July 2024), San Francisco, CA, IEEE. doi: 10.1109/IPDPSW63119.2024.00016\n\nJarrah, I., P. Pal, M. Ameen, and M. M. Joly. “Wall-Resolved LES Study of Shaped-Hole Film Cooling Flow for Varying In-Hole Surface Roughness,” AIAA Aviation Forum and Ascend 2024 (July 2024), Las Vegas, NV, AIAA. doi: 10.2514/6.2024-4262\n\nJiang, W., J. Yasmin, J. Jones, N. Synovic, J. Kuo, N. Bielanski, Y. Tian, G. K. Thiruvathukal, and J. C. Davis. “PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software,” MSR ‘24: Proceedings of the 21st International Conference on Mining Software Repositories (July 2024), Lisbon, Portugal, ACM. doi: 10.1145/3643991.3644907\n\nKavroulakis, I., D. Papageorgiou, C. E. Frouzakis, P. Fischer, and A. Tomboulides. “Non-Conforming Schwarz-Spectral Element Method for Low Mach Number Reacting Flows,” Proceedings of the Combustion Institute (July 2024), Elsevier. doi: 10.1016/j.proci.2024.105506\n\nKlar, P. B., D. G. Waterman, T. Gruene, D. Mullick, Y. Song, J. B. Gilchrist, C. D. Owen, W. Wen, I. Biran, L. Houben, N. Regev-Rudzki, R. Dzikowski, N. Marom, L. Palatinus, P. Zhang, L. Leiserowitz, and M. Elbaum. “Cryo-Tomography and 3D Electron Diffraction Reveal the Polar Habit and Chiral Structure of the Malaria Pigment Crystal Hemozoin,” ACS Central Science (July 2024), ACS. doi: 10.1021/acscentsci.4c00162\n\nKrainova, N., H. M. Johnson, R. Lampande, S. Gao, N. Marom, B. P. Rand, and N. C. Giebink. “Vacuum Deposition of χ(2) Nonlinear Organic Single Crystal Films on Silicon,” Applied Physics Letters (July 2024), AIP Publishing. doi: 10.1063/5.0207675\n\nLim, C., M.-S. Kim, D. C. Lim, S. Kim, Y. Lee, J. Cha, G. Lee, S. Y. Song, D. Thapa, J. D. Denlinger, S.-G. Kim, S. W. Kim, J. Seo, and Y. Kim. “Topological Fermi-Arc Surface State Covered by Floating Electrons on a Two-Dimensional Electride,” Nature Communications (July 2024), Springer Nature. doi: 10.1038/s41467-024-49841-6\n\nNilsson, V., A. Samaddar, S. Madireddy, and P. Nyquist. “REMEDI: Corrective Transformations for Improved Neural Entropy Estimation,” Proceedings of the 41st International Conference on Machine Learning (July 2024), vol. 235, Vienna, AT, PMLR\n\nPalumbo, D. C. M., M. Bauböck, and C. F. Gammie. “Multifrequency Analysis of Favored Models for the Messier 87* Accretion Flow,” The Astrophysical Journal (July 2024), IOP Publishing. doi: 10.3847/1538-4357/ad5fed\n\nPhelps, C., A. Lahiry, T. Z. Islam, and L. C. Pouchard. “Reimagine Application Performance as a Graph: Novel Graph-Based Method for Performance Anomaly Classification in High-Performance Computing,” 2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC) (July 2024), Osaka, Japan, IEEE. doi: 10.1109/COMPSAC61105.2024.00041\n\nRahman, M. H., S. Di, S. Guo, X. Lu, G. Li, and F. Cappello. “Druto: Upper-Bounding Silent Data Corruption Vulnerability in GPU Applications,” 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (July 2024), San Fransisco, CA, IEEE. doi: 10.1109/IPDPS57955.2024.00058\n\nRazahk, T. M., T. Linker, Y. Luo, R. K. Kalia, K.-I. Nomura, P. Vashishta, and A. Nakano. “Accelerating Quantum Light-Matter Dynamics on Graphics Processing Units,” 2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (July 2024), San Francisco, CA, IEEE. doi: 10.1109/IPDPSW63119.2024.00176\n\nRobinson, J. P., K. Fan, S. Petruzza, T. Gilray, and S. Kumar. “Investigating Data Movement Strategies for Distribution of Repartitioned Data,” PEARC ‘24: Practice and Experience in Advanced Research Computing 2024: Human Powered Computing (July 2024), Providence, RI, ACM. doi: 10.1145/3626203.3670534\n\nRuggeri, M., B. L. Bemis, M. C. Sieve, C. L. Running, J. L. Hill, M. P. Borg, K. Jantze, and C. Scalo. “Hypersonic Boundary Layer Treatment with Wavy Wall,” AIAA Aviation Forum and Ascend 2024 (July 2024), Las Vegas, NV, AIAA. doi: 10.2514/6.2024-3835\n\nShen, R., A.-L. R. Brownless, N. Alansson, M. Corbella, S. C. L. Kamerlin, and A. C. Hengge. “SHP-1 Variants Broaden the Understanding of pH-Dependent Activities in Protein Tyrosine Phosphatases,” JACS Au (July 2024), ACS Publications. doi: 10.1021/jacsau.4c00078\n\nShi, Q., H. T. Ding, X. Gao, S. Mukherjee, P. Petreczky, S. Syritsyn, and Y. Zhao. “Lattice QCD Calculation of the Pion Generalized Parton Distribution,” 25th International Symposium on Spin Physics (SPIN2023) (July 2024), Durham, NC, Sissa Medialab. doi: 10.22323/1.456.0024\n\nVasan, A., O. Gokdemir, A. Brace, A. Ramanathan, T. Brettin, R. Stevens, and V. Vishwanath. “High Performance Binding Affinity Prediction with a Transformer-Based Surrogate Model,” 2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW) (July 2024), San Francisco, CA, IEEE. doi: 10.1109/IPDPSW63119.2024.00114\n\nWang, T., and A. Burrows. “Supernova Explosions of the Lowest-Mass Massive Star Progenitors,” The Astrophysical Journal (July 2024), IOP Publishing. doi: 10.3847/1538-4357/ad5009\n\nXie, Z., M. Emani, X. Yu, D. Tao, X. He, P. Su, K. Zhou, and V. Vishwanath. “Centimani: Enabling Fast AI Accelerator Selection for DNN Training with a Novel Performance Predictor,” USENIX ATC’24: Proceedings of the 2024 USENIX Conference on Usenix Annual Technical Conference (July 2024), USENIX Association. doi: 10.5555/3691992.3692065\n\nYuwono, S. H., T. Zhang, K. A. Surjuse, E. F. Valeev, X. Li, and A. E. DePrince III. “Relativistic Coupled Cluster with Completely Renormalized and Perturbative Triples Corrections,” The Journal of Physical Chemistry A (July 2024), ACS Publications. doi: 10.1021/acs.jpca.4c02583\n\nZhang, C. “Comet: Development of an Unstructured Mesh Direct Simulation Monte Carlo Code on GPUs,” AIAA Aviation Forum and Ascend 2024 (July 2024), Las Vegas, NV, AIAA. doi: 10.2514/6.2024-3951\n\nZhang, D., M. S. Raj, B. Xie, S. Di, and D. Dai. “Cross-System Analysis of Job Characterization and Scheduling in Large-Scale Computing Clusters,” 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS) (July 2024), San Francisco, CA, IEEE. doi: 10.1109/IPDPS57955.2024.00069\n\nAUGUST\n\nAhn, J., S.-H. Kang, M. Yoon, and J. T. Krogel. “Exploring Interlayer Coupling in the Twisted Bilayer PtTe2,” Physical Review Research (August 2024), APS. doi: 10.1103/PhysRevResearch.6.033177\n\nAlkan, M., B. Q. Pham, D. Del Angel Cruz, J. R. Hammond, T. A. Barnes, and M. S. Gordon. “LibERI—A Portable and Performant Multi-GPU Accelerated Library for Electron Repulsion Integrals via OpenMP Offloading and Standard Language Parallelism,” The Journal of Chemical Physics (August 2024), AIP Publishing. doi: 10.1063/5.0215352\n\nAdhianto, L., J. Anderson, R. M. Barnett, D. Grbic, V. Indic, M. Krentel, Y. Liu, S. Milaković, W. Phan, and J. Mellor-Crummey. “Refining HPCToolkit for Application Performance Analysis at Exascale,” The International Journal of High Performance Computing Applications (August 2024), SAGE Publications. doi: 10.1177/10943420241277839\n\nAntepara, O., T. Zhao, B. Sepanski, P. Basu, H. Johansen, M. Hall, and S. Williams. “Bricks: A High-Performance Portability Layer for Computations on Block-Structured Grids,” The International Journal of High Performance Computing Applications (August 2024), SAGE Publications. doi: 10.1177/10943420241268288\n\nAulakh, D. J. S., X. Yang, and R. Maulik. “Robust Experimental Data Assimilation for the Spalart-Allmaras Turbulence Model,” Physical Review Fluids (August 2024), APS. doi: 10.1103/PhysRevFluids.9.084608\n\nChitty-Venkata, K. T., V. K. Sastry, M. Emani, V. Vishwanath, S. Shanmugavelu, and S. Howland. “WActiGrad: Structured Pruning for Efficient Finetuning and Inference of Large Language Models on AI Accelerators,” Euro-Par 2024: Parallel Processing (August 2024), Springer Nature. doi: 10.1007/978-3-031-69766-1_22\n\nDanciu, B. A., G. K. Giannakopoulos, M. Bode, and C. E. Frouzakis. “Multi-Cycle Direct Numerical Simulations of a Laboratory Scale Engine: Evolution of Boundary Layers and Wall Heat Flux,” Flow, Turbulence and Combustion (August 2024), Springer Nature. doi: 10.1007/s10494-024-00576-w\n\nGao, X., J. He, R. Zhang, and Y. Zhao. “Systematic Uncertainties from Gribov Copies in Lattice Calculation of Parton Distributions in the Coulomb Gauge,” Chinese Physics Letters (August 2024), IOP Publishing. doi: 10.1088/0256-307X/41/12/121201\n\nGensini, V. A., W. S. Ashley, A. C. Michaelis, A. M. Haberlie, J. Goodin, and B. C. Wallace. “Hailstone Size Dichotomy in a Warming Climate,” npj Climate and Atmospheric Science (August 2024), Springer Nature. doi: 10.1038/s41612-024-00728-9\n\nHo, T. H. Q. A. T. Nguyen, B.-T. Truong Le, S.-G. Kim, and V. Q. Bui. “Unlocking Electrocatalytic Dynamics with Anti-MXene Borides Monolayers for Nitrate Reduction,” Applied Surface Science (August 2024), Elsevier. doi: 10.1016/j.apsusc.2024.160908\n\nHupfeld, E., S. Schlee, J. P. Wurm, C. Rajendran, D. Yehorova, E. Vos, D. R. Raju, S. C. L. Kamerlin, R. Sprangers, and R. Sterner. “Conformational Modulation of a Mobile Loop Controls Catalysis in the (βα)8-Barrel Enzyme of Histidine Biosynthesis HisF,” JACS Au (August 2024), ACS Publications. doi: 10.1021/jacsau.4c00558\n\nKim, K., S. Yuan, A. M. Dive, A. Grieder, N. Adelstein, S. Kang, B. C. Wood, and L. Wan. “Probing Interfacial Degradation in Solid-State Batteries Using Machine Learning Force Fields,” ECS Meeting Abstracts (August 2024), The Electrochemical Society. doi: 10.1149/MA2024-012221mtgabs\n\nMarrinan, T., V. A. Mateevitsi, M. Moeller, A. Kanayinkal, and M. E. Papka. “2023 IEEE Scientific Visualization Contest Winner: VisAnywhere: Developing Multiplatform Scientific Visualization Applications,” IEEE Computer Graphics and Applications (August 2024), IEEE. doi: 10.1109/MCG.2024.3444460\n\nMaurya, A., R. Underwood, M. M. Rafique, F. Cappello, and B. Nicolae. “DataStates-LLM: Lazy Asynchronous Checkpointing for Large Language Models,” HPDC ‘24: Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing (August 2024), New York, NY, IEEE. doi: 10.1145/3625549.3658685\n\nMoreland, K., T. M. Athawale, V. Bolea, M. Bolstad, E. Brugger, H. Childs, A. Huebl, L.-T. Lo, B. Geveci, N. Marsaglia, S. Philip, D. Pugmire, S. Rizzi, Z. Wang, and A. Yenpure. “Visualization at Exascale: Making It All Work with VTK-m,” The International Journal of High Performance Computing Applications (August 2024), SAGE Publications. doi: 10.1177/10943420241270969\n\nMuñoz-Muñoz, D., F. Garcia-Carballeira, D. Camarmas-Alonso, A. Calderon-Mateos, and J. Carretero. “Fault Tolerant in the Expand Ad-Hoc Parallel File System,” Euro-Par 2024: Parallel Processing (August 2024), Madrid, Spain, Springer Nature. doi: 10.1007/978-3-031-69766-1_5\n\nNguyen, Q. A. T., T. H. Ho, S.-G. Kim, A. Kumar, and V. Q. Bui. “Orbital-Engineered Anomalous Hall Conductivity in Stable Full Heusler Compounds: A Pathway to Optimized Spintronics,” Journal of Materials Chemistry C (August 2024), Royal Society of Chemistry. doi: 10.1039/D4TC02116A\n\nPatwa, H., N. S. Babcock, and P. Kurian. “Quantum-Enhanced Photoprotection in Neuroprotein Architectures Emerges from Collective Light-Matter Interactions,” Frontiers in Physics (August 2024), Frontiers Media SA. doi: 10.3389/fphy.2024.1387271\n\nShin, H., K. Gasperich, T. Rojas, A. T. Ngo, J. T. Krogel, and A. Benali. “Systematic Improvement of Quantum Monte Carlo Calculations in Transition Metal Oxides: sCI-Driven Wavefunction Optimization for Reliable Band Gap Prediction,” Journal of Chemical Theory and Computation (August 2024), ACS Publications. doi: 10.1021/acs.jctc.4c00335\n\nSu, C.-C., Z. Zhang, M. Lyu, M. Cui, and E. W. Yu. “Cryo-EM Structures of the Human Band 3 Transporter Indicate a Transport Mechanism Involving the Coupled Movement of Chloride and Bicarbonate Ions,” PLOS Biology (August 2024), Public Library of Science. doi: 10.1371/journal.pbio.3002719\n\nUnderwood, R., M. Madhyastha, R. Burns, and B. Nicolae. “EvoStore: Towards Scalable Storage of Evolving Learning Models,” HPDC ‘24: Proceedings of the 33rd International Symposium on High-Performance Parallel and Distributed Computing (August 2024), New York, NY, ACM. doi: 10.1145/3625549.3658679\n\nWillman, J. T., J. M. Gonzalez, K. Nguyen-Cong, S. Hamel, V. Lordi, and I. I. Oleynik. “Accuracy, Transferability, and Computational Efficiency of Interatomic Potentials for Simulations of Carbon Under Extreme Conditions,” The Journal of Chemical Physics (August 2024), AIP Publishing. doi: 10.1063/5.0218705\n\nWu, Q., J. A. Insley, V. A. Mateevitsi, S. Rizzi, M. E. Papka, and K.-L. Ma. “Distributed Neural Representation for Reactive In Situ Visualization,” IEEE Transactions on Visualization and Computer Graphics (August 2024), IEEE. doi: 10.1109/TVCG.2024.3432710\n\nZuo, W., J. Gim, T. Li, D. Hou, Y. Gao, S. Zhou, C. Zhao, X. Jia, Z. Yang, Y. Liu, X. Xiao, G. L. Xu, and K. Amine. “Microstrain Screening Towards Defect-less Layered Transition Metal Oxide Cathodes,” Nature Nanotechnology (August 2024), Springer Nature. doi: 10.1038/s41565-024-01734-x\n\nSEPTEMBER\n\nBollweg, D., H.-T. Ding, J. Goswami, F. Karsch, S. Mukherjee, P. Petreczky, and C. Schmidt. “Strangeness-Correlations on the Pseudocritical Line in (2+1)-Flavor QCD,” Physical Review D (September 2024), APS. doi: 10.1103/PhysRevD.110.054519\n\nBrower, R. C., E. Owen, C. Rebbi, C. Culver, D. Schaich, K. K. Cushman, G. T. Fleming, A. Gasbarro, A. Hasenfratz, E. T. Neil, J. Ingoldby, X. Y. Jin, J. C. Osborn, E. Rinaldi, P. Vranas, E. Weinberg, and O. Witzel. “Light Scalar Meson and Decay Constant in SU(3) Gauge Theory with Eight Dynamical Flavors,” Physical Review D (September 2024), APS. doi: 10.1103/PhysRevD.110.054501\n\nEgele, R., F. Mohr, T. Viering, and P. Balaprakash. “The Unreasonable Effectiveness of Early Discarding After One Epoch in Neural Network Hyperparameter Optimization,” Neurocomputing (September 2024), Elsevier. doi: 10.1016/j.neucom.2024.127964\n\nHuang, J., B. Peng, C. Zhu, M. Xu, Y. Liu, Z. Liu, J. Zhou, S. Wang, X. Duan, H. Heinz, and Y. Huang. “Surface Molecular Pump Enables Ultrahigh Catalyst Activity,” Science Advances (September 2024), AAAS. doi: 10.1126/sciadv.ado3942\n\nJarmusch, A., F. Cabarcas, S. Pophale, A. Kallai, J. Doerfert, L. Peyralans, S. Lee, J. Denny, and S. Chandrasekaran. “CI/CD Efforts for Validation, Verification and Benchmarking OpenMP Implementations,” IWOMP 2024: Advancing OpenMP for Future Accelerators (September 2024), Perth, WA, Australia, Springer Nature. doi: 10.1007/978-3-031-72567-8_8\n\nLoeffler, H. H., S. Wan, M. Klähn, A. P. Bhati, and P. V. Coveney. “Optimal Molecular Design: Generative Active Learning Combining REINVENT with Precise Binding Free Energy Ranking Simulations,” Journal of Chemical Theory and Computation (September 2024), ACS Publications. doi: 10.1021/acs.jctc.4c00576\n\nLusczek, P., A. Castaldo, Y. M. Tsai, D. Mishler, and J. Dongarra. “Numerical Eigen-Spectrum Slicing, Accurate Orthogonal Eigen-Basis, and Mixed-Precision Eigenvalue Refinement Using OpenMP Data-Dependent Tasks and Accelerator Offload,” The International Journal of High Performance Computing Applications (September 2024), Sage. doi: 10.1177/109434202412810\n\nMadireddy, S., C. Akçay, S. E. Kruger, T. Bechtel Amara, X. Sun, J. McClenaghan, J. Koo, A. Samaddar, Y. Liu, P. Palaprakash, and L. L. Lao. “EFIT-Prime: Probabilistic and Physics-Constrained Reduced-Order Neural Network Model for Equilibrium Reconstruction in DIII-D,” Physics of Plasmas (September 2024), AIP Publishing. doi: 10.1063/5.0213609\n\nQiao, F., T. A. Binkowski, I. Broughan, W. Chen, A. Natarajan, G. E. Schlitz, K. A. Scheidt, W. F. Anderson, and R. Bergan. “Protein Structure Inspired Discovery of a Novel Inducer of Anoikis in Human Melanoma,” Cancers (September 2024), MDPI. doi: 10.3390/cancers16183177\n\nSampathkumar, V., K. P. Koster, B. J. Carroll, S. Murray, Sherman, and N. Kasthuri. “Synaptic Integration of Somatosensory and Motor Cortical Inputs onto Spiny Projection Neurons of Mice Caudoputamen,” European Journal of Neuroscience (September 2024), John Wiley and Sons. doi: 10.1111/ejn.16538\n\nSudharsan, S., and A. Sharma. “Criteria for Dynamic Stall Onset and Vortex Shedding in Low-Reynolds-Number Flows,” Journal of Fluid Mechanics (September 2024), Cambridge University Press. doi: 10.1017/jfm.2024.753\n\nThavappiragasam, M., J. A. Harris, E. Endeve, and B. Videau. “Performance Porting the ExaStar Multi-Physics App Thornado on Heterogeneous Systems—A Fortran-OpenMP Code-Base Evaluation,” IWOMP 2024: Advancing OpenMP for Future Accelerators (September 2024), Perth, WA, Australia, Springer Nature. doi: 10.1007/978-3-031-72567-8_2\n\nTian, M., L. Gao, D. Zhang, X. Chen, C. Fan, X. Guo, R. Haas, P. Ji, K. Krongchon, Y. Li, S. Liu, D. Luo, Y. Ma, H. Tong, K. Trinh, C. Tian, Z. Wang, B. Wu, Y. Xiong, S. Yin, M. Zhu, K. Lieret, Y. Lu, G. Liu, Y. Du, T. Tao, Ofir Press, J. Callan, E. Huerta, and H. Peng. “SciCode: A Research Coding Benchmark Curated by Scientists,” NeurIPS 2024 (September 2024), OpenReview.\n\nUpadhyay, S., A. Benali, and K. D. Jordan. “Capturing Correlation Effects in Positron Binding to Atoms and Molecules,” Journal of Chemical Theory and Computation (September 2024), ACS Publications. doi: 10.1021/acs.jctc.4c00727\n\nWard, L., B. Blaiszik, C.-W. Lee, T. Martin, I. Foster, and A. Schleife. “Accelerating Multiscale Electronic Stopping Power Predictions with Time-Dependent Density Functional Theory and Machine Learning,” npj Computational Materials (September 2024), Springer Nature. doi: 10.1038/s41524-024-01374-8\n\nWinetrout, J. J., K. Kanhaiya, J. Kemppainen, P. J. in ‘t Veld, G. Sachdeva, R. Pandey, B. Damirchi, A. van Duin, G. M. Odegard, and H. Heinz. “Implementing Reactivity in Molecular Dynamics Simulations with Harmonic Force Fields,” Nature Communications (September 2024), Springer Nature. doi: 10.1038/s41467-024-50793-0\n\nZheng, W., J. Kordas, T. J. Skluzacek, R. Kettimuthu, and I. Foster. “Globus Service Enhancements for Exascale Applications and Facilities,” The International Journal of High Performance Computing Applications (September 2024), Sage. doi: 10.1177/10943420241281744\n\nOCTOBER\n\nAhrens, J., M. Arienti, U. Ayachit, J. Bennett, R. Binyahib, A. Biswas, P.-T. Bremer, E. Brugger, R. Bujack, H. Carr, J. Chen, H. Childs, S. Dutta, A. Essiari, B. Geveci, C. Harrison, S. Hazarika, M. H. Fulp, P. Hristov, X. Huang, J. Insley, Y. Kawakami, C. Keilers, J. Kress, M. Larsen, D. Lipsa, M. Majumder, N. Marsaglia, V. A. Mateevitsi, V. Pascucci, J. Patchett, S. Patel, S. Petruzza, D. Pugmire, S. Rizzi, D. H. Rogers, O. Rübel, J. Salinas, S. Sane, S. Shudler, A. Stewart, K. Tsai, T. L. Turton, W. Usher, Z. Wang, G. H. Weber, C. Wetterer-Nelson, J. Woodring, and A. Yenpure. “The ECP ALPINE Project: In Situ and Post Hoc Visualization Infrastructure and Analysis Capabilities for Exascale,” The International Journal of High Performance Computing Applications (October 2024), SAGE Publications. doi: 10.1177/10943420241286521\n\nBertrand, A., A. Lewis, J. Camps, V. Grau, and B. Rodriguez. “Multi-Modal Characterisation of Early-Stage, Subclinical Cardiac Deterioration in Patients with Type 2 Diabetes,” Cardiovascular Diabetology (October 2024), Springer Nature. doi: 10.1186/s12933-024-02465-y\n\nBhattarai, R., and T. D. Rhone. “Exploring Bonding Configurations in MnBi2Te4-Type Materials,” ACS Applied Materials and Interfaces (October 2024), ACS Publications. doi: 10.1021/acsami.4c12946\n\nBouvier, T., B. Nicolae, H. Chaugier, A. Costan, I. Foster, and G. Antoniu. “Efficient Data-Parallel Continual Learning with Asynchronous Distributed Rehearsal Buffers,” 2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (October 2024), Philadelphia, PA, IEEE. doi: 10.1109/CCGrid59990.2024.00036\n\nBreitenfeld, M. S., H. Tang, H. Zheng, J. Henderson, and S. Byna. “HDF5 in the Exascale Era: Delivering Efficient and Scalable Parallel I/O for Exascale Applications,” The International Journal of High Performance Computing Applications (October 2024), Sage. doi: 10.1177/10943420241288244\n\nCheng, D., K.-L. Nguyen, M. Heyde, B. R. Cuenya, A. N. Alexandrova, and P. Sautet. “Structure Sensitivity and Catalyst Restructuring for CO2 Electro-Reduction on Copper,” ECS Meeting Abstracts (October 2024), The Electrochemical Society. doi: 10.1149/MA2024-02624231mtgabs\n\nChoi, L., A. Burrows, and D. Vartanyan. “Gravitational-Wave and Gravitational-Wave Memory Signatures of Core-Collapse Supernovae,” The Astrophysical Journal (October 2024), IOP Publishing. doi: 10.3847/1538-4357/ad74f8\n\nCui, M., Y. Lu, M. Mezei, and D. E. Logothetis. “Molecular Dynamics (MD) Simulations Provide Insights into the Activation Mechanisms of 5-HT2A Receptors,” Molecules (October 2024), MDPI. doi: 10.3390/molecules29204935\n\nCui, M., Y. Lu, X. Ma, and D. E. Logothetis. “Molecular Mechanism of GIRK2 Channel Gating Modulated by Cholesteryl Hemisuccinate,” Frontiers in Physiology (October 2024), Frontiers Media SA. doi: 10.3389/fphys.2024.1486362\n\nDing, H.-T., X. Gao, A. D. Hanlon, S. Mukherjee, P. Petreczky, Q. Shi, S. Syritsyn, R. Zhang, and Y. Zhao. “QCD Predictions for Meson Electromagnetic Form Factors at High Momenta: Testing Factorization in Exclusive Processes,” Physical Review Letters (October 2024), APS. doi: 10.1103/PhysRevLett.133.181902\n\nHong, I., H. Bae, J. Ahn, H. Shin, H. Lee, and Y. Kwon. “Design of Biphenylene-Derived Tunable Dirac Materials,” FlatChem (October 2024), Elsevier. doi: 10.1016/j.flatc.2024.100760\n\nLiu, Q., J. Xu, W. Jiang, J. Gim, A. P. Tornheim, R. Pathak, Q. Zhu, P. Zuo, Z. Yang, K. Z. Pupek, E. Lee, C. Wang, C. Liu, J. R. Croy, K. Xu, and Z. Zhang. “High-Energy LiNiO2 Li Metal Batteries Enabled by Hybrid Electrolyte Consisting of Ionic Liquid and Weakly Solvating Fluorinated Ether,” Advanced Science (October 2024), John Wiley and Sons. doi: 10.1002/advs.202409662\n\nLiu, Y., F. Alsafadi, T. Mui, D. O’Grady, and R. Hu. “Development of Whole System Digital Twins for Advanced Reactors: Leveraging Graph Neural Networks and SAM Simulations,” Nuclear Technology (October 2024), Taylor and Francis. doi: 10.1080/00295450.2024.2385214\n\nMaharjan, R., Z. Zhang, P. A. Klenotic, W. D. Gregor, M. L. Tringides, M. Cui, G. E. Purdy, and E. W. Yu. “Structures of the Mycobacterial MmpL4 and MmpL5 Transporters Provide Insights into Their Role in Siderophore Export and Iron Acquisition,” PLOS Biology (October 2024), Public Library of Science. doi: 10.1371/journal.pbio.3002874\n\nNgom, M., C. Graziani, and N. Paulson. “Atomic Layer Deposition Optimization via Targeted Adaptive Design,” 38th Conference on Neural Information Processing Systems (October 2024), OpenReview.\n\nSamaddar, A., S. K. Ravi, N. Ramachandra, L. Luan, S. Madireddy, A. Bhaduri, P. Pandita, C. Sun, and L. Wang. “Data-Efficient Dimensionality Reduction and Surrogate Modeling of High-Dimensional Stress Fields,” Journal of Mechanical Design (October 2024), ASME. doi: 10.1115/1.4066224\n\nShilpika, S., B. Lusch, M. Emani, F. Simini, V. Vishwanath, M. E. Papka, and K.-L. Ma. “A Multi-Level, Multi-Scale Visual Analytics Approach to Assessment of Multifidelity HPC Systems,” 2024 IEEE 24th International Symposium on Cluster, Cloud and Internet Computing (CCGrid) (October 2024), Philadelphia, PA, IEEE. doi: 10.1109/CCGrid59990.2024.00060\n\nTramm, J., P. Romano, P. Shriwise, A. Lund, J. Doerfert, P. Steinbrecher, A. Siegel, and G. Ridley. “Performance Portable Monte Carlo Particle Transport on Intel, NVIDIA, and AMD GPUs,” Joint International Conference on Supercomputing in Nuclear Applications + Monte Carlo (SNA + MC 2024) (October 2024), Paris, France, EDP Sciences. doi: 10.1051/epjconf/202430204010\n\nWang, T., and A. Burrows. “Insights into the Production of 44Ti and Nickel Isotopes in Core-Collapse Supernovae,” The Astrophysical Journal (October 2024), IOP Publishing. doi: 10.3847/1538-4357/ad6983\n\nWard, L., J. G. Pauloski, V. Hayot-Sasson, Y. Babuji, A. Brace, R. Chard, K. Chard, R. Thakur, and I. Foster. “Employing Artificial Intelligence to Steer Exascale Workflows with Colmena,” The International Journal of High Performance Computing Applications (October 2024), SAGE Publications. doi: 10.1177/10943420241288242\n\nWu, X., P. Balaprakash, M. Kruse, J. Koo, B. Videau, P. Hovland, V. Taylor, B. Geltz, S. Jana, and M. Hall. “ytopt: Autotuning Scientific Applications for Energy Efficiency at Large Scales,” Concurrency and Computation: Practice and Experience (October 2024), John Wiley and Sons. doi: 10.1002/cpe.8322\n\nWu, X., J. R. Tramm, J. Larson, J.-L. Navarro, P. Balaprakash, B. Videau, M. Kruse, P. Hovland, V. Taylor, and M. Hall. “Integrating ytopt and libEnsemble to Autotune OpenMC,” The International Journal of High Performance Computing Applications (October 2024), SAGE Publications. doi: 10.1177/10943420241286476\n\nNOVEMBER\n\nAbbott, R., M. S. Albergo, D. Boyda, D. Hackett, G. Kanwar, F. Romero-Lopez, P. Shanahan, and J. Urban. “Multiscale Normalizing Flows for Gauge Theories,” The 40th International Symposium on Lattice Field Theory (LATTICE2023) (November 2024), Batavia, IL, Sissa Medialab. doi: https://doi.org/10.22323/1.453.0035\n\nAbbott, R., D. Boyda, D. Hackett, G. Kanwar, F. Romero-Lopez, P. Shanahan, J. Urban, and M. Albergo. “Practical Applications of Machine-Learned Flows on Gauge Fields,” The 40th International Symposium on Lattice Field Theory (LATTICE2023) (November 2024), Batavia, IL, Sissa Medialab. doi: 10.22323/1.453.0011\n\nAdeniji, I. A., J. A. Insley, D. Joiner, V. A. Mateevitsi, M. E. Papka, and S. Rizzi. “Exploring Large-Scale Scientific Data in Virtual Reality,” 2024 IEEE 14th Symposium on Large Data Analysis and Visualization (LDAV) (November 2024), St Pete Beach, FL, IEEE. doi: 10.1109/LDAV64567.2024.00019\n\nBarwey, S., R. Balin, B. Lusch, S. Patel, R. Balakrishnan, P. Pal, R. Maulik, and V. Vishwanath. “Scalable and Consistent Graph Neural Networks for Distributed Mesh-Based Data-Driven Modeling,” SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (November 2024), IEEE. doi: 10.1109/SCW63240.2024.00146\n\nBilondi, A. M., N. Scapin, L. Brandt, and P. Mirbod. “Turbulent Convection in Emulsions: The Rayleigh–Bénard Configuration,” Journal of Fluid Mechanics (November 2024), Cambridge University Press. doi: 10.1017/jfm.2024.765\n\nBrower, R. C., C. Culver, K. K. Cushman, G. T. Fleming, A. Hasenfratz, D. Howarth, J. Ingoldby, X. Y. Jin, G. D. Kribs, A. S. Meyer, E. T. Neil, J. C. Osborn, E. Owen, S. Park, C. Rebbi, E. Rinaldi, D. Schaich, P. Vranas, E. Weinberg, and O. Witzel. “Stealth Dark Matter Spectrum Using Laplacian Heaviside Smearing and Irreducible Representations,” Physical Review D (November 2024), APS. doi: 10.1103/PhysRevD.110.095001\n\nChambers-Wall, G., A. Gnech, G. B. King, S. Pastore, M. Piarulli, R. Schiavilla, and R. B. Wiringa. “Magnetic Structure of 𝐴≤10 Nuclei Using the Norfolk Nuclear Models with Quantum Monte Carlo Methods,” Physical Review C (November 2024), APS. doi: 10.1103/PhysRevC.110.054316\n\nChambers-Wall, G., A. Gnech, G. B. King, S. Pastore, M. Piarulli, R. Schiavilla, and R. B. Wiringa. “Quantum Monte Carlo Calculations of Magnetic Form Factors in Light Nuclei,” Physical Review Letters (November 2024), APS. doi: 10.1103/PhysRevLett.133.212501\n\nChu, X., D. Hofstätter, S. Ilager, S. Talluri, D. Kampert, D. Podareanu, D. Duplyakin, I. Brandic, and A. Iosup. “Generic and ML Workloads in an HPC Datacenter: Node Energy, Job Failures, and Node-Job Analysis,” 2024 IEEE 30th International Conference on Parallel and Distributed Systems (ICPADS) (November 2024), Belgrade, Serbia, IEEE. doi: 10.1109/ICPADS63350.2024.00097\n\nDai, Z., and F. Giustino. “Identification of Large Polarons and Exciton Polarons in Rutile and Anatase Polymorphs of Titanium Dioxide,” Proceedings of the National Academy of Sciences of the United States of America (November 2024), National Academy of Sciences. doi: 10.1073/pnas.2414203121\n\nDevarajan, H., L. Pottier, K. Velusamy, H. Zheng, I. Yildirim, O. Kogiou, W. Yu, A. Kougakas, X.-H. Sun, J. S. Yeom, and K. Mohror. “DFTracer: An Analysis-Friendly Data Flow Tracer for AI-Driven Workflows,” SC24: International Conference for High Performance Computing, Networking, Storage and Analysis (November 2024), Atlanta, GA, IEEE. doi: 10.1109/SC41406.2024.00023\n\nDominski, J., P. Maget, P. Manas, J. Morales, S. Ku, A. Scheinberg, C. S. Chang, R. Hager, M. O’Mullane, and the WEST team. “Gyrokinetic Prediction of Core Tungsten Peaking in a WEST Plasma with Nitrogen Impurities,” Nuclear Fusion (November 2024), IOP Publishing. doi: 10.1088/1741-4326/ad8c63\n\nGeng, P., S. Huang, and J. Marian. “Stability of Trivalent and Hexavalent Chromium Oxide Layers on Aluminum Substrates from Electronic Structure Calculations,” Physical Review Materials (November 2024), APS. doi: 10.1103/PhysRevMaterials.8.113603\n\nGrubbs, T., and I. A. Bolotnov. “V2684603: Interface Resolved Simulation of Two-Phase Flow Within a 360° Steam Separator Geometry,” 77th Annual Meeting of the APS Division of Fluid Dynamics (November 2024), Salt Lake City, UT, APS. doi: 10.1103/APS.DFD.2024.GFM.V2684603\n\nHeinemann, C., J. Amstutz, J. A. Insley, V. A. Mateevitsi, M. E. Papka, and S. Rizzi. “Graphical Representation Through a User Interface for In Situ Scientific Visualization with Ascent,” 2024 IEEE 14th Symposium on Large Data Analysis and Visualization (LDAV) (November 2024), St Pete Beach, FL, IEEE. doi: 10.1109/LDAV64567.2024.00017\n\nJung, C., and N. H. Christ. “Riemannian Manifold HMC with Fermions,” The 40th International Symposium on Lattice Field Theory (LATTICE2023) (November 2024), Batavia, IL, Sissa Medialab. doi: 10.22323/1.453.0009\n\nKing, G. B., G. Chambers-Wall, A. Gnech, S. Pastore, M. Piarulli, and R. B. Wiringa. “Longitudinal Form Factors of 𝐴≤10 Nuclei in a Chiral Effective Field Theory Approach,” Physical Review C (November 2024), APS. doi: 10.1103/PhysRevC.110.054325\n\nKumar, P., S. Kabra, and J. M. Cole. “A Database of Stress-Strain Properties Auto-Generated from the Scientific Literature Using ChemDataExtractor,” Scientific Data (November 2024), Springer Nature. doi: 10.1038/s41597-024-03979-6\n\nLavroff, R. H., E. Cummings, K. Sawant, Z. Zhang, P. Sautet, and A. N. Alexandrova. “Cu-Supported ZnO under Conditions of CO2 Reduction to Methanol: Why 0.2 ML Coverage?,” The Journal of Physical Chemistry Letters (November 2024), ACS Publications. doi: 10.1021/acs.jpclett.4c02908\n\nLi, Z., K. Shi, D. Dubbeldam, M. Dewing, C. Knight, Á. Vásquez-Mayagoitia, and R. Q. Snurr. “Efficient Implementation of Monte Carlo Algorithms on Graphical Processing Units for Simulation of Adsorption in Porous Materials,” Journal of Chemical Theory and Computation (November 2024), ACS Publications. doi: 10.1021/acs.jctc.4c01058\n\nMartinez-Navarro, H., A. Bertrand, R. Doste, H. Smith, J. Tomek, G. Ristagno, R. S. Oliveira, R. Weber dos Santos, S. V. Pandit, and B. Rodriguez. “ECG Analysis of Ventricular Fibrillation Dynamics Reflects Ischaemic Progression Subject to Variability in Patient Anatomy and Electrode Location,” Frontiers in Cardiovascular Medicine (November 2024), Frontiers Media. doi: 10.3389/fcvm.2024.1408822\n\nMateevitsi, V. A., A. Sewell, M. Bode, P. Fischer, J. H. Göbbert, J. A. Insley, I. Kavroulakis, Y.-H. Lan, M. Min, D. Papageorgiou, M. E. Papka, S. Petruzza, S. Rizzi, and A. Tomboulides. “Visuals on the House: Optimizing HPC Workflows with No-Cost CPU Visualization,” 2024 IEEE 14th Symposium on Large Data Analysis and Visualization (LDAV) (November 2024), St Pete Beach, FL, IEEE. doi: 10.1109/LDAV64567.2024.00016\n\nMinch, P., R. Bhattarai, K. Choudary, and T. D. Rhone. “Predicting Magnetic Properties of van der Waals Magnets Using Graph Neural Networks,” Physical Review Materials (November 2024), APS. doi: 10.1103/PhysRevMaterials.8.114002\n\nPovaliaiev, D., R. Liem, J. Kunkel, J. Lofstead, and P. Carns. “High-Quality I/O Bandwidth Prediction with Minimal Data via Transfer Learning Workflow,” 2024 IEEE 36th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD) (November 2024), Hilo, HI, IEEE. doi: 10.1109/SBAC-PAD63648.2024.00017\n\nPrince, M. H., H. Chan, A. Vriza, T. Zhou, V. K. Sastry, Y. Luo, M. T. Dearing, R. J. Harder, R. K. Vasudevan, and M. J. Cherukara. “Opportunities for Retrieval and Tool Augmented Large Language Models in Scientific Facilities,” npj Computational Materials (November 2024), Springer Nature. doi: 10.1038/s41524-024-01423-2\n\nSewell, A., L. Dyken, V. A. Mateevitsi, W. Usher, J. Amstutz, T. Marrinan, K. Reda, S. Rizzi, J. A. Insley, M. E. Papka, S. Kumar, and S. Petruzza. “High-Quality Approximation of Scientific Data using 3D Gaussian Splatting,” 2024 IEEE 14th Symposium on Large Data Analysis and Visualization (LDAV) (November 2024), St Pete Beach, FL, IEEE. doi: 10.1109/LDAV64567.2024.00018\n\nXu, N., and I. Bermejo-Moreno. “Wall-Modeled Large-Eddy Simulations of the Flow over a Gaussian-Shaped Bump with a Sensor-Based Blended Wall Model,” Physical Review Fluids (November 2024), APS. doi: 10.1103/PhysRevFluids.9.114605\n\nDECEMBER\n\nAndreoli, L., G. B. King, S. Pastore, M. Piarulli, J. Carlson, S. Gandolfi, and R. B. Wiringa. “Quantum Monte Carlo Calculations of Electron Scattering from 12C in the Short-Time Approximation,” Physical Review C (December 2024), APS. doi: 10.1103/PhysRevC.110.064004\n\nBhati, A. P., S. Wan, and P. V. Coveney. “Equilibrium and Nonequilibrium Ensemble Methods for Accurate, Precise and Reproducible Absolute Binding Free Energy Calculations,” Journal of Chemical Theory and Computation (December 2024), ACS Publications. doi: 10.1021/acs.jctc.4c01389\n\nBothmann, E., T. Childers, W. Giele, S. Höche, J. Isaacson, and M. Knobbe. “A Portable Parton-Level Event Generator for the High-Luminosity LHC,” SciPost Physics (December 2024), SciPost Foundation. doi: 10.21468/SciPostPhys.17.3.081\n\nCloët, I., X. Gao, S. Mukherjee, S. Syritsyn, N. Karthik, P. Petreczky, R. Zhang, and Y. Zhao. “Lattice QCD Calculation of X-Dependent Meson Distribution Amplitudes at Physical Pion Mass with Threshold Logarithm Resummation,” Physical Review D (December 2024), APS. doi: 10.1103/PhysRevD.110.114502\n\nComiss, L., G. R. Farrar, and M. S. Muzio. “Ultra-High-Energy Cosmic Rays Accelerated by Magnetically Dominated Turbulence,” The Astrophysical Journal Letters (December 2024), American Astronomical Society. doi: 10.3847/2041-8213/ad955f\n\nDharuman, G., K. Hippe, A. Brace, S. Foreman, V. Hatanpää, V. K. Sastry, H. Zheng, L. Ward, S. Muralidharan, A. Vasan, B. Kale, C. M. Mann, H. Ma, Y.-H. Cheng, Y. Zamora, S. Liu, C. Xiao, M. Emani, T. Gibbs, M. Tatineni, D. Canchi, J. Mitchell, K. Yamada, M. Garzaran, M. E. Papka, I. Foster, R. Stevens, A. Anandkumar, V. Vishwanath, and A. Ramanathan. “MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization,” SC24-W: Workshops of the International Conference for High Performance Computing, Networking, Storage and Analysis (December 2024), Atlanta, GA, IEEE. doi: 10.1109/SC41406.2024.00013\n\nGoldsborough, S. S., S. Cheng, D. Kang, J. P. Molnar, Y. M. Wright, and C. E. Frouzakis. “Asynchronicity in Opposed-Piston RMCs: Does It Matter?,” Proceedings of the Combustion Institute (December 2024), Elsevier. doi: 10.1016/j.proci.2024.105406\n\nGovett, M., B. Bah, P. Bauer, D. Berod, V. Bouchet, S. Corti, C. Davis, Y. Duan, T. Graham, Y. Honda, A. Hines, M. Jean, J. Ishida, B. Lawrence, J. Li, J. Luterbacher, C. Muroi, K. Rowe, M. Schultz, M. Visbeck, and K. Williams. “Exascale Computing and Data Handling: Challenges and Opportunities for Weather and Climate Prediction,” Bulletin of the American Meteorological Society (December 2024), American Meteorological Society. doi: 10.1175/BAMS-D-23-0220.1\n\nGraziani, C., and M. Ngom. “Targeted Adaptive Design,” SIAM/ASA Journal on Uncertainty Quantification (December 2024), Society for Industrial and Applied Mathematics. doi: 10.1137/22M149898X\n\nHarlass, M., R. R. Dalmat, J. Chubak, R. van den Puttelaar, N. Udaltsova, D. A. Corley, C. D. Jensen, N. Collier, J. Ozik, I. Lansdorp-Vogelaar, and R. G. S. Meester. “Optimal Stopping Ages for Colorectal Cancer Screening,” JAMA Network Open (December 2024), AMA. doi: doi.org/10.1001/jamanetworkopen.2024.51715\n\nJin, H., G. Papadimitriou, K. Raghavan, P. Zuk, P. Balaprakash, and C. Wang. “Large Language Models for Anomaly Detection in Computational Workflows: From Supervised Fine-Tuning to In-Context Learning,” SC24: International Conference for High Performance Computing, Networking, Storage and Analysis (December 2024), Atlanta, GA, IEEE. doi: 10.1109/SC41406.2024.00098\n\nKéruzoré, F., L. E. Bleem, N. Frontiere, N. Krishnan, M. Buehlmann, J. D. Emberson, S. Habib, and P. Larsen. “The Picasso Gas Model: Painting Intracluster Gas on Gravity-Only Simulations,” The Open Journal of Astrophysics (December 2024), Maynooth Academic Publishing. doi: 10.33232/001c.127486\n\nLin, P. T., P. Nayak, A. Kashi, D. Kulkarni, A. Scheinberg, and H. Anzt. “Accelerating Fusion Plasma Collision Operator Solves with Portable Batched Iterative Solvers on GPUs,” ISC High Performance 2024 International Workshops (December 2024), Hamburg, Germany, Springer Nature. doi: 10.1007/978-3-031-73716-9_9\n\nLiu, J., J. Tian, S. Wu, S. Di, B. Zhang, R. Underwood, Y. Huang, J. Huang, K. Zhao, G. Li, D. Tao, Z. Chen, and F. Cappelo. “CUSZ-i: High-Ratio Scientific Lossy Compression on GPUs with Optimized Multi-Level Interpolation,” SC24: International Conference for High Performance Computing, Networking, Storage and Analysis (December 2024), Atlanta, GA, IEEE. doi: 10.1109/SC41406.2024.00019\n\nMartin, A., G. Liu, B. Joo, R. Wu, M. S. Kabir, and E. W. Draeger. “Designing a GPU-Accelerated Communication Layer for Efficient Fluid-Structure Interaction Computations on Heterogeneous Systems,” SC24: International Conference for High Performance Computing, Networking, Storage and Analysis (December 2024), Atlanta, GA, IEEE. doi: 10.1109/SC41406.2024.00099\n\nPauloski, J. G., V. Hayot-Sasson, L. Ward, A. Brace, A. Bauer, K. Chard, and I. Foster. “Object Proxy Patterns for Accelerating Distributed Applications,” IEEE Transactions of Parallel and Distributed Systems (December 2024), IEEE. doi: 10.1109/TPDS.2024.3511347\n\nPearl, A. N., G. D. Beltz-Mohrmann, and A. P. Hearin. “DiffOpt: Parallel Optimization of Jax Models,” The Journal of Open Source Software (December 2024), Open Source Initiative. doi: 10.21105/joss.07522\n\nRaj, M. S., T. MacDougall, D. Zhang, and D. Dai . “An Empirical Study of a Machine Learning-Based Synthetic Job Trace Generation Methods,” Job Scheduling Strategies for Parallel Processing 2024 (December 2024), San Francisco, CA, Springer Nature. doi: 10.1007/978-3-031-74430-3_2\n\nTan, N., K. Assogba, W. J. Ashworth, B. Bogale, F. Cappello, M. M. Rafique, M. Taufer, and B. Nicolae. “Towards Affordable Reproducibility Using Scalable Capture and Comparison of Intermediate Multi-Run Results,” Middleware ‘24: Proceedings of the 25th International Middleware Conference (December 2024), Hong Kong, ACM. doi: 10.1145/3652892.3700780\n\nWisesa, P., and W. A. Saidi. “Overcoming Inaccuracies in Machine Learning Interatomic Potential Implementation for Ionic Vacancy Simulations,” Physical Insights into Chemistry, Catalysis, and Interfaces (December 2024), ACS. doi: 10.1021/acs.jpclett.4c02934\n\nYu, V. W., Y. Jin, G. Galli, and M. Govoni. “ GPU-Accelerated Solution of the Bethe–Salpeter Equation for Large and Heterogeneous Systems,” Journal of Chemical Theory and Computation (December 2024), ACS Publications. doi: 10.1021/acs.jctc.4c01253\n\nZhang, Z., W. Gee, R. H. Lavroff, and A. N. Alexandrova. “GOCIA: A Grand Canonical Global Optimizer for Clusters, Interfaces, and Adsorbates,” Physical Chemistry Chemical Physics (December 2024), Royal Society of Chemistry. doi: 10.1039/D4CP03801K\n\nZhang, Z., M. Lyu, X. Han, S. Bandara, M. Cui, E. S. Istvan, X. Geng, M. L. Tringides, W. D. Gregor, M. Miyagi, J. Oberstaller, J. H. Adams, Y. Zhang, M. T. Nieman, J. Von Lintig, D. E. Goldberg, and E. W. Yu. “The Plasmodium Falciparum NCR1 Transporter Is an Antimalarial Target that Exports Cholesterol from the Parasite’s Plasma Membrane,” Science Advances (December 2024), AAAS. doi: 10.1126/sciadv.adq6651",
          "url": "http://localhost:4000//science/publications"
        },
  
    
      "search-html": {
          "title": "Search Results",
          "content": "Back to Top &uarr;",
          "url": "http://localhost:4000//search.html"
        },
  
    
      "features": {
          "title": "Features",
          "content": "",
          "url": "http://localhost:4000//features"
        },
  
    
      "expertise-and-resources": {
          "title": "Expertise and Resources",
          "content": "",
          "url": "http://localhost:4000//expertise-and-resources"
        },
  
    
      "science": {
          "title": "Science",
          "content": "",
          "url": "http://localhost:4000//science"
        },
  
    
      "community-and-outreach": {
          "title": "Growing the HPC Community",
          "content": "",
          "url": "http://localhost:4000//community-and-outreach"
        },
  
    
      "expertise-and-resources-staff-news": {
          "title": "Staff News",
          "content": "ALCF Researchers Contribute to Gordon Bell Prize Finalist Study\n\nAn Argonne-led team was selected as a finalist for the 2024 ACM Gordon Bell Prize for their work to develop MProt-DPO, an AI-driven protein design framework. The team’s study, “MProt-DPO: Breaking the ExaFLOPS Barrier for Multimodal Protein Design Workflows with Direct Preference Optimization,” leveraged five of the world’s leading HPC systems, including the ALCF’s Aurora exascale supercomputer, to develop and demonstrate a scalable, end-to-end workflow for accelerating the discovery of new proteins for medicine, catalysis, and other applications. The team includes Argonne researchers Gautham Dharuman, Kyle Hippe, Alexander Brace, Sam Foreman, Väinö Hatanpää, Varuni Sastry, Huihuo Zheng, Logan Ward, Servesh Muralidharan, Archit Vasan, Bharat Kale, Carla Mann, Heng Ma, Murali Emani, Michael Papka, Ian Foster, Rick Stevens, and Venkatram Vishwanath. Additional contributors include Yun-Hsuan Cheng, Yuliana Zamora, and Tom Gibbs (NVIDIA); Shengchao Liu (University of California, Berkeley); Chaowei Xiao (University of Wisconsin-Madison); Mahidhar Tatineni (San Diego Supercomputing Center); and Deepak Canchi, Jerome Mitchell, Koichi Yamada, and Maria Garzaran (Intel).\n\nALCF Team Receives Best Paper Award at ISAV 2024\n\nA team including ALCF researchers received the Best Paper Award at the SC24 conference’s In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization (ISAV 2024) Workshop. The paper, “Bridging Gaps in Simulation Analysis through a General Purpose, Bidirectional Steering Interface with Ascent,” was authored by Argonne’s Victor Mateevitsi, Silvio Rizzi, Joseph Insley, Michael Papka, Thomas Marrinan, and Dimitrios Fytanidis, along with Utah State University’s Andres Sewell and Steve Petruzza, and Cyrus Harrison and Nicole Marsaglia of Lawrence Livermore National Laboratory. Sewell, the lead author, has been a summer student at the ALCF for the past two years. The paper introduces a framework for adding interactive, human-in-the-loop steering controls to existing simulation codes. This capability allows scientists to pause, adjust, and resume large-scale simulations without starting over.\n\nALCF-Led Team Receives Best Paper Award at PMBS24\n\nA team of researchers led by ALCF staff received the Best Paper Award at SC24’s Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS24) Workshop. The paper, “Ponte Vecchio Across the Atlantic: Single-Node Benchmarking of Two Intel GPU Systems,” provides micro-benchmarking data from applications running on the ALCF’s Aurora supercomputer and another Intel GPU-powered system, housed at the University of Cambridge. Argonne’s Thomas Applencourt, Servesh Muralidharan, Colleen Bertoni, JaeHyuk Kwack, Ye Luo, Esteban Rangel, John Tramm, and Yasaman Ghadar authored the paper in collaboration with University of Bristol’s Aditya Sadawarte and Tom Deakin, and University of Cambridge’s Christopher Edsall.\n\nAurora Install Team Recognized for Outstanding Safety Performance\n\nAs part of the 2024 Argonne Board of Governors Awards, the Aurora installation team received the James B. Porter, Jr. Team Award for Outstanding Safety Performance, which recognizes teams that embody the principles of integrated safety management and contribute to a positive safety culture. Honored for their efforts in safely installing and preparing Argonne’s Aurora exascale system, team members included ALCF’s William Allcock, Jonathan Bouvet, Susan Coghlan, Joseph Crawford, Gregory Cross, Jeff Goetz, Michael Hogan, Carissa Holohan, Ti Leggett, and Haseebuddin Syed in collaboration with contributors from across the laboratory, including Christopher Baltas, Adena Banas, Erika Gutierrez, Cari Helberg, William Lucnik, Mitchell McClellan, Lisa Polowy, Raihan Rahman, Dillon Roark, Dana Silvestri, Michael Talamonti, and Jeremy Young.\n\nPapka Named UIC Warren S. McCulloch Professor of Computer Science, Director of Electronic Visualization Laboratory\n\nALCF Director Michael Papka was named the Warren S. McCulloch Professor of Computer Science at the University of Illinois Chicago, an endowed professorship recognizing leadership and impact in the field while supporting continued research and teaching. In addition to this honor, Papka was also appointed director of UIC’s Electronic Visualization Laboratory (EVL), an interdisciplinary research space focused on collaborative visualization, virtual reality, and advanced computing and networking infrastructure.\n\nECP Team Earns DOE Secretary of Energy Achievement Award\n\nThe U.S. Department of Energy (DOE) recognized the Exascale Computing Project (ECP) with the Secretary of Energy Honor Achievement Award for its role in delivering an exascale computing ecosystem for the nation. The multi-lab collaboration involved several key staff from Argonne, including Lois Curfman McInnes, David Martin, Susan Coghlan, Todd Munson, and Yasaman Ghadar, who were on hand to receive the award as part of the ECP leadership team.\n\n\n\nShilpika Earns Recognition in Argonne’s Postdoc Research Slam\n\nALCF postdoctoral researcher Shilpika tied for third place in Argonne’s 2024 Postdoc Research Slam with her presentation, “Alice in the Data Labyrinth: Solving Supercomputer Mysteries Through Intelligent Visuals.” The event showcased research from postdoctoral scholars in three-minute pitches before a live audience and panel of judges. Shilpika later presented her work as part of the Argonne OutLoud lecture series event, “Global Problem Solvers: Early-Career Scientists Explore New Frontiers to Unleash Discoveries for Today and Tomorrow.”\n\nMateevitsi Named Senior Member of IEEE and ACM\n\nIn 2024, Victor Mateevitsi, ALCF assistant computer scientist, was named a senior member of both the Institute of Electrical and Electronics Engineers (IEEE) and the Association for Computing Machinery (ACM), recognizing his professional experience and contributions to the field.\n\n\n\nPapka Honored with IEEE Chicago Section Award\n\nALCF Director Michael Papka received the Institute of Electrical and Electronics Engineers (IEEE) Chicago Section’s Distinguished Senior Research and Development Award, recognizing his contributions to high-performance computing. Argonne National Laboratory was also honored with the Friends of the IEEE Chicago Section Award, highlighting the lab’s collaborative efforts with the organization.\n\nPapka Receives NIU Distinguished Alumni Award\n\nALCF Director Michael Papka received the Distinguished Alumni Award from Northern Illinois University’s College of Liberal Arts and Sciences. The award honors alumni who have made significant contributions in their professional field or through civic, cultural, or charitable service.\n\nResearch Team Using ALCF Resources Wins HPCwire Award for Best Use of HPC in Physical Sciences\n\nA team with researchers from Argonne National Laboratory, as well as University of Chicago, University of Illinois-Urbana Champaign, National Center for Supercomputing Applications, and University of Minnesota, received the HPCwire Editors’ Choice Award for Best Use of HPC in Physical Sciences for work utilizing ALCF resources including the Polaris supercomputer and the Globus data management platform. This work was aimed at developing a physics-informed transformer model to predict gravitational wave evolution for spinning binary black hole mergers. The team’s AI approach dramatically reduces simulation time from days to seconds, handling terabyte-scale datasets with high accuracy.\n\nALCF-APS Pipeline Honored with HPCwire Readers’ Choice Award\n\nAn automated pipeline integrating ALCF supercomputers and Advanced Photon Source instruments to enable near-real-time data analysis was named the Best HPC in the Cloud Use Case, winning an HPCwire Readers’ Choice Award. The pipeline allows scientists to adjust experiments on the fly, potentially accelerating scientific breakthroughs by delivering rapid results while researchers still have facility access.",
          "url": "http://localhost:4000//expertise-and-resources/staff-news"
        },
  
    
      "expertise-and-resources-staff-spotlights": {
          "title": "Staff Spotlights",
          "content": "Riccardo Balin, Assistant Computational Scientist\n\nRiccardo Balin joined the ALCF in 2021 as a postdoc under the Aurora Early Science Program (ESP) to work on methods to incorporate machine learning with computational fluid dynamics (CFD) and turbulence modeling simulations. In 2024 he joined the Data Services and Workflows team as an Assistant Computational Scientist. In this role, Riccardo works with ALCF users on developing, scaling, and applying novel workflows which aim to accelerate traditional simulations with AI methods. He also works closely with HPC vendors, ensuring the necessary software tools as well as new innovative solutions are available to our users.\n\nIn the last year, Riccardo continued his involvement in an Aurora ESP project aimed at developing machine learning-based closure models from ongoing simulations of turbulent aerodynamic flows. Thanks to his efforts, the project demonstrated INCITE readiness on the Aurora supercomputer. Additionally, Riccardo has spearheaded an Argonne-led effort to incorporate AI methods into the nekRS CFD code called nekRS-ML. This collaboration now includes researchers from four divisions at Argonne and led to a new ALCC proposal for an AI-powered toolkit to accelerate simulation-based design space exploration. Riccardo has also fostered new collaborations spanning across DOE labs to develop benchmarks for coupled simulation and AI workflows in order to better understand how these workloads can take full advantage of current and future HPC systems.\n\nMurali Emani, Assistant Computational Scientist\n\nMurali Emani is a Computer Scientist in the Artificial Intelligence and Machine Learning (AIML) group with the Argonne Leadership Computing Facility.\n\nMurali develops performance models to identify and address bottlenecks while scaling machine learning and deep learning frameworks on emerging supercomputers for scientific applications. He also co-designs emerging hardware architectures to scale up machine learning algorithms.\n\nMurali co-leads the ALCF AI Testbed, where he explores the performance and efficiency of AI accelerators for scientific machine learning applications. He works in close collaboration with existing vendors to influence their future product offerings and with potential vendors to aid in procurement. He has also organized various training sessions and tutorials on programming the AI Testbed accelerators at various venues, such as the 2024 Supercomputing Conference (SC24), that gained tremendous response from the community.\n\nMurali is a core member of the Model Architecture and Performance Evaluation working group in the Trillion Parameter Consortium (TPC) and organized tutorials and hackathon sessions at the TPC event. He is actively engaged in the AuroraGPT project, which aims to develop open science foundation models across various science disciplines. As a technical committee member of the first NAIRR workshop, he chaired breakout sessions to understand the gaps and goals for performance of AI applications.\n\nMurali was a co-author of the “MProt-DPO” paper, a Gordon Bell Prize finalizst at SC24. He serves as a co-chair for the MLPerf HPC group at MLCommons, which benchmarks large-scale machine learning on HPC systems. He also has been actively engaged in Aurora development, serving as point of contact for two benchmark applications, and works with Intel’s deep learning frameworks team to evaluate the software stack and work to optimize performance at scale.\n\n\n\nVaruni Sastry, Assistant Computational Scientist\n\nVaruni Sastry joined Argonne in the Data Science and Learning Division in 2021 as a predoctoral appointee, primarily working on AI testbeds with a focus on evaluating and benchmarking different AI and ML workloads on next-generation dataflow-based hardware. She also assisted ALCF users in deploying different scientific AI workloads on these accelerators. In 2023, she officially joined the ALCF as an Assistant Computer scientist and continues to lead several efforts in enabling AI for Science workloads on AI Testbed and other supercomputers at the ALCF.\n\nIn 2024, Varuni joined the AuroraGPT pre-training team, setting up the data processing pipeline and developing several key features for distributed training framework for scalable language and vision models. Varuni contributed to three different Gordon Bell submissions for 2024, and the “MProt-DPO” work on multimodal protein design workflow was selected as a Gordon Bell finalist. She was awarded an Impact Argonne Award for the Enhancement of Argonne’s Reputation for her contribution. She was also honored with an Impact Argonne Award for Extraordinary Efforts for her contribution to “AuroraGlimmer,” a project aiming to build a scalable pipeline for the AuroraGPT project. In collaboration with teams from Argonne’s Center for Nanoscale Materials and Advanced Photon Source, she developed a large language model-based chat framework incorporating retrieval-augmented generation and tailored for scientific facilities, published in npj Computational Materials.\n\nAs part of outreach activities, Varuni co-organized several tutorial and workshops on AI Testbeds including sessions at the 2024 SuperComputing Conference, Argonne Training Program on Extreme-Scale Computing (ATPESC) and other ALCF events. She also delivered an invited talk at NNSA Emergent Technology Seminar. In addition, she co-organized an INCITE GPU hackathon and served as a reviewer for both the ATPESC 2024 and INCITE 2025 committees.\n\nChristine Simpson, Assistant Computational Scientist\n\nChristine Simpson joined the ALCF in 2022 as an Assistant Computational Scientist in the Data Science group. Now part of the Data Services and Workflows group, she primarily focuses on workflows on ALCF systems, which involves development and testing of workflow tools and user support and training. She is currently the lead developer for Balsam, an ALCF-developed workflow package that has enabled users to deploy high-throughput workflows on ALCF systems. She also works closely with the Parsl and Globus teams to help optimize their tools and services for ALCF users.\n\nIn the past year, Christine has been heavily involved in ALCF efforts relating to Integrated Research Infrastructure (IRI), an initiative to interconnect DOE experimental and computational facilities. She has been the ALCF point person in a collaboration with the DIII-D National Fusion Facility and NERSC, and led efforts to run a production IRI workflow on the ALCF’s Polaris supercomputer to analyze the dynamics of neutral beams within the DIII-D tokamak during its fall campaign. A demonstration was performed at the 2024 Supercomputing Conference wherein Polaris analyzed live DIII-D experiments concurrent with an IRI workflow at NERSC. In addition to DIII-D effots, Christine works with a number of other IRI users to analyze data on ALCF systems.\n\nChristine has also worked on exploring new approaches for coupling simulation and AI/ML codes on ALCF systems. She has co-led efforts to explore a new workflow and data management tool called Dragon developed by HPE. She has worked to port a drug discovery workflow from the CANDLE project to Dragon and presented on this effort at the 2024 Platform for Advanced Scientific Computing Conference (PASC24). She has worked closely with HPE developers to improve Dragon performance and features for ALCF applications.\n\nChristine is also the postdoc hiring lead for ALCF. Prior to joining Argonne, she received her PhD in Astronomy studying galaxy formation and evolution with numerical simulations.\n\n\n\nSheeja Susan, Software Development Specialist\n\nSheeja Susan joined the ALCF in 2019 as a Software Development Specialist under the Advanced Integration Group. She was a contractor for 5 years and became a full-time employee in 2024.\n\nAt Argonne, Sheeja is responsible for developing and maintaining frontend screens within the ALCF portal and Allocation Request Management websites. She also creates test automation scripts to ensure the website pages run correctly, and was part of the team that managed migration from AngularJS to Angular in ALCF scripts. She has developed ALCF screens such as the Director’s Discretionary Allocation Request and View Systems pages—along with many others for administrators—and helped implement the page routing. She works closely with UX/design teams and effectively translates design concepts into functional webpages. Coordinating with the backend team, Sheeja creates modular, reusable code components for the development of user-friendly, responsive pages across various screen sizes. She has written numerous test scripts in Cypress to test the functionality of webpages and helped to reduce significantly the number of failing ALCF test scripts.\n\nMuch of Sheeja’s work in 2024 revolved around the frontend development of Allocation Request Management (ARM) website and the new version of the Director’s Discretionary Allocation Request form (DD Allocation Request form). The ALCF Allocations Committee reviews requests from the DD Allocation Request form through the ARM site. The ARM site is the internal clearinghouse for ALCF staff to view, analyze, and ultimately approve allocation requests. Within the site, the Committee reviews a project’s goals and discerns whether the project fits with the ALCF resources and mission. Sheeja also participated in the ALCF portal development to replace the UB3 home page and login section with that of portal, to create functionalities for the redirection of UB3-specific URLs from email links, and to modify the UB3 menu with the same look and feel of the portal menu. Lastly, she was also able to successfully implement Angular Route Guards to activate and deactivate the navigation to specific routes in ALCF.\n\nPeter Upton, Systems Integration Administrator/Support\n\nPeter Upton is a Systems Integration Administrator/Support at the ALCF. He manages, supports, and updates ALCF GitLab installations and also assists in managing DNS and Salt systems. In this capacity he provides peer reviews, troubleshoots GitLab user issues, and supports work on the Aurora system, for which he has helped create a new node type for administrative tasks.\n\nIn addition to assisting vendors in using Aurora to improve functionality and stability—while also assisting and tracking work around a new ALCF testbed-—Peter aids his coworkers with HPCM-related tasks and participates in miscellaneous data-center physical activities (such as remote troubleshooting and cable routing). He also addresses security vulnerabilities promptly, keeping documentation up to date and coordinating GitLab usage, licensing, and upgrades.\n\nIn 2024, Upton upgraded GitLab instances to RHEL9 and GitLab runners to RHEL9. He configured GitLab JLSE container registry and reworked the GitLab upgrade process to minimize downtime and increase consistency. He also automated Jacamar-CI RPM import.",
          "url": "http://localhost:4000//expertise-and-resources/staff-spotlights"
        },
  
    
      "expertise-and-resources-systems": {
          "title": "ALCF Systems",
          "content": "Supercomputing Resources\n\nALCF supercomputing resources support large-scale, computationally intensive projects aimed at solving some of the world’s most complex and challenging scientific problems.\n\n\n\n\n  \n    \n      System Name\n      Purpose\n      Architecture\n      Peak Performance\n      Processors per Node\n      GPUs per Node\n      Nodes\n      Cores\n      Memory\n      Interconnect\n      Racks\n    \n  \n  \n    \n      Aurora\n      Purpose Science Campaigns\n      Architecture HPE Cray EX\n      Peak Performance 2 EF\n      Processors per Node 2 Intel Xeon CPU Max Series processors\n      GPUs per Node 6 Intel Data Center GPU Max Series\n      Nodes 10,624CPUs: 21,248GPUs: 63,744\n      Cores 9,264,128\n      Memory 20.4 PB\n      Interconnect HPE Slingshot 11 with Dragonfly Configuration\n      Racks 166\n    \n    \n      Polaris\n      Purpose Science Campaigns\n      Architecture HPE Apollo 6500 Gen10+\n      Peak Performance 34 PF; 44 PF of Tensor CoreFP64 performance\n      Processors per Node 3rd Gen AMD EPYC\n      GPUs per Node 4 NVIDIA A100 Tensor Core\n      Nodes 560\n      Cores 17,920\n      Memory 280 TB (DDR4); 87.5 TB (HBM)\n      Interconnect HPE Slingshot 11 with Dragonfly configuration\n      Racks 40\n    \n    \n      Sophia\n      Purpose Science Campaigns\n      Architecture NVIDIA DGX A100\n      Peak Performance 3.9 PF\n      Processors per Node 2 AMD EPYC 7742\n      GPUs per Node 8 NVIDIA A100 Tensor Core\n      Nodes 24\n      Cores 3,072\n      Memory 26 TB (DDR4); 8.32 TB (GPU)\n      Interconnect NVIDIA HDR IniniBand\n      Racks 7\n    \n  \n\n\nALCF AI Testbed\n\nThe ALCF AI Testbed provides an infrastructure of next-generation AI-accelerator machines for research campaigns at the intersection of AI and science. AI testbeds include:\n\n\n  \n    \n      System Name\n      System Size\n      Compute Units per Accelerator\n      Estimated Performance of a Single Accelerator (TFlops)\n      Software Stack Support\n      Interconnect\n    \n  \n  \n    \n      Cerebras CS-2\n      2 Nodes (Each with a Wafer-Scale Engine) Including MemoryX and SwarmX\n      850,000 Cores\n      &gt; 5,780 (FP16)\n      Cerebras SDK, TensorFlow, PyTorch\n      Ethernet-based\n    \n    \n      SambaNova Cardinal SN30\n      64 Accelerators (8 Nodes and 8 Accelerators per Node)\n      1,280 Programmable Compute Units\n      &gt;660 (BF16)\n      SambaFlow, PyTorch\n      Ethernet-based\n    \n    \n      GroqRack\n      72 Accelerators (9 Nodes and 8 Accelerators per Node)\n      5,120 Vector ALUs\n      &gt;188 (FP16) &gt;750 (INT8)\n      GroqWare SDK, ONNX\n      RealScale™\n    \n    \n      Graphcore Bow Pod-64\n      64 Accelerators (4 Nodes and 16 Accelerators per Node)\n      1,472 Independent Processing Units\n      &gt;250 (FP16)\n      PopART, TensorFlow, PyTorch, ONNX\n      IPU Link\n    \n    \n      Habana Gaudi-1\n      16 Accelerators (2 Nodes and 8 Accelerators per Node)\n      8 TPC + GEMM Engine\n      &gt;150 (FP16)\n      SynapseAI, TensorFlow, PyTorch\n      Ethernet-based\n    \n  \n\n\nData Storage Systems\n\nALCF disk storage systems provide intermediate-term storage for users to access, analyze, and share computational and experimental data. Tape storage is used to archive data from completed projects.\n\n\n\n\n  \n    \n      System Name\n      File System\n      Storage System\n      Usable Capacity\n      Sustained Data Transfer Rate\n      Disk Drives\n    \n  \n  \n    \n      Eagle\n      File System Lustre\n      Storage System HPE ClusterStor E1000\n      Usable Capacity 100 PB\n      Sustained Data Transfer Rate 650 GB/s\n      Disk Drives 8,480\n    \n    \n      Grand\n      File System Lustre\n      Storage System HPE ClusterStor E1000\n      Usable Capacity 100 PB\n      Sustained Data Transfer Rate 650 GB/s\n      Disk Drives 8,480\n    \n    \n      Swift\n      File System Lustre\n      Storage System All NVMe Flash Storage Array\n      Usable Capacity 123 TB\n      Sustained Data Transfer Rate 48 GB/s\n      Disk Drives 24\n    \n    \n      Tape Storage\n      File System –\n      Storage System LT06 and LT08 Tape Technology\n      Usable Capacity 300 PB\n      Sustained Data Transfer Rate –\n      Disk Drives –\n    \n  \n\n\nNetworking\n\nNetworking is the fabric that ties all of the ALCF’s computing systems together. InfiniBand enables communication between system I/O nodes and the ALCF’s various storage systems. The Production HPC SAN is built upon NVIDIA Mellanox High Data Rate (HDR) InfiniBand hardware. Two 800-port core switches provide the backbone links between 80 edge switches, yielding 1600 total available host ports, each at 200 Gbps, in a non-blocking fat-tree topology. The full bisection bandwidth of this fabric is 320 Tbps. The HPC SAN is maintained by the NVIDIA Mellanox Unified Fabric Manager (UFM), providing Adaptive Routing to avoid congestion, as well as the NVIDIA Mellanox Self-Healing Interconnect Enhancement for InteLligent Datacenters (SHIELD) resiliency system for link fault detection and recovery.\n\nWhen external communications are required, Ethernet is the interconnect of choice. Remote user access, systems maintenance and management, and high-performance data transfers are all enabled by the Local Area Network (LAN) and Wide Area Network (WAN) Ethernet infrastructure.\n\nThis connectivity is built upon a combination of Extreme Networks SLX and MLXe routers and NVIDIA Mellanox Ethernet switches. \nALCF systems connect to other research institutions over multiple 100 Gbps connections that link to many high-performance research networks, including regional networks like the Metropolitan Research and Education Network (MREN), as well as national and international networks like the Energy Sciences Network (ESnet) and Internet2.\n\nJoint Laboratory for System Evaluation\n\nArgonne’s Joint Laboratory for System Evaluation (JLSE) provides access to leading-edge testbeds for research aimed at evaluating future extreme-scale computing systems, technologies, and capabilities. Here is a partial listing of the novel technology that make up the JLSE.\n\n\n  Florentia: Test and development system equipped with early versions of the Sapphire Rapids CPUs and Ponte Vecchio GPUs that power Aurora\n  Arcticus, DevEP, Iris: Intel discrete and integrated GPU testbeds to\nsupport the development, optimization, and scaling of applications and software for Aurora\n  Aurora Software Development Kit: Frequently updated version of the publicly available Intel oneAPI toolkit for Aurora development\n  Arm Ecosystem: Apollo 80 Fujitsu A64FX Arm system, NVIDIA Ampere Arm and A100 test kits, and an HPE Comanche with Marvell ARM64 CPU platform provide an ecosystem for porting applications and measuring performance on next-generation systems\n  Presque: Intel DAOS nodes for testing the Aurora storage system\n  Edge Testbed: NVIDIA Jetson Xavier and Jetson Nano platforms provide a resource for testing and developing edge computing applications\n  NVIDIA and AMD GPUs: Clusters of NVIDIA V100, A100, and A40 GPUs, and AMD MI50 and MI100 GPUs for preparing applications for heterogeneous computing architectures\n  NVIDIA Bluefield-2 DPU SmartNICs: Platform used for confidential computing, MPICH offloading, and APS data transfer acceleration\n  NextSilicon Maverick: First-generation product being tested by Argonne researchers\n  Atos Quantum Learning Machine: Platform for testing and developing quantum algorithms and applications",
          "url": "http://localhost:4000//expertise-and-resources/systems"
        },
  
    
      "expertise-and-resources-team": {
          "title": "ALCF Team",
          "content": "Operations\nThe ALCF’s HPC systems administrators manage and support all ALCF computing systems, ensuring users have stable, secure, and highly available resources to pursue their scientific goals. This includes the ALCF’s production supercomputers, AI accelerators, supporting system environments, storage systems, and network infrastructure. The team’s software developers create tools to support the ALCF computing environment, including software for user account and project management, job failure analysis, and job scheduling. User support specialists provide technical assistance to ALCF users and manage the workflows for user accounts and projects. In the business intelligence space, staff data architects assimilate and verify ALCF data to ensure accurate reporting of facility information.\n\nScience\nComputational scientists with multidisciplinary domain expertise work directly with ALCF users to maximize and accelerate their research efforts. In addition, the ALCF team applies broad expertise in data science, machine learning, data visualization and analysis, and mathematics to help application teams leverage ALCF resources to pursue data-driven discoveries. With a deep knowledge of the ALCF computing environment and experience with a wide range of numerical methods, programming models, and computational approaches, staff scientists and performance engineers help researchers optimize the performance and productivity of simulation, data, and learning applications on ALCF systems.\n\nTechnology\nThe ALCF team plays a key role in designing and validating the facility’s next-generation supercomputers. By collaborating with compute vendors and the performance tools community, staff members ensure the requisite programming models, tools, debuggers, and libraries are available on ALCF platforms. The team also helps manage Argonne’s Joint Laboratory for System Evaluation, which houses next-generation testbeds that enable researchers to explore and prepare for emerging computing technologies. ALCF computer scientists, performance engineers, and software engineers develop and optimize new tools and capabilities to facilitate science on the facility’s current and future computing resources. This includes the deployment of scalable machine learning frameworks, in-situ visualization and analysis capabilities, data management services, workflow packages, and container technologies. In addition, the ALCF team is actively involved in programming language standardization efforts and contributes to cross-platform libraries to further enable the portability of HPC applications.\n\nOutreach\nALCF staff members organize and participate in training events that prepare researchers for efficient use of leadership computing systems. They also participate in a wide variety of educational activities aimed at cultivating a diverse and skilled HPC community and workforce in the future. In addition, staff outreach efforts include facilitating partnerships with industry and academia, and communicating the impactful research enabled by ALCF resources to external audiences.",
          "url": "http://localhost:4000//expertise-and-resources/team"
        },
  
    
      "community-and-outreach-user-training-activities": {
          "title": "Training ALCF Users",
          "content": "ALCF AI Testbed Training Workshops\nStarting April of 2024, the ALCF hosted a series of training workshops that introduced researchers to the novel AI accelerators deployed at the ALCF AI Testbed. The four individual workshops demonstrated to participants the architecture and software of the SambaNova DataScale SN30 system, the Cerebras CS-2 system, the Graphcore Bow Pod system, and the GroqRack system.\n\nALCF Hands-on HPC Workshop\nHeld in October at Argonne, the ALCF Hands-on HPC Workshop is designed to help attendees boost application performance on ALCF systems. The three-day workshop provided an opportunity for hands-on time on Polaris and AI Testbeds focusing on porting applications to heterogeneous architectures (CPU + GPU), improving code performance, and exploring AI/ML applications development on ALCF systems. For a recap of the event, read Argonne’s article about the 2024 workshop.\n\n\n\nALCF INCITE GPU Hackathon\nIn May, the ALCF hosted its GPU Hackathon for the fourth time, a hybrid event designed to help developers accelerate their codes on ALCF resources and prepare for the INCITE call for proposals. The multi-day hackathon gave attendees access to ALCF’s Polaris system. Out of the 20 teams that participated this year, 11 teams were part of ongoing INCITE projects, and out of the 9 remaining teams, six teams applied for 2025.  Teams involved were researching a vast array of topics including genome structure and function, deep protein design for synthetic biology, and genetics data. For a recap of the event, read Argonne’s article about the 2024 Hackathon.\n\nATPESC 2024\nThe annual Argonne Training Program on Extreme-Scale Computing (ATPESC) marked its 12th year in 2024. The two-week event offers training on key skills, approaches, and tools needed to design, implement, and execute computational science and engineering applications on high-end computing systems, including exascale supercomputers. The program features talks from leading computer scientists and HPC experts, as well as hands-on training using DOE leadership-class systems at ALCF, OLCF, and NERSC. In 2024, ATPESC attracted 75 attendees from 54 different institutions worldwide.\n\nAurora Early Science Program Workshops (ESP)\nThe Intel Center of Excellence (COE), in collaboration with ALCF’s Early Science Program, held multiday events where select ESP and ECP project teams worked on developing, porting, and profiling their codes on Sunspot with help from Intel and Argonne experts. The events were geared toward developers and emphasized using the Intel software development kit to get applications running on testbed hardware. Teams were also given the opportunity to consult with ALCF staff and provide feedback. ALCF staff also held dedicated office hours on a range of topics from programming models to profiling tools.\n\n2024 INCITE Proposal Writing Webinars\nIn spring, the INCITE program, ALCF, and the Oak Ridge Leadership Computing Facility (OLCF) jointly hosted two webinars on effective strategies for writing an INCITE proposal.\n\nMonthly ALCF Webinars\nThe ALCF continued to host the monthly ALCF Developer Sessions, aimed at training researchers and increasing the dialogue between HPC users and the developers of leadership-class systems and software. Speakers in the series included developers from Argonne, covering topics such as deep learning frameworks on Aurora, Intel performance profiling tools on Aurora, remote workflows at the ALCF, and the QMCPACK application team’s journey to exascale on Aurora.",
          "url": "http://localhost:4000//community-and-outreach/user-training-activities"
        },
  
    
      "community-and-outreach-building-an-hpc-workforce": {
          "title": "Building the Computing Workforce of the Future",
          "content": "Scientific breakthroughs in HPC and AI are driven by the ingenuity and expertise of the people who design, optimize, and support these powerful technologies. At the ALCF, computational scientists, software engineers, HPC system administrators, and research support staff work to maximize the capabilities of the facility’s HPC and AI resources, ensuring they can drive scientific discoveries across a wide range of disciplines. These efforts include collaborating with ALCF users to help maximize their computing time and research outcomes; deploying and maintaining cutting-edge HPC and AI systems for peak scientific performance; optimizing codes for ALCF resources; advancing open-source software; and developing tools and services to enhance data management, workflows, and computational efficiency.\n\nAttracting and training the next generation of computing professionals is crucial to sustaining innovation in the use of HPC and AI for scientific research. The ALCF is committed to expanding the HPC and AI talent pipeline by engaging with students, educators, and researchers through a variety of outreach, training, and internship programs.\n\nStudent Outreach\n\nThe ALCF is actively working to inspire future computer scientists through its contributions to educational programs that introduce students to coding, computational thinking, and scientific research. Each December, ALCF staff volunteer in Chicagoland schools for the Hour of Code initiative, a global effort to teach students to the basics of programming and broaden participation in computer science. The facility also engages students through programs like CodeGirls@Argonne Camp, which encourages them to explore coding, and annual outreach events such as Introduce a Girl to Engineering Day and Science Careers in Search of Women, where they connect with Argonne researchers and learn about careers in STEM.\n\nFor high school students, the ALCF contributes to immersive learning experiences that introduce them to computational science and data-driven research. Argonne’s annual Coding for Science Camp and Big Data Camp provide hands-on training in programming, data science, and problem-solving techniques used in scientific research. Additionally, ALCF staff mentor students participating in the ACT-SO (Afro-Academic, Cultural, Technological &amp; Scientific Olympics) High School Research Program, guiding them through independent research projects that use Argonne’s computing resources.\n\n\n\nHands-On Training and Career Development\n\nAt the college level, the ALCF provides hands-on learning experiences through its annual summer student program. Through programs like DOE’s Science Undergraduate Laboratory Internship (SULI) and Argonne’s research aide appointments, undergraduate and graduate students have the opportunity to work alongside ALCF staff mentors on real-world research projects in areas such as HPC system administration, data analytics, computational science, and AI-driven workflows.\n\nBeyond summer internships, the ALCF extends its educational outreach with training opportunities throughout the year. One such program is the annual “Intro to AI-driven Science on Supercomputers” training series, which provides undergraduate and graduate students with an introduction to AI and supercomputing for scientific research. Launched in 2021, the virtual series has successfully reached over 700 participants from across the nation.\n\nFor those seeking more advanced training, the facility also hosts the Argonne Training Program on Extreme-Scale Computing (ATPESC), an intensive, two-week program that teaches attendees the key skills and tools needed to use the world’s powerful supercomputers. Since its launch in 2013, ATPESC has trained more than 800 participants, equipping them with knowledge in areas such as programming methodologies, numerical algorithms, HPC architectures, scientific machine learning, and data analysis. The program has been a pivotal experience for many attendees, helping them build valuable connections and advance their careers in computational science.\n\nIn 2024, the ALCF launched the Lighthouse Initiative, a program designed to establish long-term partnerships with academic institutions. This initiative aims to broaden the facility’s user base while fostering connections with the next generation of computing professionals. As part of its workforce development efforts, the ALCF offers internship opportunities to students from partner institutions, providing them with hands-on experience in scientific computing.\n\nRecruitment and Community Engagement\n\nTo connect with new talent and share career opportunities, the ALCF maintains a presence at major computing conferences and industry events. Staff members engage with students and professionals at events such as SC (Supercomputing), the Grace Hopper Celebration, and computing workshops to highlight the impact of HPC in advancing research and innovation.\n\nThe ALCF also supports efforts to build a stronger professional network for women in HPC. In collaboration with the University of Illinois Chicago, the facility helped establish Chicago Women in High Performance Computing (WHPC), a chapter dedicated to broadening participation in the field. The group provides mentorship, resources, and networking opportunities to support members pursuing careers in HPC.\n\nThrough these outreach and engagement efforts, the ALCF is working to grow the community of researchers and professionals who use and develop HPC and AI technologies, ensuring that these fields continue to evolve and drive scientific breakthroughs.",
          "url": "http://localhost:4000//community-and-outreach/building-an-hpc-workforce"
        },
  
    
      "year-in-review-year-in-review": {
          "title": "ALCF Leadership",
          "content": "Bill Allcock, ALCF Director of Operations\n\nFor the operations team, and much of ALCF, 2024 was the year of Aurora. It was the single largest effort, involving every aspect of the team, and culminated in the completion of acceptance testing in December. This concluded an effort that spanned many years and while it was a challenge, I am proud of the operations team for the part they played in bringing Aurora to fruition.\n\nAurora brought many challenges. From the operations side, one of the challenges was managing the sheer volume of log and monitoring data. To address this, we created a “data lake” that ingests data into Parquet files and then incrementally improves and filters them. It also enables SQL-like queries against the files on disk. We are currently running this in parallel with the ETL (Extract, Transform, Load) processes, but we are likely going to shift to ingesting everything into the data lake and then having our Business Intelligence system perform ETL from there. Many of the AI frameworks have native support for the Parquet files, and this should be a valuable tool for AI researchers moving forward.\n\nWhile Aurora sometimes felt like the whole of the ALCF universe, it certainly was not the only focus. Polaris, the ALCF’s primary production machine, had a very good year. Although the official numbers are not yet in, we exceeded all our performance metrics, some by a significant margin. Our Integrated Resource Infrastructure (IRI) work also continued apace. With the Advanced Photon Source coming up from their upgrade, we have begun doing production processing of beamline data and expanded the scope of this work to other facilities and science communities. As long-time users are undoubtedly aware, we typically have a weekly or bi-weekly cadence of regular preventative maintenance. To better support the IRI concept, we put significant effort into improving our systems and processes by adding redundancy, enabling maintenance while systems are live, and implementing other measures to minimize disruption. Our target was a maintenance cadence of four months, and we successfully accomplished that on Polaris. We completed a software upgrade on Aurora using an improved process, where the upgrade was treated much like a software development project. This involved using Sirius as the stage system, making all changes and conducting all testing there, before pushing them up to Aurora. It went well, but there is also room for improvement, and we will continue refining this process.\n\nWe also brought some new resources online this year. Crux, a CPU-only system originally developed as a testbed for evaluating system software for Aurora, was deployed to users late this year. It is small relative to other ALCF systems at about 1 PF, but it is a non-trivial resource and there is a small, but important, set of science applications that can take advantage of a CPU-only machine. Additionally, we deployed Sophia, a 24-node Nvidia A100 DGX system, in July. Previously part of ThetaGPU, Sophia became a standalone system after Theta’s retirement. It is targeted at smaller workloads that need near-immediate turnaround, such as Jupyter notebooks and interactive visualizations.\n\nThe ALCF AI Testbed did not add new systems in 2024, but several upgrades were implemented, and new systems are planned for next year. Resources were also made available to the National Artificial Intelligence Research Resource (NAIRR) pilot, and we supported multiple workshops and an SC24 tutorial.\n\nOn the storage front, the most significant change for users was that we enabled the ability for users to share their data on the Eagle and Grand storage systems by default. Users still must create shares, but they no longer need to explicitly request this capability be enabled. Another significant effort on the storage side, that was, by design, completely transparent to the users, was the migration of all data in the Grand filesystem path onto the same hardware as Eagle so we could repurpose the hardware to support Aurora during testing. This isolated the storage for Aurora from the rest of ALCF to avoid the ongoing changes disrupting other production resources. We also completed routine upgrades of the Lustre storage and HPSS.\n\nWe also released a significant upgrade to the ALCF account and project management website. Named myALCF, the new platform greatly expands web tools for ALCF users to manage their projects and resources. It offers a dashboard with real-time data graphs to show allocation usage, allocation on-track trends, daily/14-day/monthly job activity, and node usage through a project all at a glance. Additionally, myALCF includes a web tool to help new users learn our sbank accounting tool. The sbank tool also helps seasoned ALCF users to be aware of options in sbank that they may not have seen or used before. With myALCF, users still have all the functionality of the previous accounts and project management system (requesting allocation, managing projects and Unix groups, etc.) in a cleaner user interface. We plan to continue expanding myALCF with an improved cluster accounting (sbank) web presence, web UI for job management, and a user notification framework in the future.\n\n\n\nKalyan Kumaran, ALCF Director of Technology\n\nOver the past year, our team made significant strides in advancing the ALCF-3 project, successfully facilitating the transition of the Aurora supercomputer into production. At the same time, we laid the groundwork for the ALCF-4 project, contributing extensively to the early stages of our next system acquisition.\n\nThe Aurora efforts were multifaceted, with our team collaborating closely with Intel on Non-Recurring Engineering (NRE) tasks for key software components, including those integrated within the oneAPI framework. Additionally, we focused on stabilizing and optimizing system performance by running a diverse array of workloads designed to stress-test Aurora, pinpointing performance bottlenecks and addressing system failures. In parallel, our team worked diligently to optimize and scale several science applications from the Aurora Early Science Program and the Exascale Computing Project. A major accomplishment was the preparation and execution of an extensive set of tests and applications that were essential for the functional, performance, and stability assessments during Aurora’s acceptance at the end of the year.\n\nFor the ALCF-4 project, the team played an instrument role in drafting several sections of the Request for Proposal (RFP), actively enaging in internal reviews, and visiting vendors to gain a deeper understanding of their roadmaps while effectively communicating the facility’s evolving needs.\nThroughout the year, the team also made impressive contributions in showcasing Aurora’s extraordinary scientific capabilities. Notable achievements include:\n\n\n  Performance Engineering Excellence: The performance engineering team partnered with Intel to run a comprehensive suite of benchmarks, including the High-Performance Linpack (HPL) benchmark for the TOP500 list and the mixed-precision variant (HPL MxP) to evaluate Aurora’s AI processing capabilities. The team also executed the Graph 500 benchmark, which evaluates large-scale data analytics performance, and the High-Performance Conjugate Gradient (HPCG) benchmark, which measures computational efficiency in scientific simulations. These results placed Aurora among the most powerful supercomputers in the world, establishing its place at the top of the global rankings.\n  AI/ML Innovations: The AI/ML team, in collaboration with other research groups, earned widespread recognition for their groundbreaking work in scaling and fine-tuning a large language model on Aurora. Their efforts were recognized as a finalist for Gordon Bell Prize, underscoring their significant contributions to advancing  AI and machine learning capabilities in supercomputing.\n  Visualization and Data Analytics Milestone: In partnership with Intel, the visualization and data analytics team ensured that Intel’s oneAPI Render Kit fully leveraged Aurora’s GPU ray-tracing cores. As a result, Aurora became the first supercomputer in the world to use GPU-accelerated ray tracing as its primary rendering method, pushing the boundaries of high-performance visualization and data analytics.\n  Data Services and Workflows for Advanced AI: The data services and workflows team continues to work at the forefront of developing scalable inference services to meet the growing demand for large language model prompting on ALCF systems. These services support a broad spectrum of applications, from querying complex scientific phenomena to protein sequence analysis and even code generation. The team’s innovative work has resulted in the creation of user-friendly web applications and robust programmable interfaces for both individual queries and large-batch prompt processing, further expanding the accessibility to cutting-edge AI capabilities.\n  Commitment to Standards and User Empowerment: The team remains actively engaged in various standards committees, advocating for the integration of new features into programming languages and models that are driven by ALCF user needs and the technical requirements of our systems. The team’s commitment to user empowerment is reflected in their ongoing development of comprehensive technical documentation and workshop presentations designed to educate and train the user community. Moreover, the team continues to evaluate emerging systems at Argonne’s Joint Laboratory for System Evaluation and the ALCF AI Testbed, ensuring the ALCF remains at the forefront of technological innovation in anticipation of future acquisitions.\n\n\n\n\nJini Ramprakash, ALCF Deputy Director\n\nIn 2024, we ramped up our efforts to plan for the facility’s next-generation supercomputer via the ALCF-4 project. Even though our newest system, Aurora, is just getting started, it’s critical to lay the groundwork for future supercomputers well in advance. We formally launched the ALCF-4 effort in 2023 with a target deployment in the 2028–2029 timeframe. Over the past year, we’ve reached several key milestones: releasing the draft technical requirements in June, conducting a successful technical design review in August, and engaging with potential vendors to assess their technology roadmaps and timelines. The project will gain further momentum next year with major reviews, including Critical Decision-1 (CD-1), which evaluates the project’s technical approach, design, and implementation while establishing preliminary cost and schedule estimates.\n\nOur work to plan for future computing resources also extended beyond ALCF-4. We contributed to a charge from the DOE Advanced Scientific Computing Advisory Committee (ASCAC) to develop a 10-year outlook for the DOE computing facilities. This effort culminated in a report outlining the investments and upgrades needed to ensure DOE remains at the forefront of scientific computing.\n\nWorkforce development has remained a priority as well. Our annual “Intro to AI” training series continues to provide hundreds of college students with hands-on experience in using AI and supercomputers for science. In addition to training, we offer students the opportunity to apply their skills in real research environments through various appointments and internships at the ALCF. This year, we partnered with the University of Illinois Chicago’s Sprintership program, welcoming five students for three-week internships at the ALCF in the spring—four of whom returned for summer internships to work in their areas of interest. These efforts are part of our broader vision to inspire the next generation of STEM professionals by providing opportunities to explore careers in computing.\n\n\n\nKatherine Riley, ALCF Director of Science\n\nIn 2024, our user community once again pushed the boundaries of scientific computing, achieving breakthroughs in fields ranging from biology and materials science to cosmology and fusion energy. At the same time, we continued to see more and more projects with complex workflows that integrate simulation, data science, and AI methods, reflecting the growing convergence of these approaches in scientific computing. The year was marked by several high-impact projects, including a collaboration between DOE and NASA to perform simulations that are helping prepare for future observations of the cosmos, and the development of an innovative AI-driven protein design framework that was recognized as a finalist for the Gordon Bell Prize. We also made strides in integrating our supercomputing resources with experimental facilities to speed up data-intensive discoveries, while early science teams on Aurora had some promising pre-production results that hint at the transformative advances to come.\n\nOne of the year’s most exciting moments was seeing David Baker, a longtime ALCF user from the University of Washington, receive the 2024 Nobel Prize in Chemistry for his pioneering work in computational protein design. Baker was among the first researchers to use ALCF systems nearly 20 years ago and has since led multiple INCITE projects aimed at designing novel proteins and peptides for medical and industrial applications.\n\nWhile the Nobel Prize underscores the lasting impact of our community’s efforts, computational protein design represents just one area of the dynamic research portfolio supported by the ALCF. In 2024, we provided computing resources to 31 INCITE projects, 31 ALCC projects (spanning two award cycles), and numerous Director’s Discretionary projects. We also began supporting some of the initial projects awarded through the National AI Research Resource (NAIRR) pilot, providing them with access to the ALCF AI Testbed.\n\nTo kick off the year, a multi-institutional team from DOE, NASA, and academia used our now-retired Theta system for a final run before it was decommissioned. The team generated nearly 4 million images of the cosmos to help researchers prepare for upcoming observations from NASA’s Nancy Grace Roman Space Telescope and the NSF-DOE Vera C. Rubin Observatory. By publicly releasing the simulation data, they are enabling the scientific community to refine processing pipelines, improve analysis codes, and prepare to interpret real observations as soon as they start coming in.\n\nThis year also saw major progress toward building an Integrated Research Infrastructure (IRI), a DOE initiative designed to accelerate data-intensive science by connecting supercomputers with large-scale experimental and observational facilities. One of our key partners in this effort is the Advanced Photon Source (APS), a DOE user facility at Argonne. In a demonstration of IRI capabilities, the team used a fully automated pipeline between ALCF and APS to rapidly process data from an X-ray experiment. The researchers reconstructed the experimental data on Polaris and returned results to APS within 15 minutes, highlighting an approach for experiment-time data analysis that can be applied at other facilities.\n\nWe also saw some innovative work being carried out by early science team using pre-production time on Aurora. This included the Gordon Bell Prize finalist project that used Aurora and four other powerful supercomputers to develop an AI-driven framework for protein design. In December, another team ran a pair of cosmological simulations on Aurora to explore recent findings from the Dark Energy Spectroscopic Instrument, which hint at new physics beyond the standard model of cosmology. While these early examples offer a preview of Aurora’s impact, we can’t wait to see the discoveries the system will enable when it is released to the broader scientific community next year.",
          "url": "http://localhost:4000//year-in-review/year-in-review"
        },
  
    
  
    
      "feed-xml": {
          "title": "",
          "content": "&lt;?xml version=”1.0” encoding=”utf-8”?&gt;&lt;?xml-stylesheet type=”text/xml” href=””?&gt;&lt;feed xmlns=”http://www.w3.org/2005/Atom” xml:lang=””&gt;Jekyll&lt;link href=”” rel=”self” type=”application/atom+xml” /&gt;&lt;link href=”” rel=”alternate” type=”text/html” hreflang=”” /&gt;&lt;/updated&gt;&lt;/id&gt;&lt;/title&gt;&lt;/subtitle&gt;&lt;/name&gt;&lt;/email&gt;&lt;/uri&gt;&lt;/author&gt;&lt;entryxml:lang=””&gt;&lt;link href=”” rel=”alternate” type=”text/html” title=”” /&gt;&lt;/published&gt;&lt;/updated&gt;&lt;/id&gt;&lt;content type=”html” xml:base=””&gt;&lt;![CDATA[]]&gt;&lt;/content&gt;&lt;/name&gt;&lt;/email&gt;&lt;/uri&gt;&lt;/author&gt;&lt;category term=”” /&gt;&lt;category term=”” /&gt;&lt;category term=”” /&gt;&lt;summary type=\"html\"&gt;&lt;![CDATA[]]&gt;&lt;/summary&gt;&lt;media:thumbnail xmlns:media=”http://search.yahoo.com/mrss/” url=”” /&gt;&lt;media:content medium=”image” url=”” xmlns:media=”http://search.yahoo.com/mrss/” /&gt;&lt;/entry&gt;&lt;/feed&gt;",
          "url": "http://localhost:4000//feed.xml"
        }
  
  };

  window.store = {...highlights_json, ...performance_json, ...pages_json}
</script>
<script>
  const queryString = window.location.search;
  const urlParams = new URLSearchParams(queryString);
  const searchTerm = urlParams.get('query');
  document.getElementById('search-title').innerHTML = 'Results for: ' + searchTerm;
</script>
<script src="https://unpkg.com/lunr/lunr.js"></script>
<script src="http://localhost:4000/assets/js/search.js"></script>



  <button class="to-top js-to-top">
    Back to Top &uarr;
  </button>
</main>



    </div><footer>

	<div class="content-wrapper">


		<div class="alcf-logo">
			<img src="http://localhost:4000/assets/images/logo-h.png" />
		</div>

		<div class="alcf-info">
			<h2>CONTACT</h2>
			<p>
				<a href="mailto:media@alcf.anl.gov">media@alcf.anl.gov</a><br>
				<a href="https://alcf.anl.gov/">alcf.anl.gov</a>
			</p>
		</div>

		<div class="smallprint">
			<ul>
				<li><a href="http://localhost:4000/about">About</a></li>
				<li><a href="http://localhost:4000/disclaimer">Disclaimer</a></li>
				<li><a href="http://localhost:4000/credits">Credits</a></li>
			</ul>
		</div>




		<div class="bottom">
			<hr>

			<!-- <div class="llc">
				<img src="http://localhost:4000/assets/images/llc-logo.png" />
			</div> -->

			<div class="doe-info">
				<img class='llc' src="http://localhost:4000/assets/images/llc-logo.png" />
				<img class='doe' src="http://localhost:4000/assets/images/doe.png" />
				<p>Argonne National Laboratory is a U.S. Department of Energy laboratory<br>managed by UChicago Argonne, LLC.</p>
			</div>

		</div>
	
  </div>

  <script src="http://localhost:4000/assets/js/plugin--totop.js"></script>
  <script src="http://localhost:4000/assets/js/plugin--dropdown.js"></script>
  <script src="http://localhost:4000/assets/js/app.js"></script>

</footer>
</body>

</html>
