---
layout: highlight

theme: dark
permalink: 'features/aurora-performance-highlights/phasta'

title: 'Machine Learning-Guided Computational Fluid Dynamics at Extreme Scales'
pi: 'Kenneth Jansen, University of Colorado'
award: 'Aurora Early Science Program, INCITE'
systems: '-'

image: 'phasta.png' 

---



{% include txt-intro.html 
    blurb = "This University of Colorado-led effort is aimed at developing a scalable framework to enable in situ machine learning (ML) capabilities from exascale simulations, with the goal of using high-fidelity data from direct numerical simulations (DNS) of complex, wall-bounded turbulent flows to learn accurate and generalizable sub-grid stress (SGS) models for large eddy simulation (LES). LES solves the filtered Navier Stokes equations, and therefore can be run on a coarser mesh, reducing the computational expense, however it relies on SGS models to accurately predict the effects of the unresolved scales. The project also aims to address I/O bottlenecks that are accentuated by the rise of exascale computing. Aurora enables simulations to be conducted at unprecedented scales, producing more data than ever before, making the traditional approach of naively saving the simulation data to disk to then perform training of ML models a bottleneck. By performing training of the ML model concurrently alongside the simulation, we can avoid the file system altogether and efficiently stream data between simulation and ML training modules, storing the necessary data in-memory on the compute nodes. The research team has worked to develop such a framework and ensure scalability to thousands of nodes on Aurora, and to enable efficient inferencing from within the CFD code so these ML models could be deployed in a performant manner on Aurora’s GPUs."
%}



## Challenge
The main challenge for the online ML component was to ensure the framework which supports running a large-scale high-fidelity CFD simulation alongside distributed ML training, and the periodic transfer of training data between the two (up to terabytes in size for a single simulation snapshot when running at 2000 nodes), would scale to thousands of nodes with minimal overhead on either the simulation or the training. To do this, the development team leveraged an open-source library called SmartSim, allowing them to decouple the data transfer between simulation and training by staging the data in a database deployed on the compute nodes. In this way, the data production rate from the simulation is completely independent of the data consumption rate of the ML training, with neither component blocking the progress of the other. Additionally, the team spearheaded a co-located framework with SmartSim which places all components (simulation, training, and database) on the same set of nodes, sharing the available CPU and GPU resources. With this approach, all training data transfer between simulation and ML training is confined within a single node: even though both the MPI simulation and distributed training span thousands of nodes, the framework, in effect, is made scalable.


## Performance Results
The development team has been able to run the PHASTA application on Aurora using as many as 4096 notes; depending on the problem size, the team has achieved a desirable timestep on the order of .6 seconds.

Scaling on up to 1536 nodes, the total overhead generated by the CFD simulation and ML trainer has remained small—approximately 5 percent of runtime—and, importantly, effectively constant. These results have demonstrated that the framework is both performant and scalable.


{% include media-img.html
   source= "phasta-chart.png"
%}

## Impact
Bringing PHASTA to Aurora has provided a scalable and performant solution to performing in-situ ML at exascale, thereby enabling researchers to access and utilize much richer datasets for training ML models using simulation-generated data. Additionally, accuracy and uncertainty quantification metrics obtained from the ongoing training inform the data-generation process within the simulation, which can help permit intelligent data sub-sampling techniques that train models on fewer data samples with reduced energy costs.

The high-resolutions simulations of turbulent flows at high Reynolds numbers performed will help refine the turbulence models currently available, while improving the robustness and accuracy of turbine models. These models can in turn be used for improved design of aircraft and other mechanical systems.


