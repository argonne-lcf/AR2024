---
layout: highlight

theme: dark
permalink: 'features/aurora-performance-highlights/phasta'

title: 'Machine Learning-Guided Computational Fluid Dynamics at Extreme Scales'
pi: 'Kenneth Jansen, University of Colorado'
award: 'Aurora Early Science Program, INCITE'
systems: '-'

image: 'gamess.png' 

---



{% include txt-intro.html 
    blurb = "This University of Colorado-led effort is aimed at developing a scalable framework to enable in situ machine learning (ML) capabilities from exascale simulations, with the goal of using high-fidelity data from direct numerical simulations (DNS) of complex, wall-bounded turbulent flows to learn accurate and generalizable sub-grid stress (SGS) models for large eddy simulation (LES). LES solves the filtered Navier Stokes equations, and therefore can be run on a coarser mesh, reducing the computational expense, however it relies on SGS models to accurately predict the effects of the unresolved scales. The project also aims to address I/O bottlenecks that are accentuated by the rise of exascale computing. Aurora enables simulations to be conducted at unprecedented scales, producing more data than ever before, making the traditional approach of naively saving the simulation data to disk to then perform training of ML models a bottleneck. By performing training of the ML model concurrently alongside the simulation, we can avoid the file system altogether and efficiently stream data between simulation and ML training modules, storing the necessary data in-memory on the compute nodes. The research team has worked to develop such a framework and ensure scalability to thousands of nodes on Aurora, and to enable efficient inferencing from within the CFD code so these ML models could be deployed in a performant manner on Auroraâ€™s GPUs."
%}



# Challenge
The main challenge for the online ML component was to ensure the framework which supports running a large-scale high-fidelity CFD simulation alongside distributed ML training, and the periodic transfer of training data between the two (up to terabytes in size for a single simulation snapshot when running at 2000 nodes), would scale to thousands of nodes with minimal overhead on either the simulation or the training. To do this, the development team leveraged an open-source library called SmartSim, allowing them to decouple the data transfer between simulation and training by staging the data in a database deployed on the compute nodes. In this way, the data production rate from the simulation is completely independent of the data consumption rate of the ML training, with neither component blocking the progress of the other. Additionally, the team spearheaded a co-located framework with SmartSim which places all components (simulation, training, and database) on the same set of nodes, sharing the available CPU and GPU resources. With this approach, all training data transfer between simulation and ML training is confined within a single node: even though both the MPI simulation and distributed training span thousands of nodes, the framework, in effect, is made scalable.


# Performance Results
Focusing on the performance of the online ML framework, the main results is summarized by the figure below. The relative overhead generated by CFD simulation and ML trainer communication of data to and from the database as the application scaled from 1 to 1536 nodes of Aurora at a constant level. In other words, the plots show how much the framework slows down the progress of the simulation and ML training as a fraction of the run time. Total the overhead is small, only around 5% of the runtime and, moreover, remains effectively constant independent of the number of nodes the application is run on. The framework is therefore both performant and scalable, with low and constant overhead.

{% include media-img.html
   source= "phasta-chart.png"
%}

# Impact
The high-resolutions simulations of turbulent flows at high Reynolds numbers performed will help refine the turbulence models currently available, while improving the robustness and accuracy of turbine models. These models can in turn be used for improved design of aircraft and other mechanical systems.

This project provides a scalable and performant solution to performing in situ machine learning in the exascale era, thereby unlocking the ability to access much richer datasets for training ML models from scientific simulations. Additionally, accuracy and uncertainty quantification metrics obtained from the ongoing training inform the data-generation process within the simulation, which can lead to intelligent data sub-sampling techniques to train models on fewer data samples and at reduced energy cost.

